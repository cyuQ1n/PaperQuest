[
    {
        "序号": 1,
        "标题": "P-Masking: Power Law Masking Improves Multi-attribute Controlled Generation",
        "链接": "http://arxiv.org/abs/2410.24201v1",
        "作者": [
            "Mohamed Elgaar",
            "Hadi Amiri"
        ],
        "摘要": "We introduce LingGen, a novel approach for controlled text generation that\noffers precise control over a wide array of linguistic attributes, even as the\nnumber of attributes varies. LingGen employs a dynamic P-MASKING strategy,\nwhich samples masking rates from a power law distribution during training. This\ninnovative approach enables the model to develop robust representations and\nadapt its attribute control capabilities across a variable number of\nattributes, from a single attribute to multiple complex configurations. The\nP-MASKING technique enhances LingGen's ability to manage different levels of\nattribute visibility, resulting in superior performance in multi-attribute\ngeneration tasks. Our experiments demonstrate that LingGen surpasses current\nstate-of-the-art models in both attribute control accuracy and text fluency,\nparticularly excelling in scenarios with varying attribute demands.\nAdditionally, our ablation studies highlight the effectiveness of P-MASKING and\nthe influence of different base language models on performance. These findings\ndemonstrate LingGen's potential for applications requiring precise and\nadaptable control over multiple linguistic attributes in text generation.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T17:55:45+00:00",
        "概述": "本文介绍了LingGen，一种新的多属性控制文本生成方法，通过动态P-MASKING策略，从幂律分布采样遮掩率，使模型能够精确控制多种语言属性，无论属性数量如何变化。实验表明，LingGen在属性控制准确性和文本流畅度方面超越了当前最先进的模型，特别是在属性需求变化的场景中表现优异。研究还验证了P-MASKING的有效性及其对基语言模型性能的影响。",
        "摘要译文": "我们介绍了LingGen，一种新型的受控文本生成方法，即使属性数量变化，也能对广泛的语言属性提供精确控制。LingGen采用了一种动态P-MASKING策略，在训练过程中从幂律分布中采样遮蔽率。这一创新方法使模型能够开发出稳健的表示，并在属性数量从单一属性到多种复杂配置变化的情况下，适应其属性控制能力。P-MASKING技术增强了LingGen在管理不同属性可见性级别方面的能力，从而在多属性生成任务中表现出色。我们的实验表明，LingGen在属性控制准确性及文本流畅度方面超过了当前最先进的模型，特别是在属性需求变化的场景中表现出色。此外，我们的消融研究强调了P-MASKING的有效性以及不同基础语言模型对性能的影响。这些发现展示了LingGen在需要对多个语言属性进行精确和适应性控制的文本生成应用中的潜力。"
    },
    {
        "序号": 3,
        "标题": "Multi-Attribute Linguistic Tuning for Controlled Paraphrase Generation",
        "链接": "http://arxiv.org/abs/2410.24199v1",
        "作者": [
            "Mohamed Elgaar",
            "Hadi Amiri"
        ],
        "摘要": "We present a novel approach to paraphrase generation that enables precise\ncontrol and fine-tuning of 40 linguistic attributes for English. Our model is\nan encoder-decoder architecture that takes as input a source sentence and\ndesired linguistic attributes, and produces paraphrases of the source that\nsatisfy the desired attributes. To guarantee high-quality outputs at inference\ntime, our method is equipped with a quality control mechanism that gradually\nadjusts the embedding of linguistic attributes to find the nearest and most\nattainable configuration of desired attributes for paraphrase generation. We\nevaluate the effectiveness of our method by comparing it to recent controllable\ngeneration models. Experimental results demonstrate that the proposed model\noutperforms baselines in generating paraphrases that satisfy desired linguistic\nattributes.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T17:55:27+00:00",
        "概述": "该研究提出了一种新的多属性语义调整方法，旨在生成具有精准控制的英语同义句。研究采用编码-解码架构，输入源句子和所需的语言属性，生成符合要求的同义句。方法中引入质量控制机制，确保生成高质量的同义句。实验表明，该模型在生成满足需求语言属性的同义句方面优于现有可控生成模型。",
        "摘要译文": "我们提出了一种新颖的同义句生成方法，能够精确控制和调整40种语言属性以适用于英语。我们的模型采用了编码器-解码器架构，输入为源句子和所需的语言属性，输出满足所需属性的同义句。为确保推理时生成高质量的输出，我们的方法配备了一种质量控制机制，该机制逐步调整语言属性的嵌入，以找到最接近且最容易实现的目标属性配置，用于同义句生成。我们通过将该方法与近期可控生成模型进行比较来评估其有效性。实验结果表明，所提出模型在生成满足所需语言属性的同义句方面优于基线模型。"
    },
    {
        "序号": 0,
        "标题": "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use",
        "链接": "http://arxiv.org/abs/2410.24218v1",
        "作者": [
            "Jiajun Xi",
            "Yinong He",
            "Jianing Yang",
            "Yinpei Dai",
            "Joyce Chai"
        ],
        "摘要": "In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "补充信息": "EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL",
        "日期": "2024-10-31T17:59:52+00:00",
        "概述": "本文旨在通过研究不同类型的语言输入，探讨其如何优化基于强化学习的实体代理的学习和推理能力。研究动机在于现有方法多采用简单低级指令，未能充分利用自然语言的丰富性。研究通过分析语言反馈的详尽度（包括对过去行为的反馈和未来指导）和多样性（语言表达的变异性）对代理学习的影响，发现在四个强化学习基准上，接受多样且详尽语言反馈的代理表现出更好的泛化能力和更快的新任务适应速度。",
        "摘要译文": "在现实世界的应用场景中，希望具备实体能力的代理能够利用人类语言来获得显式的或隐含的知识，从而辅助学习任务。尽管近期取得了进展，但大多数之前的方法采用简单的低级指令作为语言输入，这可能无法反映自然的人类交流方式。尚不清楚如何整合丰富的语言使用来促进任务学习。为了解决这一问题，本文研究了不同类型的语言输入如何在增强学习（RL）实体代理方面起促进作用。更具体地说，我们考察了不同语言信息量（即对过去行为的反馈和未来指导）和多样性（即语言表达的变化）如何影响代理学习和推理。基于四个RL基准的实证结果表明，用多样化和信息丰富的语言反馈训练的代理可以实现更好的泛化和快速适应新任务的能力。这些发现强调了在开放环境下通过语言使用向实体代理教授新任务的关键作用。项目网址：\nhttps://github.com/sled-group/Teachable_RL"
    },
    {
        "序号": 2,
        "标题": "Length-Induced Embedding Collapse in Transformer-based Models",
        "链接": "http://arxiv.org/abs/2410.24200v1",
        "作者": [
            "Yuqi Zhou",
            "Sunhao Dai",
            "Zhanshuo Cao",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "摘要": "Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "补充信息": null,
        "日期": "2024-10-31T17:55:36+00:00",
        "概述": "该研究探讨了基于Transformer的模型在处理长文本时性能下降的问题，提出“Length Collapse”现象导致了长文本嵌入压缩至狭窄空间，从而影响下游任务。研究通过自注意力机制的低通滤波器效应，证明了长序列增加了低频信号衰减，深入多层后会使信号仅保留直流分量，导致嵌入压缩。为此，研究提出TempScale方法，即在softmax中引入温度参数，增强低频信号衰减，实验结果显示，该方法在长文本输入上能显著提升模型性能，尤其在大规模文本嵌入基准（MTEB）和长文嵌入（LongEmbed）上分别取得了0.53%和0.82%的性能增益。",
        "摘要译文": "文本嵌入能够支持多种应用，但在处理较长文本时，其性能会下降。在本文中，我们发现这种性能下降的原因是现象称为“长度塌缩”，即较长文本的嵌入会聚集在狭窄的空间中。这种塌缩导致不同长度文本嵌入之间的分布一致性受损，最终影响下游任务的性能。理论上，由于自注意力机制本质上充当低通滤波器，我们的研究表明长序列会增加自注意力机制低通滤波器效应的衰减率。随着层数加深，过度的低通滤波会导致token信号仅保留其直流分量，这意味着输入的token特征图会聚集在狭窄的空间中，特别是在较长文本中。基于上述分析，我们提出通过在softmax()中引入温度来缓解不必要的长度塌缩限制，这种方法可以实现更高的低通衰减率。名为TempScale的无调参方法可以插入多种基于Transformer的嵌入模型中。实验证明，TempScale可以在现有嵌入模型中提高性能，特别是在长文本输入方面，在Massive Text Embedding Benchmark (MTEB)的40个数据集上可获得高达0.53%的性能提升，在LongEmbed的4个数据集上可获得高达0.82%的性能提升，后者专门针对长上下文检索。"
    },
    {
        "序号": 4,
        "标题": "SelfCodeAlign: Self-Alignment for Code Generation",
        "链接": "http://arxiv.org/abs/2410.24198v1",
        "作者": [
            "Yuxiang Wei",
            "Federico Cassano",
            "Jiawei Liu",
            "Yifeng Ding",
            "Naman Jain",
            "Zachary Mueller",
            "Harm de Vries",
            "Leandro von Werra",
            "Arjun Guha",
            "Lingming Zhang"
        ],
        "摘要": "Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.",
        "分类": [
            "cs.CL",
            "cs.LG",
            "cs.SE"
        ],
        "补充信息": "Accepted to NeurIPS 2024",
        "日期": "2024-10-31T17:55:13+00:00",
        "概述": "SelfCodeAlign 是一种无需大量标注或蒸馏的自对齐方法，用于代码生成的干预。其动机是改进大型语言模型（LLMs）的人类指令遵循能力。该方法通过从高质量代码片段中提取多样化的编程概念，生成新任务并生成多个响应，在沙箱环境中进行验证，最终选择通过的示例进行指令调优。实验表明，使用SelfCodeAlign与CodeQwen1.5-7B生成的数据集调优后的模型在HumanEval+上通过率（pass@1）达到67.1%，远超CodeLlama-70B-Instruct，且在不同规模的LLM上均表现优异。",
        "摘要译文": "指令调优是一种监督微调方法，能显著提高大规模语言模型（LLMs）遵循人类指令的能力。我们提出了SelfCodeAlign，这是首个完全透明且许可的管道，用于在无需大量人工注释或蒸馏的情况下自我对齐代码LLMs。SelfCodeAlign在整个数据生成过程中使用相同的基模型进行推理。首先，它从高质量的种子代码片段中提取多样化的编码概念，生成新的任务。然后，它为每个任务采样多个响应，将每个响应与测试案例配对，并在沙盒环境中进行验证。最后，通过示例被选中进行指令调优。在我们的主要实验中，我们使用SelfCodeAlign与CodeQwen1.5-7B生成了一个包含74,000个指令-响应对的数据集。在该数据集上进行微调后，模型在HumanEval+上达到了67.1的pass@1，尽管其规模仅为CodeLlama-70B-Instruct的十分之一，但仍超越了后者。在所有基准测试中，经过微调的模型始终优于使用OctoPack训练的原版模型，OctoPack是之前没有人工注释或蒸馏的情况下指令调优的最先进方法。此外，我们展示了SelfCodeAlign在不同规模的LLM（从3B到33B）中均有效，并且基模型可以从与自身数据分布的对齐中获益更多。我们还验证了管道中每个组件的有效性，显示SelfCodeAlign优于直接从GPT-4o蒸馏以及GPT-3.5基于的领先蒸馏方法，如OSS-Instruct和Evol-Instruct。SelfCodeAlign还催生了StarCoder2-Instruct，这是首个完全透明、许可宽松且自我对齐的代码LLM，达到了最先进的编码性能。"
    },
    {
        "序号": 6,
        "标题": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
        "链接": "http://arxiv.org/abs/2410.24177v1",
        "作者": [
            "Heng-Jui Chang",
            "Hongyu Gong",
            "Changhan Wang",
            "James Glass",
            "Yu-An Chung"
        ],
        "摘要": "Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs.",
        "分类": [
            "eess.AS",
            "cs.CL",
            "cs.LG",
            "cs.SD"
        ],
        "补充信息": "Preprint",
        "日期": "2024-10-31T17:43:13+00:00",
        "概述": "这篇论文旨在通过引入Double-Codebook Speaker-invariant Clustering (DC-Spin)技术，提高语音分词的准确性，进而增强语音理解与生成能力。DC-Spin通过提取鲁棒性强且富含语音特征的speaker-invariant tokens，解决了传统方法在处理语音信号时的输入变化敏感问题。研究比较了多种分词方法，发现易于n-gram语言模型建模或与音素对齐的token表现出色，为设计适用于SLMs的语音分词器提供了指导。",
        "摘要译文": "随言语言模型（SLMs）随着基于文本的 decoder-only 语言模型的发展而获得了越来越多的关注。SLMs 处理文本和语音，使同时理解发音和生成成为可能。本文介绍了 Double-Codebook Speaker-invariant Clustering（DC-Spin），其目的是通过连接音频信号和 SLM 令牌来提高语音分词的效果。DC-Spin 提取了丰富的音素信息且对输入变化具有鲁棒性的 speaker-invariant 令牌，增强了一次性 SLM 任务和语音合成。我们提出了一种 chunk-wise 的方法，以实现无需重新训练和退化的流式 DC-Spin。对分词方法（自我监督和神经音频编解码器）、模型可扩展性和下游任务代理的比较表明，那些易于由 n-gram 语言模型建模或与音素对齐的令牌表现出色，为设计 SLM 的语音分词器提供了见解。"
    },
    {
        "序号": 7,
        "标题": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models",
        "链接": "http://arxiv.org/abs/2410.24175v1",
        "作者": [
            "Yunjia Qi",
            "Hao Peng",
            "Xiaozhi Wang",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "摘要": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "14 pages, 6 figures",
        "日期": "2024-10-31T17:42:26+00:00",
        "概述": "该研究着眼于大型语言模型在处理复杂格式和长度约束的指令时的不足。为了解决这一问题，研究者提出了一种新的数据生成技术——约束反向翻译（Constraint Back-Translation，CBT）。通过利用现有高质量的指令-响应对，并仅使用先进的大语言模型添加已经满足的复杂约束，这种方法既减少了成本又降低了数据噪音。实验表明，基于此新数据集（名为CRAB）的后训练显著提升了多个大语言模型处理复杂指令的能力，并发现约束反向翻译也是一种有效的辅助训练目标。",
        "摘要译文": "大型语言模型（LLMs）在遵循格式、长度等复杂的约束指令时存在困难。按照传统的指令微调方法，先前的工作通过向高级LLM提供复杂指令，生成复杂的指令-响应对，从而在微调后处理这些复杂的指令-响应对。然而，即使高级LLM也不能很好地遵循复杂的指令，从而限制了生成数据的质量。在本文中，我们发现现有的数据集本身就内含隐式的复杂约束，并提出了一种新颖的数据生成技术——约束反翻译。具体而言，我们利用现有数据集中高质量的指令-响应对，仅使用高级LLM将响应已经满足的复杂约束添加到指令中，从而自然地降低了成本和数据噪声。在实验中，我们使用Llama3-70B-Instruct进行约束反翻译，创建了一个高质量的复杂指令-响应数据集，命名为CRAB。我们表明，在CRAB上的微调提高了多种骨干LLM在广泛指令遵循基准上的复杂指令遵循能力。我们还发现，约束反翻译也可以作为有用的辅助训练目标在微调中发挥作用。我们的代码、数据和模型将被发布以促进未来的研究。"
    },
    {
        "序号": 5,
        "标题": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
        "链接": "http://arxiv.org/abs/2410.24190v1",
        "作者": [
            "Yujin Potter",
            "Shiyang Lai",
            "Junsol Kim",
            "James Evans",
            "Dawn Song"
        ],
        "摘要": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while leaving some open\nquestions.",
        "分类": [
            "cs.CL",
            "cs.CY"
        ],
        "补充信息": "EMNLP 2024 Main",
        "日期": "2024-10-31T17:51:00+00:00",
        "概述": "该研究旨在探讨大型语言模型（LLMs）的政治倾向及其对选民投票行为的影响。通过模拟选举和实际实验，研究者发现LLMs（如Claude-3、Llama-3和GPT-4）在与选民互动后，显著倾向于支持民主党候选人，使投票差距扩大至4.6%，远超以往政治竞选的影响。研究还探讨了保持LLMs政治中立的安全方法。",
        "摘要译文": "这些大语言模型（LLMs）如何影响我们的民主？我们通过在美国总统选举的背景下进行多项实验，研究了LLMs的政治倾向及其对选民潜在影响。首先，通过选举模拟，我们展示了18个开放和关闭权重的LLMs（大模型）对民主党候选人相较于共和党候选人的政治偏好。我们展示了这些模型在指令调优版本中对民主党候选人的偏好更为明显，这通过分析它们对候选人政策相关问题的回答体现出来。我们进一步通过一项针对935名美国注册选民的实验，探讨了LLMs对选民选择的潜在影响。实验中，参与者与LLMs（Claude-3、Llama-3和GPT-4）进行了五轮交流。实验结果显示，与LLM交流后，选民对民主党候选人的选择倾向有所增加，投票边际从0.7%扩大到4.6%，尽管LLMs在交流过程中并未被要求说服用户支持民主党候选人。这一效应比以往许多关于政治竞选说服力的研究（这些研究在总统选举中显示出较小的影响）更为显著。许多用户也表达了希望与LLM进一步互动的愿望。这些选民选择变化背后的具体因素尚需进一步研究。最后，我们探讨了如何通过一种安全方法使LLMs更加政治中立，但仍留下了一些未解的问题。"
    },
    {
        "序号": 10,
        "标题": "GPT or BERT: why not both?",
        "链接": "http://arxiv.org/abs/2410.24159v1",
        "作者": [
            "Lucas Georges Gabriel Charpentier",
            "David Samuel"
        ],
        "摘要": "We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "22 pages; submission to the BabyLM Challenge 2024",
        "日期": "2024-10-31T17:18:11+00:00",
        "概述": "该研究旨在结合GPT（自回归模型）和BERT（掩码模型）的优势，提出了一种简单的混合训练目标，即同时进行因果语言建模和掩码语言建模。通过BabyLM挑战赛的测试，结果显示这种混合模型优于仅使用掩码或因果语言模型的方法。该研究开放地发布了模型、训练数据和代码。",
        "摘要译文": "我们提出了一种简单的方法，将掩码语言模型与因因果语言模型结合在一起。这种混合训练目标导致了一个在单一变压器堆栈中结合了两种建模范式优势的模型：GPT-BERT 可以像任何标准的因果或掩码语言模型一样透明地使用。我们在这项2024年的BabyLM挑战赛中测试了实现这种灵活行为的预训练过程。结果显示，混合预训练优于仅掩码或仅因果模型。我们公开发布了模型、训练语料库和代码。"
    },
    {
        "序号": 8,
        "标题": "Novel Architecture for Distributed Travel Data Integration and Service Provision Using Microservices",
        "链接": "http://arxiv.org/abs/2410.24174v1",
        "作者": [
            "Biman Barua",
            "M. Shamim Kaiser"
        ],
        "摘要": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
        "分类": [
            "cs.CE",
            "cs.CL",
            "cs.DC"
        ],
        "补充信息": "20 pages, 12 figures",
        "日期": "2024-10-31T17:41:14+00:00",
        "概述": "该研究提出了一种基于微服务的分布式架构，旨在提高航空预订系统的灵活性和性能。通过集成Redis缓存、Kafka和RabbitMQ消息系统、MongoDB和PostgreSQL存储，以及OAuth2和JWT安全通信等方式，实现高一致性和低延迟的数据传输，提高系统吞吐量和可扩展性。研究结果显示，该架构的数据一致性达99.5%，延迟低于75毫秒，并通过Docker和Kubernetes提升了系统的水平扩展能力，确保了高并发情况下的响应效率，整体提升用户体验和操作效率。",
        "摘要译文": "本文介绍了一种微服务架构，旨在增强航空公司预订系统的灵活性和性能。该架构集成了Redis缓存技术，两种不同的消息系统（Kafka和RabbitMQ），以及两种类型的存储（MongoDB和PostgreSQL）。此外，还引入了授权技术，包括通过OAuth2和JWT实现的安全通信，这对于管理高需求的旅行服务是必不可少的。根据选定的指标，该架构在数据一致性方面达到了99.5%的水平，并且数据传播的延迟低于75毫秒，允许微服务之间快速可靠地相互通信。系统每秒处理1050个事件，即使在高峰时段也能保持可接受的性能水平。Redis缓存将数据库的缓存命中率提高到了92%，从而减轻了对数据库的负担并提高了响应速度。通过使用Docker和Kubernetes进一步提高了系统的可扩展性，使服务能够水平扩展以应对需求变化。错误率极低，仅为0.2%，进一步提高了系统处理实时数据集成的效率。此方法被建议用于满足航空公司预订系统的特定需求。它安全、快速、可扩展，旨在改善用户体验以及运营效率。低延迟和高数据集成水平以及有效的资源使用展示了该架构在不断增长的高需求情况下持续提供支持的能力。"
    },
    {
        "序号": 9,
        "标题": "Redefining <Creative> in Dictionary: Towards a Enhanced Semantic Understanding of Creative Generation",
        "链接": "http://arxiv.org/abs/2410.24160v1",
        "作者": [
            "Fu Feng",
            "Yucheng Xie",
            "Jing Wang",
            "Xin Geng"
        ],
        "摘要": "Creativity, both in human and diffusion models, remains an inherently\nabstract concept; thus, simply adding \"creative\" to a prompt does not yield\nreliable semantic recognition by the model. In this work, we concretize the\nabstract notion of \"creative\" through the TP2O task, which aims to merge two\nunrelated concepts, and introduce CreTok, redefining \"creative\" as the token\n$\\texttt{<CreTok>}$. This redefinition offers a more concrete and universally\nadaptable representation for concept blending. This redefinition occurs\ncontinuously, involving the repeated random sampling of text pairs with\ndifferent concepts and optimizing cosine similarity between target and constant\nprompts. This approach enables $\\texttt{<CreTok>}$ to learn a method for\ncreative concept fusion. Extensive experiments demonstrate that the creative\ncapability enabled by $\\texttt{<CreTok>}$ substantially surpasses recent SOTA\ndiffusion models and achieves superior creative generation. CreTok exhibits\ngreater flexibility and reduced time overhead, as $\\texttt{<CreTok>}$ can\nfunction as a universal token for any concept, facilitating creative generation\nwithout retraining.",
        "分类": [
            "cs.CV",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T17:19:03+00:00",
        "概述": "本文针对创造性生成这一抽象概念，提出了一种新的方法。动机在于现有模型仅凭“创意”提示无法可靠生成创意内容。方法是通过TP2O任务（将两个不相关概念融合）和引入CreTok（创意标记），重新定义“创意”。该方法通过随机采样不同概念的文本对，优化目标提示与常量提示的余弦相似度，使CreTok学习创意概念融合。实验结果表明，CreTok显著超越了当前最先进的扩散模型，在创意生成方面表现出更优效果，且具有更高的灵活性和效率。",
        "摘要译文": "创造力，无论是对于人类还是扩散模型而言，始终是一个抽象的概念；因此，在提示中简单加上“创造性”并不能使模型可靠地进行语义识别。在本文中，我们通过TP2O任务具体化了抽象的“创造性”概念，该任务旨在将两个不相关的概念融合在一起，并引入了CreTok，将其定义为$\\texttt{<CreTok>}$标记。这种重新定义提供了更为具体且普遍适用的概念融合表示。这一重新定义是持续进行的，涉及重复地随机采样具有不同概念的文本对，并优化目标提示和固定提示之间的余弦相似度。这种方法使$\\texttt{<CreTok>}$能够学习一种创造概念融合的方法。大量的实验表明，由$\\texttt{<CreTok>}$所提供的创造性能力远远超过了最近的SOTA扩散模型，并实现了更高质量的创造性生成。CreTok展示了更大的灵活性和减少的时间开销，因为$\\texttt{<CreTok>}$可以作为一个通用标记应用于任何概念，从而在无需重新训练的情况下促进创造性生成。"
    },
    {
        "序号": 11,
        "标题": "Thought Space Explorer: Navigating and Expanding Thought Space for Large Language Model Reasoning",
        "链接": "http://arxiv.org/abs/2410.24155v1",
        "作者": [
            "Jinghan Zhang",
            "Fengran Mo",
            "Xiting Wang",
            "Kunpeng Liu"
        ],
        "摘要": "Recent advances in large language models (LLMs) have demonstrated their\npotential in handling complex reasoning tasks, which are usually achieved by\nconstructing a thought chain to guide the model to solve the problem with\nmulti-step thinking. However, existing methods often remain confined to\npreviously explored solution spaces and thus overlook the critical blind spot\nwithin LLMs' cognitive range. To address these issues, we design the Thought\nSpace Explorer (TSE), a novel framework to expand and optimize thought\nstructures to guide LLMs to explore their blind spots of thinking. By\ngenerating new reasoning steps and branches based on the original thought\nstructure with various designed strategies, TSE broadens the thought space and\nalleviates the impact of blind spots for LLM reasoning. Experimental results on\nmultiple levels of reasoning tasks demonstrate the efficacy of TSE. We also\nconduct extensive analysis to understand how structured and expansive thought\ncan contribute to unleashing the potential of LLM reasoning capabilities.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T17:12:14+00:00",
        "概述": "这项研究旨在通过设计“Thought Space Explorer”框架，解决大型语言模型在复杂推理任务中容易受限于已有解空间，忽视认知范围内的盲区问题。TSE通过生成新的推理步骤和分支，扩展思维空间，从而优化模型的推理能力。实验结果显示，TSE在多个推理任务级别上有效提升了模型的性能。研究表明，结构化且扩展性的思考能够释放大型语言模型的潜在推理能力。",
        "摘要译文": "近年来，大型语言模型（LLMs）在处理复杂推理任务方面展现了潜力，通常通过构建思维链来引导模型通过多步思考解决问题。然而，现有方法往往局限于之前探索过的解决方案空间，从而忽略了LLMs认知范围内的重要盲点。为解决这些问题，我们设计了思维空间探索者（TSE），这是一种新颖的框架，用于扩展和优化思维结构，以引导LLMs探索其思维盲点。通过基于原始思维结构生成各种设计策略的新推理步骤和分支，TSE扩大了思维空间，并减轻了LLMs推理中的盲点影响。在多个推理任务层级上的实验结果表明了TSE的有效性。我们还进行了广泛的分析，以了解有组织且扩展性的思维如何有助于释放LLMs推理能力的潜力。"
    },
    {
        "序号": 12,
        "标题": "Scaling Concept With Text-Guided Diffusion Models",
        "链接": "http://arxiv.org/abs/2410.24151v1",
        "作者": [
            "Chao Huang",
            "Susan Liang",
            "Yunlong Tang",
            "Yapeng Tian",
            "Anurag Kumar",
            "Chenliang Xu"
        ],
        "摘要": "Text-guided diffusion models have revolutionized generative tasks by\nproducing high-fidelity content from text descriptions. They have also enabled\nan editing paradigm where concepts can be replaced through text conditioning\n(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of\nreplacing a concept, can we enhance or suppress the concept itself? Through an\nempirical study, we identify a trend where concepts can be decomposed in\ntext-guided diffusion models. Leveraging this insight, we introduce\nScalingConcept, a simple yet effective method to scale decomposed concepts up\nor down in real input without introducing new elements. To systematically\nevaluate our approach, we present the WeakConcept-10 dataset, where concepts\nare imperfect and need to be enhanced. More importantly, ScalingConcept enables\na variety of novel zero-shot applications across image and audio domains,\nincluding tasks such as canonical pose generation and generative sound\nhighlighting or removal.",
        "分类": [
            "cs.CV",
            "cs.CL"
        ],
        "补充信息": "Project page: https://wikichao.github.io/ScalingConcept/",
        "日期": "2024-10-31T17:09:55+00:00",
        "概述": "这篇文章研究了如何通过文本指导的扩散模型增强或削弱特定概念。motivation是现有模型仅能替换概念而不能增强或削弱，作者提出ScalingConcept方法，通过解构概念并调整其在输入中的强度来实现这一目标。研究者通过WeakConcept-10数据集验证了方法的有效性，并展示了其在图像和音频领域零样本应用的潜力，如姿态生成和声音生成中的高亮或消除。",
        "摘要译文": "文本引导的扩散模型通过从文本描述生成高保真内容，已经革新了生成任务。它们还使通过文本条件替代概念的编辑成为可能（例如，从狗变为老虎）。在本文中，我们探索了一种新颖的方法：我们能否增强或抑制概念本身，而不仅仅是替换概念？通过一项实证研究，我们发现文本引导的扩散模型中概念可以被分解。利用这一洞见，我们引入了ScalingConcept，这是一种简单而有效的方法，可以在不引入新元素的情况下，增强或减弱真实输入中的分解概念。为了系统地评估我们的方法，我们提出了WeakConcept-10数据集，其中的概念是不完美的，需要增强。更重要的是，ScalingConcept使跨图像和音频领域的多种新颖的零样本应用成为可能，包括生成标准姿势、生成声音突出或移除等任务。"
    },
    {
        "序号": 13,
        "标题": "Don't Touch My Diacritics",
        "链接": "http://arxiv.org/abs/2410.24140v1",
        "作者": [
            "Kyle Gorman",
            "Yuval Pinter"
        ],
        "摘要": "The common practice of preprocessing text before feeding it into NLP models\nintroduces many decision points which have unintended consequences on model\nperformance. In this opinion piece, we focus on the handling of diacritics in\ntexts originating in many languages and scripts. We demonstrate, through\nseveral case studies, the adverse effects of inconsistent encoding of\ndiacritized characters and of removing diacritics altogether. We call on the\ncommunity to adopt simple but necessary steps across all models and toolkits in\norder to improve handling of diacritized text and, by extension, increase\nequity in multilingual NLP.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "6 pages",
        "日期": "2024-10-31T17:03:44+00:00",
        "概述": "本文探讨了在预处理文本时对语言中重音符号处理不当对NLP模型性能的负面影响。研究通过案例分析揭示了不一致的重音符号编码和完全去除重音符号的危害。作者呼吁社区采取简单但必要的措施，以改善对重音符号文本的处理，从而提高多语言NLP的公平性。",
        "摘要译文": "在将文本输入NLP模型之前进行预处理的常见做法引入了许多决策点，这些决策点对模型性能产生了意想不到的影响。在这篇文章中，我们重点关注源自多种语言和书写的重音符号的处理。我们通过几个案例研究展示了不一致的重音符号编码以及完全去除重音符号的负面影响。我们呼吁社区在整个模型和工具集中采取一些简单但必要的步骤，以改善对重音符号文本的处理，并由此增加多语言NLP的公平性。"
    },
    {
        "序号": 14,
        "标题": "Multi-environment Topic Models",
        "链接": "http://arxiv.org/abs/2410.24126v1",
        "作者": [
            "Dominic Sobhani",
            "Amir Feder",
            "David Blei"
        ],
        "摘要": "Probabilistic topic models are a powerful tool for extracting latent themes\nfrom large text datasets. In many text datasets, we also observe per-document\ncovariates (e.g., source, style, political affiliation) that act as\nenvironments that modulate a \"global\" (environment-agnostic) topic\nrepresentation. Accurately learning these representations is important for\nprediction on new documents in unseen environments and for estimating the\ncausal effect of topics on real-world outcomes. To this end, we introduce the\nMulti-environment Topic Model (MTM), an unsupervised probabilistic model that\nseparates global and environment-specific terms. Through experimentation on\nvarious political content, from ads to tweets and speeches, we show that the\nMTM produces interpretable global topics with distinct environment-specific\nwords. On multi-environment data, the MTM outperforms strong baselines in and\nout-of-distribution. It also enables the discovery of accurate causal effects.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T16:50:39+00:00",
        "概述": "该研究旨在通过Multi-environment Topic Model (MTM)解决文本数据中不同环境（如来源、风格、政治倾向）对主题影响的建模问题。MTM是一种无监督的概率模型，能够分离全局和环境特定的主题词。实验显示，MTM在多环境文本数据上表现优于强基线，并能发现准确的因果效应。",
        "摘要译文": "概率主题模型是从大型文本数据集中提取潜在主题的有力工具。在许多文本数据集中，我们还会观察到每篇文档的协变量（例如，来源、风格、政治派别），它们作为环境，影响“全局”（环境无关）的主题表示。准确学习这些表示对于在未见过的环境中对新文档进行预测以及估计主题对实际结果的因果效应非常重要。为此，我们引入了多环境主题模型（MTM），这是一个无监督的概率模型，可以分离全局和环境特定的术语。通过在各种政治内容上的实验，从广告到推文和演讲，我们展示了MTM产生了可解释的全局主题和具有明显环境特定词汇的主题。在多环境数据上，MTM在分布内和分布外表现优于强大基线，还能够发现准确的因果效应。"
    },
    {
        "序号": 15,
        "标题": "Nearest Neighbor Normalization Improves Multimodal Retrieval",
        "链接": "http://arxiv.org/abs/2410.24114v1",
        "作者": [
            "Neil Chowdhury",
            "Franklin Wang",
            "Sumedh Shenoy",
            "Douwe Kiela",
            "Sarah Schwettmann",
            "Tristan Thrush"
        ],
        "摘要": "Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.",
        "分类": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T16:44:10+00:00",
        "概述": "本文提出了Nearest Neighbor Normalization（NNN）方法，旨在改善对比模型的多模态检索性能，无需额外训练。NNN需要一个参考数据库进行规范化处理，适用于多种模型（如CLIP、BLIP、ALBEF等）和数据集（如MS-COCO、Flickr30k），能够提升文本和图像检索的准确性。",
        "摘要译文": "多模态模型利用大规模预训练在图像字幕、视觉问答和跨模态检索等任务上取得了很强但仍然不够完美的性能。在本文中，我们介绍了一种简单而高效的方法，用于在无需额外训练的情况下纠正训练好的对比图像-文本检索模型中的错误，这种方法称为最近邻归一化（NNN）。我们展示了在我们测试的所有对比模型（CLIP、BLIP、ALBEF、SigLIP、BEiT）和所使用的两个数据集（MS-COCO和Flickr30k）上，NNN在文本检索和图像检索的检索指标上都取得了改进。NNN 需要一个参考数据库，但不需要对这个数据库进行任何训练，甚至可以在模型微调后提高检索准确性。"
    },
    {
        "序号": 16,
        "标题": "In-Context Fine-Tuning for Time-Series Foundation Models",
        "链接": "http://arxiv.org/abs/2410.24087v1",
        "作者": [
            "Abhimanyu Das",
            "Matthew Faw",
            "Rajat Sen",
            "Yichen Zhou"
        ],
        "摘要": "Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T16:20:04+00:00",
        "概述": "该研究旨在提升时间序列预测的准确性，动机源于时间序列基础模型在零样本预测中的成功应用。研究设计了一种新的“上下文调优”方法，该方法通过在预测时提供多个相关时间序列示例，帮助模型更好地适应目标领域。实验结果表明，该方法在流行的时间序列预测基准测试中表现优于-supervised深度学习方法、统计模型及其他时间序列基础模型，甚至接近于在目标领域进行显式调优的模型。",
        "摘要译文": "受近期时间序列基础模型在零样本预测方面的成功启发，我们提出了一种时间序列基础模型的“在上下文中微调”方法。特别地，我们设计了一个预训练的基础模型，在推理时可以被提示多个时间序列示例，从而对未来的目标时间序列进行预测。我们的基础模型专门训练，以便在上下文窗口中利用多个相关时间序列的示例（除目标时间序列的历史记录外），以帮助它在推理时适应目标域的具体分布。我们展示了，在推理时使用上下文示例的基础模型在流行的预测基准测试中能比监督深度学习方法、统计模型以及其他时间序列基础模型获得更好的性能。有趣的是，我们的在上下文中微调方法甚至能与在目标域显式微调的基础模型性能相匹敌。"
    },
    {
        "序号": 18,
        "标题": "Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks",
        "链接": "http://arxiv.org/abs/2410.24032v1",
        "作者": [
            "Yingzhe Peng",
            "Xiaoting Qin",
            "Zhiyang Zhang",
            "Jue Zhang",
            "Qingwei Lin",
            "Xu Yang",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "摘要": "The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.",
        "分类": [
            "cs.HC",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T15:30:55+00:00",
        "概述": "本文旨在通过结合多代理大语言模型框架和结构化用户界面，提出CARE系统，以增强探索性任务中的个性化支持。该系统通过迭代查询 refinement 和动态解决方案生成，帮助识别用户的显式和隐式需求，提供更加定制化的解决方案。研究结果显示，CARE优于基准大语言模型聊天机器人，显著减轻了用户的认知负担，激发了创造力，并提供了更加个性化的解决方案，显示出转化为主动式个性化问题解决伙伴的潜力。",
        "摘要译文": "大型语言模型（LLMs）的兴起已经彻底改变了用户与知识型系统的交互方式，使聊天机器人能够综合大量信息并协助处理复杂、探索性的任务。然而，基于LLM的聊天机器人在提供个性化支持方面常常遇到困难，尤其是在用户提出模糊查询或缺乏足够上下文信息的情况下。本文介绍了协作式个性化探索助理（CARE），这是一种结合多Agent LLM框架和结构化用户界面的系统，旨在通过这种方法增强探索任务中的个性化。CARE的界面包括聊天面板、解决方案面板和需求面板，支持迭代查询改进和动态解决方案生成。多Agent框架协作识别用户的显性和隐性需求，并提供量身定制的可操作解决方案。在一项包含22位参与者的单被试用户研究中，CARE比基线LLM聊天机器人更受用户欢迎，用户们称赞它能减轻认知负担、激发创造力并提供更个性化的解决方案。我们的研究发现表明，CARE有可能将基于LLM的系统从被动的信息检索者转变为个性化问题解决和探索过程中的主动合作伙伴。"
    },
    {
        "序号": 19,
        "标题": "Joint Training for Selective Prediction",
        "链接": "http://arxiv.org/abs/2410.24029v1",
        "作者": [
            "Zhaohui Li",
            "Rebecca J. Passonneau"
        ],
        "摘要": "Classifier models are prevalent in natural language processing (NLP), often\nwith high accuracy. Yet in real world settings, human-in-the-loop systems can\nfoster trust in model outputs and even higher performance. Selective Prediction\n(SP) methods determine when to adopt a classifier's output versus defer to a\nhuman. Previous SP approaches have addressed how to improve softmax as a\nmeasure of model confidence, or have developed separate confidence estimators.\nOne previous method involves learning a deferral model based on engineered\nfeatures. We introduce a novel joint-training approach that simultaneously\noptimizes learned representations used by the classifier module and a learned\ndeferral policy. Our results on four classification tasks demonstrate that\njoint training not only leads to better SP outcomes over two strong baselines,\nbut also improves the performance of both modules.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-31T15:28:26+00:00",
        "概述": "这项研究旨在通过结合模型和决策策略提高分类准确性和可信度。以往方法主要优化softmax或开发独立的信任估计器，而本文提出了一种联合训练方法，同时优化分类器和决策策略。实验表明，这种联合训练方法不仅提高了选择性预测的性能，还提升了分类器和决策策略的单独性能。",
        "摘要译文": "分类模型在自然语言处理（NLP）中很常见，往往准确性很高。然而，在实际应用中，带有人类干预的系统可以增强对模型输出的信任并提高性能。选择性预测（SP）方法决定何时采用分类器的输出，何时将任务转交给人类。之前的SP方法侧重于改进softmax作为模型置信度的衡量标准，或者开发独立的置信度估计器。一种先前的方法是基于工程特征学习一个转交模型。我们提出了一个新颖的联合训练方法，同时优化了分类器模块使用的学习表示和学习到的转交策略。我们在四个分类任务上的结果表明，联合训练不仅在两种强baseline之上取得了更好的SP结果，而且还提升了两个模块的性能。"
    },
    {
        "序号": 20,
        "标题": "Detecting text level intellectual influence with knowledge graph embeddings",
        "链接": "http://arxiv.org/abs/2410.24021v1",
        "作者": [
            "Lucian Li",
            "Eryclis Silva"
        ],
        "摘要": "Introduction: Tracing the spread of ideas and the presence of influence is a\nquestion of special importance across a wide range of disciplines, ranging from\nintellectual history to cultural analytics, computational social science, and\nthe science of science.\n  Method: We collect a corpus of open source journal articles, generate\nKnowledge Graph representations using the Gemini LLM, and attempt to predict\nthe existence of citations between sampled pairs of articles using previously\npublished methods and a novel Graph Neural Network based embedding model.\n  Results: We demonstrate that our knowledge graph embedding method is superior\nat distinguishing pairs of articles with and without citation. Once trained, it\nruns efficiently and can be fine-tuned on specific corpora to suit individual\nresearcher needs.\n  Conclusion(s): This experiment demonstrates that the relationships encoded in\na knowledge graph, especially the types of concepts brought together by\nspecific relations can encode information capable of revealing intellectual\ninfluence. This suggests that further work in analyzing document level\nknowledge graphs to understand latent structures could provide valuable\ninsights.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T15:21:27+00:00",
        "概述": "本文旨在通过知识图嵌入方法追踪文本级别的智识影响。研究动机来自对跨学科领域（如智力历史、文化分析、计算社会科学和科学学）中思想传播和影响痕迹的重视。研究方法包括收集开源期刊文章，使用Gemini LLM生成知识图表示，并使用先前方法及新型图神经网络嵌入模型预测文章引用关系。研究结果表明，知识图嵌入方法在区分有无引用的文对方面表现更优，并且训练后可以高效地针对特定语料库进行微调。研究结果暗示，进一步分析文档级别知识图以揭示潜在结构，将提供宝贵见解。",
        "摘要译文": "介绍：追溯思想的传播和影响的存在对于多个学科都具有特殊的重要性，包括智力历史、文化分析、计算社会科学以及科学学等领域。\n方法：我们收集了一组开源期刊文章，使用Gemini LLM生成知识图谱表示，并尝试使用先前发表的方法和一种基于图神经网络的嵌入模型来预测样本文章对之间是否存在引用关系。\n结果：我们展示了我们的知识图嵌入方法在区分有引用关系和无引用关系的文章对方面更为优越。训练后，该方法运行效率高，并可根据特定的语料库进行微调以满足个别研究者的需要。\n结论：该实验显示，知识图谱中编码的关系，尤其是特定关系组合的概念类型，可以包含能够揭示智力影响的信息。这表明进一步分析文档级别的知识图谱以理解潜在结构可能提供有价值的见解。"
    },
    {
        "序号": 17,
        "标题": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs",
        "链接": "http://arxiv.org/abs/2410.24049v1",
        "作者": [
            "Muhammed Saeed",
            "Elgizouli Mohamed",
            "Mukhtar Mohamed",
            "Shaina Raza",
            "Shady Shehata",
            "Muhammad Abdul-Mageed"
        ],
        "摘要": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T15:45:23+00:00",
        "概述": "这篇论文旨在评估大型语言模型（LLM）在对阿拉伯人与西方人的偏见以及模型的安全性方面的表现。研究通过创建两个数据集分别评估LLM的偏见和模型的安全性，并测试其对负面特质的生成能力。研究评估了包括GPT-4、GPT-4o、LlaMA 3.1（8B与405B）、Mistral 7B和Claude 3.5 Sonnet在内的六种模型。结果显示，79%的LLM样本对阿拉伯人存在偏见，LlaMA 3.1-405B最为偏见。在模型安全性测试中，GPT-4o表现最差，Claude 3.5 Sonnet相对最安全，但仍有偏见。研究指出，优化版本GPT-4o反而更易产生偏见和脱逃，突显了偏见缓解和安全性增强的紧迫性。",
        "摘要译文": "大型语言模型（LLMs）虽然广泛使用，但由于嵌入了社会偏见，引发了伦理上的担忧。本研究探讨了LLMs对阿拉伯人与西方人之间的偏见，涉及女性权益、恐怖主义、反犹太主义等领域，并评估了模型抵制延续这些偏见的能力。为此，我们创建了两个数据集：一个用于评估LLM对阿拉伯人与西方人的偏见，另一个用于测试模型在面对夸大负面特征的提示（“脱狱”）时的安全性。我们评估了六种LLM——GPT-4、GPT-4o、LlaMA 3.1（8B & 405B）、Mistral 7B、Claude 3.5 Sonnet。结果显示，在79%的情况下，模型表现为对阿拉伯人的负面偏见，其中LlaMA 3.1-405B表现最为偏见。我们的“脱狱”测试显示，尽管GPT-4o是一个优化版本，但它仍然是最脆弱的，其次是LlaMA 3.1-8B和Mistral 7B。在三个类别中，所有LLM除了Claude之外，在攻击成功率上均超过87%。我们还发现Claude 3.5 Sonnet是最安全的，但仍然在七个类别中表现出偏见。尽管GPT-4o是GPT4的一个优化版本，我们发现它更容易出现偏见和“脱狱”现象，这表明优化可能存在缺陷。我们的研究结果强调了在LLM中需要采取更加 robust的偏见缓解策略，并加强安全措施的迫切需求。"
    },
    {
        "序号": 21,
        "标题": "Speech is More Than Words: Do Speech-to-Text Translation Systems Leverage Prosody?",
        "链接": "http://arxiv.org/abs/2410.24019v1",
        "作者": [
            "Ioannis Tsiamas",
            "Matthias Sperber",
            "Andrew Finch",
            "Sarthak Garg"
        ],
        "摘要": "The prosody of a spoken utterance, including features like stress, intonation\nand rhythm, can significantly affect the underlying semantics, and as a\nconsequence can also affect its textual translation. Nevertheless, prosody is\nrarely studied within the context of speech-to-text translation (S2TT) systems.\nIn particular, end-to-end (E2E) systems have been proposed as well-suited for\nprosody-aware translation because they have direct access to the speech signal\nwhen making translation decisions, but the understanding of whether this is\nsuccessful in practice is still limited. A main challenge is the difficulty of\nevaluating prosody awareness in translation. To address this challenge, we\nintroduce an evaluation methodology and a focused benchmark (named ContraProST)\naimed at capturing a wide range of prosodic phenomena. Our methodology uses\nlarge language models and controllable text-to-speech (TTS) to generate\ncontrastive examples. Through experiments in translating English speech into\nGerman, Spanish, and Japanese, we find that (a) S2TT models possess some\ninternal representation of prosody, but the prosody signal is often not strong\nenough to affect the translations, (b) E2E systems outperform cascades of\nspeech recognition and text translation systems, confirming their theoretical\nadvantage in this regard, and (c) certain cascaded systems also capture\nprosodic information in the translation, but only to a lesser extent that\ndepends on the particulars of the transcript's surface form.",
        "分类": [
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "补充信息": "WMT 2024",
        "日期": "2024-10-31T15:20:50+00:00",
        "概述": "这项研究旨在探讨语音翻译系统是否能够利用语音语调（如重音、语调和节奏）来改善翻译质量。为了解决现有系统对语音语调处理不充分的问题，研究引入了一种评估方法和基准（ContraProST），使用大规模语言模型和可控文本转语音生成对比例句。实验结果显示，尽管语音翻译模型内部存在一定的语音语调表示，但其影响有限；端到端系统优于级联系统；部分级联系统也能捕捉到语调信息，但效果受转录表面形式影响。",
        "摘要译文": "口语表达的音韵特征，如重音、语调和节奏，可以显著影响其语义底层，进而也会影响其文本翻译。尽管如此，音韵特征在语音转文本系统中的研究仍很少。特别是，端到端（E2E）系统被认为非常适合音韵意识翻译，因为它们在做翻译决策时可以直接访问语音信号，但关于这一点在实际应用中是否成功仍然缺乏了解。一个主要挑战是评估翻译中的音韵意识困难重重。为了应对这一挑战，我们引入了一种评估方法和一个专注于捕捉广泛音韵现象的重点基准（名为ContraProST）。我们的方法使用大型语言模型和可控的文本转语音（TTS）生成对比样例。通过将英语口语翻译成德语、西班牙语和日语的实验，我们发现：（a）语音转文本模型在某种程度上具有音韵的内部表征，但音韵信号通常不够强以致不能影响翻译；（b）端到端系统优于级联的语音识别和文本翻译系统，证实了它们在这一方面的理论优势；（c）某些级联系统也在翻译中捕捉到了音韵信息，但这种捕捉程度取决于转录表层形式的具体细节。"
    },
    {
        "序号": 23,
        "标题": "Representative Social Choice: From Learning Theory to AI Alignment",
        "链接": "http://arxiv.org/abs/2410.23953v1",
        "作者": [
            "Tianyi Qiu"
        ],
        "摘要": "Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.GT"
        ],
        "补充信息": "Full version (20 pages). Under review. An excerpt was previously\n  accepted to NeurIPS 2024 Pluralistic Alignment Workshop",
        "日期": "2024-10-31T14:07:26+00:00",
        "概述": "本文探索社会选择理论在人类和人工智能决策中的应用，特别是在大规模集体决策中代表社会偏好的问题。研究提出了代表社会选择框架，以有限个个体-议题样本为基础作出社会选择决策。作者将代表社会选择中的许多深层次问题转化为统计学习问题，并利用机器学习理论证明了其泛化性质。此外，还提出了代表社会选择的公理，并使用新的组合分析工具证明了类似阿罗的不可能定理。该框架在社会选择、学习理论和AI对齐交叉领域开辟了新的研究方向。",
        "摘要译文": "社会选择理论是研究跨越整个人群的偏好聚合，既用于人类代理的机制设计，也用于语言模型的民主对齐。在本研究中，我们提出了一种代表性的社会选择框架，用于集体决策中的民主代表建模，其中议题和个体数量过多，无法直接考虑所有偏好。这种情况在现实世界中的决策过程中很普遍，如陪审团审判、间接选举、立法过程、公司治理，以及最近的语言模型对齐。在代表性社会选择中，整个人群通过基于个体-议题配对的有限样本来代表，并基于此做出社会选择决策。我们展示了代表性社会选择中许多最深刻的问题可以自然地被表述为统计学习问题，并用机器学习理论证明了社会选择机制的泛化性质。我们进一步为代表性社会选择形式化了一系列公理，并用新的组合分析工具证明了类似阿罗的不可能定理。我们的框架引入了社会选择的代表性方法，开启了社会选择、学习理论和AI对齐交叉领域的研究方向。"
    },
    {
        "序号": 22,
        "标题": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language",
        "链接": "http://arxiv.org/abs/2410.23956v1",
        "作者": [
            "Jiayi Wang",
            "Yao Lu",
            "Maurice Weber",
            "Max Ryabinin",
            "Yihong Chen",
            "Raphael Tang",
            "Pontus Stenetorp"
        ],
        "摘要": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T14:09:50+00:00",
        "概述": "该研究旨在解决非英语语言在大型语言模型（LLMs）预训练中存在的性能不足问题。通过将高质量的英语网页数据翻译成法语、德语和西班牙语，构建了一个包含300亿词的多语言数据集TransWeb-Edu，用于从零开始预训练一个1.3亿参数的模型CuatroLLM。实验结果显示，尽管使用的数据量仅为Llama3.2的1/10左右，但CuatroLLM在五个非英语推理任务中仍能匹配甚至超越最先进的模型。此外，通过少量领域特定的预训练，CuatroLLM进一步提升了多语言推理能力。",
        "摘要译文": "作为一种高资源语言，英语使得大规模语言模型（LLMs）的预训练能够生成高质量的模型。但对于大多数其他语言来说，情况并非如此，因为顶级LLMs在非英语语言上仍然表现不佳，这很可能是因为可用于多语言预训练语料库在质量和多样性上存在差距。在本项工作中，我们发现来自单一高质量源语言的机器翻译文本可以显著地为多语言LLMs的预训练做出贡献。我们将一个高质量的英语网络数据集FineWeb-Edu翻译成法语、德语和西班牙语，最终形成一个3000亿词的数据集，我们称之为TransWeb-Edu，并从头开始在该数据集上预训练了一个1.3B参数的模型CuatroLLM。在五个非英语推理任务中，我们展示了CuatroLLM即使使用比Llama3.2少一个数量级的数据（大约只使用了Llama3.2训练数据的6%的词），仍然能够与使用封闭数据训练的顶级多语言模型（如Llama3.2和Gemma2）相媲美或超越它们。进一步证明，通过额外的不到TransWeb-Edu的1%的领域特定预训练，CuatroLLM在多语言推理方面超越了现有最先进的水平。为了促进可再现性，我们以开源许可证在hf.co/britllm/CuatroLLM发布我们的语料库、模型和训练管道。"
    },
    {
        "序号": 24,
        "标题": "Language Models can Self-Lengthen to Generate Long Texts",
        "链接": "http://arxiv.org/abs/2410.23933v1",
        "作者": [
            "Shanghaoran Quan",
            "Tianyi Tang",
            "Bowen Yu",
            "An Yang",
            "Dayiheng Liu",
            "Bofei Gao",
            "Jianhong Tu",
            "Yichang Zhang",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "摘要": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T13:47:10+00:00",
        "概述": "这项研究旨在解决大语言模型在生成长文本时的局限性。传统训练方法缺乏生成长文本的有效指导，且依赖于短的数据对。研究提出了一种名为Self-Lengthen的新迭代训练框架，通过模型自身的知识和技能，无需额外数据或专有模型。该框架包含生成器和扩展器两个角色，生成器先生成初始响应，扩展器再对其进行拆分和扩展，形成更长的新响应。该方法在多个基准测试和人类评估中均优于现有方法，特别是在Qwen2和LLaMA3等顶级开源模型上表现优异。",
        "摘要译文": "最近在大型语言模型（LLMs）方面的进展显著增强了它们处理长上下文的能力，但在生成长且对齐的输出方面仍存在明显的差距。这一限制源于训练中的缺口，即预训练缺乏有效的长文本生成指令，而后续训练数据主要由短查询-响应对组成。当前的方法，如指令反向翻译和行为模仿，面临着数据质量、版权问题以及专有模型使用限制等挑战。在本文中，我们介绍了一种创新性的迭代训练框架叫做“Self-Lengthen”，该框架仅利用LLMs本身的内在知识和技能，无需辅助数据或专有模型。该框架包含两个角色：生成器和扩展器。生成器生成初始响应，然后由扩展器拆分和扩展。这一过程会产生一个更长的新响应，用于迭代训练生成器和扩展器。通过这一过程，模型逐渐被训练以处理越来越长的响应。在基准测试和人类评估中的实验表明，当应用于如Qwen2和LLaMA3等顶级开源LLM时，Self-Lengthen在长文本生成方面的表现优于现有方法。我们的代码已在https://github.com/QwenLM/Self-Lengthen 开放获取。"
    },
    {
        "序号": 25,
        "标题": "BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments",
        "链接": "http://arxiv.org/abs/2410.23918v1",
        "作者": [
            "Xinghao Wang",
            "Pengyu Wang",
            "Bo Wang",
            "Dong Zhang",
            "Yunhua Zhou",
            "Xipeng Qiu"
        ],
        "摘要": "Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-31T13:26:11+00:00",
        "概述": "本文针对大规模语言模型（LLMs）在本地设备上部署时面临的内存限制问题，提出了BitStack方法。BitStack是一种无需训练的权重压缩方法，能够在约1比特/参数的精度下，实现内存使用和模型性能之间的细粒度权衡。通过权重分解，BitStack能够动态调整模型大小，适应不同内存环境。实验结果表明，BitStack在极端压缩比下仍能匹配或超越强量化基线，且代码已公开。",
        "摘要译文": "大型语言模型（LLMs）已在众多应用程序中引发了革命，但其部署仍然受限于本地设备的内存限制。尽管扩展法则增强了LLM的能力，主要瓶颈已从“能力”转变为“可用性”，突显了高效内存管理的需求。传统的压缩方法，如量ization，通常需要预定义的压缩比率并为每个设置分别进行压缩过程，这在变内存环境中增加了部署的复杂性。在本文中，我们介绍了**BitStack**，这是一种新颖的、无需训练的权重压缩方法，能够在内存使用和模型性能之间实现兆字节级别的权衡。通过利用权重分解，BitStack可以动态调整模型大小，同时在运行内存和存储设备之间进行最少的传输。我们的方法在每个分解迭代中逐步分解权重矩阵，同时考虑每个参数的重要性，导致每个分解迭代中约有1位/参数的残差块。这些块按基本传输单元在存储中排序和堆叠，根据当前内存可用量加载不同数量。在广泛任务上的大量实验表明，尽管提供了细粒度的大小控制，但BitStack在极端压缩比率下仍然一致地匹配或超越了强大的量ization基线。据我们所知，这是首款有效弥合分解方法与如量ization等实用压缩技术之间差距的方法。代码可在https://github.com/xinghaow99/BitStack获取。"
    },
    {
        "序号": 26,
        "标题": "Responsible Retrieval Augmented Generation for Climate Decision Making from Documents",
        "链接": "http://arxiv.org/abs/2410.23902v1",
        "作者": [
            "Matyas Juhasz",
            "Kalyan Dutia",
            "Henry Franks",
            "Conor Delahunty",
            "Patrick Fawbert Mills",
            "Harrison Pim"
        ],
        "摘要": "Climate decision making is constrained by the complexity and inaccessibility\nof key information within lengthy, technical, and multi-lingual documents.\nGenerative AI technologies offer a promising route for improving the\naccessibility of information contained within these documents, but suffer from\nlimitations. These include (1) a tendency to hallucinate or mis-represent\ninformation, (2) difficulty in steering or guaranteeing properties of generated\noutput, and (3) reduced performance in specific technical domains. To address\nthese challenges, we introduce a novel evaluation framework with\ndomain-specific dimensions tailored for climate-related documents. We then\napply this framework to evaluate Retrieval-Augmented Generation (RAG)\napproaches and assess retrieval- and generation-quality within a prototype tool\nthat answers questions about individual climate law and policy documents. In\naddition, we publish a human-annotated dataset and scalable automated\nevaluation tools, with the aim of facilitating broader adoption and robust\nassessment of these systems in the climate domain. Our findings highlight the\nkey components of responsible deployment of RAG to enhance decision-making,\nwhile also providing insights into user experience (UX) considerations for\nsafely deploying such systems to build trust with users in high-risk domains.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T13:05:39+00:00",
        "概述": "这篇论文旨在解决气候决策中关键信息难以获取的问题。通过引入一个专门针对气候相关文档的评估框架，评估检索增强生成（RAG）方法的生成和检索质量。研究还发布了标注的人类数据集和自动评估工具，以促进这些系统在气候领域的更广泛采用和可靠评估。研究结果强调了负责任部署RAG的关键因素，并提供了在高风险领域安全部署此类系统的用户体验考虑，以建立用户信任。",
        "摘要译文": "气候决策制定受到冗长、技术性和多语言文档中关键信息复杂性和不可及性的限制。生成式AI技术为提高这些文档中信息的可访问性提供了前景，但存在局限性。这些局限性包括（1）倾向于产生幻觉或歪曲信息，（2）难以引导或保证生成输出的属性，以及（3）在特定技术领域表现较差。为了应对这些挑战，我们引入了一个新的评估框架，该框架针对与气候相关的文档具有特定领域维度。然后，我们应用了这一框架来评估检索增强生成（RAG）方法，并在原型工具中评估检索质量和生成质量，该工具可以回答关于个别气候法律和政策文档的问题。此外，我们发布了一个人工注释的数据集和可扩展的自动化评估工具，旨在促进这些系统在气候领域的更广泛采用和稳健评估。我们的研究结果强调了负责任部署RAG以促进决策的关键要素，同时为在高风险领域安全部署此类系统并建立用户信任提供了用户体验（UX）方面的见解。"
    },
    {
        "序号": 28,
        "标题": "Failure Modes of LLMs for Causal Reasoning on Narratives",
        "链接": "http://arxiv.org/abs/2410.23884v1",
        "作者": [
            "Khurram Yamin",
            "Shantanu Gupta",
            "Gaurav R. Ghosal",
            "Zachary C. Lipton",
            "Bryan Wilder"
        ],
        "摘要": "In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.",
        "分类": [
            "cs.LG",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T12:48:58+00:00",
        "概述": "本文研究了大规模语言模型（LLMs）在因果推理叙事方面的能力。研究发现，即使是先进的语言模型，也会依赖不可靠的捷径，无论是基于叙述的呈现还是参数知识。通过精心控制的合成实验和实际叙事评估，表明LLMs在长叙事和多事件中的长期因果推理能力较弱，并且过度依赖参数知识而忽视对提供的叙述进行推理，这在叙述与参数知识矛盾时会降低性能。研究结果揭示了当前先进模型的精确失败模式，为未来提升LLMs因果推理能力的技术铺平了道路。",
        "摘要译文": "在本文中，我们通过推理叙事中的因果关系这一代表性问题，研究了大型语言模型（LLMs）的因果推理能力。我们发现，即使是目前最先进的语言模型，在叙事呈现和参数化知识方面都依赖于不可靠的捷径。例如，LLMs往往根据事件的时间顺序（即，较早发生的事件导致较晚发生的事件）来确定因果关系，一旦事件未按其确切的因果顺序叙述，其表现就会下降。同样，我们展示了LLMs在长期因果推理方面存在困难，并且当叙事较长且包含许多事件时，它们经常失败。此外，我们证明LLMs似乎过度依赖其参数化知识，而忽视了对提供的叙事进行推理。这在叙事与参数化知识相冲突时会削弱其能力。我们通过精心控制的合成实验和对现实世界叙事的评估，广泛验证了这些失败模式。最后，我们观察到，显式生成因果图通常可以提高性能，而简单的chain-of-thought则无效。总之，我们的结果提炼出了当前最先进的模型的具体失败模式，并为进一步提升LLMs因果推理的能力提供了方向。"
    },
    {
        "序号": 27,
        "标题": "Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource languages",
        "链接": "http://arxiv.org/abs/2410.23890v1",
        "作者": [
            "Séamus Lankford",
            "Andy Way"
        ],
        "摘要": "In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "arXiv admin note: text overlap with arXiv:2403.02370,\n  arXiv:2403.01580",
        "日期": "2024-10-31T12:52:26+00:00",
        "概述": "本文研究了在危机通信场景中利用大型语言模型（LLMs）和多语言大型语言模型（MLLMs）提升机器翻译（MT）系统的能力，特别是针对低资源语言。研究旨在解决危机情境下的紧迫需求，即高效率、高准确性和多语言处理能力。方法上，研究结合了LLM前沿技术和定制化微调策略，并通过社区驱动的数据集开发，针对两种低资源语言对开展了实证研究。结果表明，微调过的MLLM模型在性能上优于未微调的LLM模型。研究提出了在危机场景下快速开发多语言通信系统的可扩展、可复制模型，为应急情况下的多语言交流系统开发提供了蓝本。",
        "摘要译文": "在不断演变的危机通信 landscape 中，稳健且灵活的机器翻译 (MT) 系统的需求比以往更为迫切，特别是在低资源语言方面。本研究全面探讨了利用大型语言模型 (LLM) 和多语言大型语言模型 (MLLM) 来增强这些场景中的 MT 能力。通过聚焦于危机情况下速度、准确性和处理多种语言范围能力至关重要的独特挑战，本研究概述了一种新颖的方法，将 LLM 的尖端能力与微调技术和社区驱动的语料库开发策略结合起来。本研究的核心在于开发并实证评估了针对两种低资源语言对定制的 MT 系统，从初始模型选择和微调到部署的过程进行了展示。这些系统是基于最近的 COVID-19 大流行进行开发和建模的。研究强调了社区参与在创建高度专业化、危机特定数据集方面的重要性，并将自定义 GPT 与 NLLB 调整的 MLLM 模型进行了比较。研究发现，微调的 MLLM 模型在性能上优于与其对应的 LLM。本文概述了一个可扩展且可复制的快速 MT 系统开发框架，适用于危机情境。我们的方法通过提供在紧急情况下开发多语言通信系统的蓝图，提升了人道主义技术的领域。"
    },
    {
        "序号": 29,
        "标题": "'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue",
        "链接": "http://arxiv.org/abs/2410.23883v1",
        "作者": [
            "Rena Gao",
            "Xuetong Wu",
            "Siwen Luo",
            "Caren Han",
            "Feng Liu"
        ],
        "摘要": "Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.MM"
        ],
        "补充信息": "16 pages, 5 figures",
        "日期": "2024-10-31T12:45:54+00:00",
        "概述": "该研究旨在改进多轮长对话系统的用户体验，通过有效检测多模态下的离分布对话和图像，解决开放域对话系统或现实生活对话中多模态输入偏差的问题。研究提出了一种名为Dialogue Image Aligning and Enhancing Framework（DIAEF）的新颖评分框架，结合视觉语言模型和新型离分布评分，以检测对话-图像配对和未知标签输入对中的偏差。实验结果表明，结合多轮对话和图像的离分布检测比单一模态更有效，特别是在检测不匹配配对时表现出了强大的鲁棒性，提升了领域感知的对话代理，并为未来研究奠定了基础。",
        "摘要译文": "在多模态上下文中进行离分布（OOD）检测对于识别来自不同模态的综合输入中的偏差，在开放域对话系统或现实生活对话交互等应用中尤为重要。本文旨在通过高效检测OOD对话和图像来改善涉及多轮对话的用户体验。我们引入了一种名为对话图像对齐与增强框架（DIAEF）的新颖评分框架，该框架将视觉语言模型与新颖提出的用于两种关键场景中的OOD检测的评分集成在一起（1）对话与图像输入对之间的不匹配；（2）带有先前未见过的标签的输入对。我们从各种基准得出的实验结果表明，将图像和多轮对话的OOD检测结合起来，在未见过的标签情况下比单独使用任一模态更有效。在存在不匹配对的情况下，我们提出的新颖评分有效地识别了这些不匹配，并在长对话中表现出强大的鲁棒性。此方法增强了领域感知的自适应对话代理，并为未来的研究所建立基线。"
    },
    {
        "序号": 30,
        "标题": "Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models",
        "链接": "http://arxiv.org/abs/2410.23861v1",
        "作者": [
            "Hao Yang",
            "Lizhen Qu",
            "Ehsan Shareghi",
            "Gholamreza Haffari"
        ],
        "摘要": "Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.",
        "分类": [
            "cs.CL",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "补充信息": null,
        "日期": "2024-10-31T12:11:17+00:00",
        "概述": "该研究针对先进音频大型多模态模型（Audio LMMs）的安全性进行了红队测试，发现这些模型在处理有潜在危害的音频和文本问题时存在安全漏洞，尤其是在被非言语音频噪音干扰时。研究选取五个音频 LMMs，在三种场景下进行测试，结果显示开源音频 LMMs 的有害音频问题攻击成功率平均为 69.14%。此外，特定语音破解攻击在 Gemini-1.5-Pro 上的成功率为 70.67%。研究揭示了潜在的安全偏差原因。",
        "摘要译文": "大型多模态模型（LMMs）通过将大规模语言模型（LLMs）与模态编码器结合，以协同视觉和听觉等多种模态信息与文本，能够在现实条件下与人类互动。然而，这类模型提出了新的安全挑战：即在文本上安全对齐的模型是否也能对多模态输入表现出一致的安全防护措施。尽管最近对视觉LMMs进行了安全对齐研究，音频LMMs的安全性仍受到忽视。在本工作中，我们分别在三种场景下对五种先进的音频LMMs进行全面的安全红队测试：（i）以音频和文本格式提出的有害问题，（ii）以文本格式提出的有害问题伴随干扰性非言语音频，以及（iii）针对语音的特别脱管攻击。在这些场景下的测试结果表明，开源音频LMMs在有害音频问题上的平均攻击成功率达到了69.14%，并且在受到非言语音频噪声干扰时显示出安全性漏洞。我们对Gemini-1.5-Pro的特别语音脱管攻击在有害查询基准上的攻击成功率达到70.67%。我们提供了可能导致这些报告的安全错位的原因的见解。注意：这篇论文包含冒犯性的例子。"
    },
    {
        "序号": 31,
        "标题": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
        "链接": "http://arxiv.org/abs/2410.23856v1",
        "作者": [
            "Zhanke Zhou",
            "Rong Tao",
            "Jianing Zhu",
            "Yiwen Luo",
            "Zengmao Wang",
            "Bo Han"
        ],
        "摘要": "This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": "Accepted by NeurIPS 2024",
        "日期": "2024-10-31T12:07:44+00:00",
        "概述": "该研究探讨了大语言模型在存在嘈杂推理的情况下进行链式推理的脆弱性。研究构建了NoRa数据集来评估模型面对嘈杂推理时的鲁棒性。实验发现当前模型在含有无关或不准确推理的线索时表现较差，准确率降低幅度达1.4%到40.4%。研究提出了对比降噪方法CD-CoT，该方法通过对比嘈杂和清洁推理线索来增强模型的降噪和推理能力，结果显示该方法能显著提升模型性能，平均提高17.8%的准确率。",
        "摘要译文": "本文探讨了大型语言模型（LLM）中一个未充分研究的挑战：包含无关或不准确推理的链式思考提示。我们构建了NoRa数据集，用于评估在存在噪声推理时的推理鲁棒性。NoRa数据集上的研究结果揭示了当前LLM普遍存在对这种噪声的脆弱性，现有的鲁棒方法如自我修正和自我一致性显示出有限的效果。值得注意的是，与使用干净推理相比，基线LLM在包含无关思考时准确率下降1.4%-19.8%，而在包含不准确思考时则下降2.2%-40.4%。\n\n解决这一挑战需要在实践中可获取的外部监督。为此，我们提出了一种带有噪声链式思考对比消噪方法（CD-CoT）。该方法通过仅使用一个干净推理与噪声推理进行对比，增强LLM的消噪推理能力，这可以作为消噪提示的最低要求。该方法遵循探索与利用的原则：（1）在输入空间中重新表述和选择推理，以实现明确的消噪；（2）在输出空间中探索不同的推理路径并投票选定答案。实验结果表明，CD-CoT在准确率上平均提高了17.8%，并在消噪能力上显著优于基准方法。源代码已公开发布于：https://github.com/tmlr-group/NoisyRationales。"
    },
    {
        "序号": 32,
        "标题": "The Automated Verification of Textual Claims (AVeriTeC) Shared Task",
        "链接": "http://arxiv.org/abs/2410.23850v1",
        "作者": [
            "Michael Schlichtkrull",
            "Yulong Chen",
            "Chenxi Whitehouse",
            "Zhenyun Deng",
            "Mubashara Akhtar",
            "Rami Aly",
            "Zhijiang Guo",
            "Christos Christodoulopoulos",
            "Oana Cocarascu",
            "Arpit Mittal",
            "James Thorne",
            "Andreas Vlachos"
        ],
        "摘要": "The Automated Verification of Textual Claims (AVeriTeC) shared task asks\nparticipants to retrieve evidence and predict veracity for real-world claims\nchecked by fact-checkers. Evidence can be found either via a search engine, or\nvia a knowledge store provided by the organisers. Submissions are evaluated\nusing AVeriTeC score, which considers a claim to be accurately verified if and\nonly if both the verdict is correct and retrieved evidence is considered to\nmeet a certain quality threshold. The shared task received 21 submissions, 18\nof which surpassed our baseline. The winning team was TUDA_MAI with an AVeriTeC\nscore of 63%. In this paper we describe the shared task, present the full\nresults, and highlight key takeaways from the shared task.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T12:01:12+00:00",
        "概述": "该研究通过AVeriTeC共享任务，旨在自动验证文本声明的真实性，利用搜索引擎或主办方提供的知识库检索证据，并评估声明的真实性。参赛团队需预测声明的真假并提供相关证据。研究采用AVeriTeC评分进行评估，要求判断准确且证据质量达标。共有21个团队提交，18个团队超越了 baseline，TUDA_MAI团队获得最高分63%。本研究解决了事实核查自动化问题，共有21个团队参加，表现最好的团队达到63%的高准确率。",
        "摘要译文": "自动文本声明的验证（AVeriTeC）共享任务要求参与者检索证据并预测事实核查人员检查的实际声明的真实性。证据可以通过搜索引擎找到，也可以通过组织者提供的知识库找到。提交的作品将根据AVeriTeC评分进行评估，该评分只在裁决正确且检索到的证据符合一定质量标准时才认为声明得到了准确验证。共享任务收到了21份提交的作品，其中有18份超过了我们的 baseline。获胜团队是TUDA_MAI，得分为63%。在这篇论文中，我们描述了共享任务，展示了完整的结果，并突出显示了共享任务中的关键要点。"
    },
    {
        "序号": 33,
        "标题": "Commonsense Knowledge Editing Based on Free-Text in LLMs",
        "链接": "http://arxiv.org/abs/2410.23844v1",
        "作者": [
            "Xiusheng Huang",
            "Yequan Wang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "摘要": "Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "11 pages, 8 figures",
        "日期": "2024-10-31T11:50:24+00:00",
        "概述": "本文探讨了基于LLMs的知识编辑技术，旨在提高大型语言模型的准确性和及时性。传统方法主要针对单个单词或实体进行知识编辑，不适用于自由文本中的常识知识。为此，作者提出了Knowledge Localization for Free-Text (KLFT) 方法和 Dynamics-aware Editing Method (DEM)，前者揭示了常识知识在MLP和Attention层中的分布挑战，后者利用动态感知模块定位与常识知识对应的参数位置，并通过知识编辑模块更新知识，从而成功编辑自由文本中的常识知识。实验结果表明，DEM方法在编辑性能方面表现出色。",
        "摘要译文": "知识编辑技术对于保持大型语言模型（LLMs）的准确性和及时性至关重要。然而，这一任务的设定忽视了现实世界中基于自由文本的常识知识，这种知识具有广泛的知识范围、长的内容和非即时性特征。之前的方法（如MEMIT）的编辑对象通常是单个词或实体，不适用于自由文本形式的常识知识。为了解决上述挑战，我们从两个角度进行了实验：知识定位和知识编辑。首先，我们引入了Free-Text知识定位方法（KLFT），揭示了常识知识在MLP和注意力层以及去中心化分布中的分布挑战。接下来，我们提出了感知动态编辑方法（DEM），该方法利用感知动态模块定位与常识知识对应的参数位置，并使用知识编辑模块更新知识。DEM方法充分探索了MLP和注意力层的潜力，并成功基于自由文本编辑了常识知识。实验结果表明，DEM可以获得优异的编辑性能。"
    },
    {
        "序号": 34,
        "标题": "Reasons and Solutions for the Decline in Model Performance after Editing",
        "链接": "http://arxiv.org/abs/2410.23843v1",
        "作者": [
            "Xiusheng Huang",
            "Jiaxiang Liu",
            "Yequan Wang",
            "Kang Liu"
        ],
        "摘要": "Knowledge editing technology has received widespread attention for low-cost\nupdates of incorrect or outdated knowledge in large-scale language models.\nHowever, recent research has found that edited models often exhibit varying\ndegrees of performance degradation. The reasons behind this phenomenon and\npotential solutions have not yet been provided. In order to investigate the\nreasons for the performance decline of the edited model and optimize the\nediting method, this work explores the underlying reasons from both data and\nmodel perspectives. Specifically, 1) from a data perspective, to clarify the\nimpact of data on the performance of editing models, this paper first\nconstructs a Multi-Question Dataset (MQD) to evaluate the impact of different\ntypes of editing data on model performance. The performance of the editing\nmodel is mainly affected by the diversity of editing targets and sequence\nlength, as determined through experiments. 2) From a model perspective, this\narticle explores the factors that affect the performance of editing models. The\nresults indicate a strong correlation between the L1-norm of the editing model\nlayer and the editing accuracy, and clarify that this is an important factor\nleading to the bottleneck of editing performance. Finally, in order to improve\nthe performance of the editing model, this paper further proposes a Dump for\nSequence (D4S) method, which successfully overcomes the previous editing\nbottleneck by reducing the L1-norm of the editing layer, allowing users to\nperform multiple effective edits and minimizing model damage. Our code is\navailable at https://github.com/nlpkeg/D4S.",
        "分类": [
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": "14 pages, 8 figures",
        "日期": "2024-10-31T11:49:44+00:00",
        "概述": "该论文旨在研究模型编辑后性能下降的原因并提出解决方案。研究发现编辑数据的多样性和序列长度会影响编辑模型的性能，同时编辑层的L1范数与编辑精度密切相关。为此，论文提出了一种名为Dump for Sequence (D4S)的方法，通过降低编辑层的L1范数来提高编辑效果，减少了模型损伤，支持多次有效编辑。",
        "摘要译文": "知识编辑技术因其实现大规模语言模型中低成本更新错误或过期知识而受到了广泛关注。然而，近期的研究发现，编辑后的模型常常表现出不同程度的性能下降。这种现象背后的原因以及潜在的解决方法尚未得到说明。为了探讨编辑模型性能下降的原因并优化编辑方法，本研究从数据和模型两个角度探索了其根本原因。具体来说，1）从数据角度来看，为了阐明数据对编辑模型性能的影响，本文首先构建了一个多问题数据集（MQD），以评估不同类型编辑数据对模型性能的影响。实验结果显示，编辑模型的性能主要受编辑目标多样性和序列长度的影响。2）从模型角度来看，本文研究了影响编辑模型性能的因素。结果表明，编辑模型层的L1-范数与编辑准确率之间存在很强的相关性，并且明确了这是导致编辑性能瓶颈的重要因素。最后，为了提高编辑模型的性能，本文进一步提出了序列降维（D4S）方法，通过降低编辑层的L1-范数成功克服了之前的编辑瓶颈，使得用户能够进行多次有效的编辑，同时将模型受损降至最低。我们的代码可在https://github.com/nlpkeg/D4S获取。"
    },
    {
        "序号": 35,
        "标题": "GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages",
        "链接": "http://arxiv.org/abs/2410.23825v1",
        "作者": [
            "Amir Hossein Kargaran",
            "François Yvon",
            "Hinrich Schütze"
        ],
        "摘要": "The need for large text corpora has increased with the advent of pretrained\nlanguage models and, in particular, the discovery of scaling laws for these\nmodels. Most available corpora have sufficient data only for languages with\nlarge dominant communities. However, there is no corpus available that (i)\ncovers a wide range of minority languages; (ii) is generated by an open-source\nreproducible pipeline; and (iii) is rigorously cleaned from noise, making it\ntrustworthy to use. We present GlotCC, a clean, document-level, 2TB general\ndomain corpus derived from CommonCrawl, covering more than 1000 languages. We\nmake GlotCC and the system used to generate it - including the pipeline,\nlanguage identification model, and filters - available to the research\ncommunity. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1,\nPipeline v. 3.0 https://github.com/cisnlp/GlotCC.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "NeurIPS 2024",
        "日期": "2024-10-31T11:14:12+00:00",
        "概述": "该研究旨在解决现有语料库主要针对大型主流语言的问题，推出了一个被称为GlotCC的开放源代码、可复现的、高质量的多语言语料库，涵盖1000多种少数民族语言，总量达2TB。研究团队通过CommonCrawl数据集构建了该语料库，并提供了一个包含管道、语言识别模型和过滤器在内的系统。 deutschenederuediger",
        "摘要译文": "随着预训练语言模型的出现，尤其是这些模型的规模定律被发现后，对大规模文本语料库的需求不断增加。大多数可用的语料库只包含了拥有庞大主导社群的语言的数据。然而，目前没有一个语料库能满足以下条件：(i) 覆盖广泛的小众语言；(ii) 由开源可复现的工作流程生成；(iii) 严格去除了噪音，使其可信可用。我们介绍了GlotCC，这是一个来源于CommonCrawl的、干净的、文档级别的、约2TB的通用领域语料库，涵盖了超过1000种语言。我们向研究界提供了GlotCC及其生成系统——包括工作流程、语言识别模型和过滤器——的相关资源。语料库版本1.0：https://huggingface.co/datasets/cis-lmu/GlotCC-v1，工作流程版本3.0：https://github.com/cisnlp/GlotCC。"
    },
    {
        "序号": 36,
        "标题": "What is Wrong with Perplexity for Long-context Language Modeling?",
        "链接": "http://arxiv.org/abs/2410.23771v1",
        "作者": [
            "Lizhe Fang",
            "Yifei Wang",
            "Zhaoyang Liu",
            "Chenheng Zhang",
            "Stefanie Jegelka",
            "Jinyang Gao",
            "Bolin Ding",
            "Yisen Wang"
        ],
        "摘要": "Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-31T09:39:28+00:00",
        "概述": "这项研究旨在评估大型语言模型（LLM）在处理长上下文输入时的能力。传统上使用 perplexity（PPL）作为评价指标，但研究发现PPL在长上下文场景中表现不稳定。为此，研究者提出了一种新的评价指标 LongPPL，以及一个新的Loss函数 LongCE，通过关注关键令牌来改进模型在长上下文任务中的性能。实验结果表明，LongPPL和LongCE相比传统PPL更具预测准确性，能有效提升LLM的长上下文能力。",
        "摘要译文": "处理长上下文输入对于大型语言模型（LLMs）在扩展对话、文档摘要和多轮次上下文学习等任务中至关重要。虽然最近的方法扩展了LLMs的上下文窗口，并采用了困惑度（PPL）作为标准评估指标，但PPL已被证明在评估长上下文能力方面不可靠。这一局限性的根本原因仍不清楚。在本工作中，我们提供了一个全面的解释。我们发现，PPL通过所有令牌的平均值忽略了关键令牌，这些关键令牌对于长上下文理解至关重要，从而掩盖了模型在长上下文场景中的真实性能。为了解决这个问题，我们提出了一种名为\\textbf{LongPPL}的新指标，通过使用长短上下文对比方法来识别关键令牌。我们的实验表明，LongPPL与各种长上下文基准测试的性能强烈相关（例如，皮尔逊相关系数为-0.96），在预测准确性方面显著优于传统的PPL。此外，我们引入了一种名为\\textbf{LongCE}（长上下文交叉熵）损失的重新加权策略，优先考虑关键令牌，从而在各种基准测试中表现出一致的改进。综上所述，这些贡献提供了对PPL局限性的更深入理解，并提出了有效的方法来准确评估和提升LLMs的长上下文能力。代码可在https://github.com/PKU-ML/LongPPL 获取。"
    },
    {
        "序号": 37,
        "标题": "The Potential of LLMs in Medical Education: Generating Questions and Answers for Qualification Exams",
        "链接": "http://arxiv.org/abs/2410.23769v1",
        "作者": [
            "Yunqi Zhu",
            "Wen Tang",
            "Ying Sun",
            "Xuebing Yang"
        ],
        "摘要": "Recent research on large language models (LLMs) has primarily focused on\ntheir adaptation and application in specialized domains. The application of\nLLMs in the medical field is mainly concentrated on tasks such as the\nautomation of medical report generation, summarization, diagnostic reasoning,\nand question-and-answer interactions between doctors and patients. The\nchallenge of becoming a good teacher is more formidable than that of becoming a\ngood student, and this study pioneers the application of LLMs in the field of\nmedical education. In this work, we investigate the extent to which LLMs can\ngenerate medical qualification exam questions and corresponding answers based\non few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic\ndiseases, we tasked the LLMs with generating open-ended questions and answers\nbased on a subset of sampled admission reports across eight widely used LLMs,\nincluding ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and\nMistral. Furthermore, we engaged medical experts to manually evaluate these\nopen-ended questions and answers across multiple dimensions. The study found\nthat LLMs, after using few-shot prompts, can effectively mimic real-world\nmedical qualification exam questions, whereas there is room for improvement in\nthe correctness, evidence-based statements, and professionalism of the\ngenerated answers. Moreover, LLMs also demonstrate a decent level of ability to\ncorrect and rectify reference answers. Given the immense potential of\nartificial intelligence in the medical field, the task of generating questions\nand answers for medical qualification exams aimed at medical students, interns\nand residents can be a significant focus of future research.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": null,
        "日期": "2024-10-31T09:33:37+00:00",
        "概述": "本文研究了大规模语言模型（LLMs）在医学教育中的应用，特别是生成医学资格考试题和答案的能力。研究利用中国老年慢性病真实数据集，让多个LLM模型生成开放性问题及其答案，并由医学专家评估。结果表明，经过少量示例提示后，LLM能有效模拟实际医学资格考试题目，但在答案的准确性和专业性方面有待提高。此外，LLMs还能较好地修正参考答案。研究指出，未来医学领域可将重点放在生成医学资格考试题和答案上。",
        "摘要译文": "近期对大型语言模型（LLMs）的研究主要集中在它们在专业领域中的适应和应用。在医疗领域的应用主要集中在自动医疗报告生成、总结、诊断推理以及医生与患者之间的问答互动等方面。成为一位好老师比成为一位好学生更具挑战性，而本研究首次在医疗教育领域应用了LLMs。在本项工作中，我们研究了LLMs在最少示例提示下生成医疗资格考试问题及其对应答案的能力。我们利用了一个真实的中国老年慢性病数据集，要求八款常用的LLMs（包括ERNIE 4、ChatGLM 4、Doubao、Hunyuan、Spark 4、Qwen、Llama 3和Mistral）生成开放性问题及其答案。此外，我们还邀请了医学专家从多个维度手动评估这些开放性问题及其答案。研究发现，经过最少示例提示后，LLMs能够有效模拟真实的医疗资格考试问题，但在生成答案的正确性、基于证据的陈述和专业性方面仍有改进空间。此外，LLMs还显示出一定的纠正和修正参考答案的能力。鉴于人工智能在医疗领域的巨大潜力，未来研究可以将生成针对医学学生、实习医生和住院医生的资格考试问题和答案任务作为重点。"
    },
    {
        "序号": 38,
        "标题": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios",
        "链接": "http://arxiv.org/abs/2410.23746v1",
        "作者": [
            "Junchao Wu",
            "Runzhe Zhan",
            "Derek F. Wong",
            "Shu Yang",
            "Xinyi Yang",
            "Yulin Yuan",
            "Lidia S. Chao"
        ],
        "摘要": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating advanced prompt usages, human\nrevisions like word substitutions, and writing errors. Our development of\nDetectRL reveals the strengths and limitations of current SOTA detectors. More\nimportantly, we analyzed the potential impact of writing styles, model types,\nattack methods, the text lengths, and real-world human writing factors on\ndifferent types of detectors. We believe DetectRL could serve as an effective\nbenchmark for assessing detectors in real-world scenarios, evolving with\nadvanced attack methods, thus providing more stressful evaluation to drive the\ndevelopment of more efficient detectors. Data and code are publicly available\nat: https://github.com/NLP2CT/DetectRL.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "Accepted to NeurIPS 2024 Dataset & Benchmarking Track",
        "日期": "2024-10-31T09:01:25+00:00",
        "概述": "本文旨在评估大语言模型（LLMs）生成文本的检测能力。研究发现现有最先进的检测技术在真实场景下表现不佳。通过创建结合heuristic规则的对抗性LLM生成文本，模拟实际应用场景中的各种因素，该研究揭示了当前检测技术的优缺点，并分析了写作风格、模型类型、攻击方法、文本长度等因素对检测效果的影响。研究开发了一个新基准DetectRL，旨在促进更有效的检测器开发，并提供了更严格的评估。",
        "摘要译文": "检测大型语言模型（LLMs）生成的文本近期引起了极大的兴趣。利用零样本方法如DetectGPT，检测能力已经达到了令人印象深刻的高度。然而，现有检测器在真实世界应用中的可靠性仍然未被充分研究。在这项研究中，我们提出了一个新的基准DetectRL，发现即使是最先进的检测技术在这项任务中也表现不佳。我们从LLMs容易被滥用的领域收集了人类撰写的数据集。使用流行的LLMs生成了更符合实际应用的数据。与以往研究不同，我们使用启发式规则创建了对抗性LLM生成的文本，模拟了高级提示使用、单词替换等人类修订和写作错误。DetectRL的开发揭示了当前最先进检测器的优势和局限性。更重要的是，我们分析了写作风格、模型类型、攻击方法、文本长度以及真实世界的写作因素对不同类型检测器的影响。我们认为，DetectRL可以作为一个有效的基准，用于评估检测器在实际场景中的表现，并随着高级攻击方法的发展不断进步，从而推动更高效的检测器的研发。数据和代码已公开发布在：https://github.com/NLP2CT/DetectRL。"
    },
    {
        "序号": 39,
        "标题": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective",
        "链接": "http://arxiv.org/abs/2410.23743v1",
        "作者": [
            "Ming Li",
            "Yanhong Li",
            "Tianyi Zhou"
        ],
        "摘要": "What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-31T08:58:06+00:00",
        "概述": "该研究探讨了大型语言模型（LLMs）在不同训练响应和初始模型下各层的训练模式，重点关注快速思维（无链式思考）与慢速思维（详细链式思考）对层间梯度的影响。研究发现，慢速思维导致的梯度更稳定，差异较小，而快速思维导致更大梯度差异。经过预训练的LLMs比指令微调的LLMs对外部不稳定因素更不敏感。慢速思维的梯度能区分正确和无关推理路径，但非推理知识学习任务中，增加响应长度对梯度行为无明显影响。研究揭示了LLM训练的基本原理，提升了模型效率和稳定性，为构建通用的“系统2”代理奠定了基础。",
        "摘要译文": "在后训练LLMs时，是什么因素起了决定性作用？我们通过梯度的视角，研究了在使用不同回应和初始模型训练时，大型语言模型（LLMs）各层的训练模式。我们特别关注快速思考与慢速思考如何影响分层梯度，特别是在采用链式思考（CoT）和过程奖励训练LLMs的背景下。在我们的研究中，未采用CoT的快速思考导致了更大的梯度和更大的层间梯度差异，这表明后者带来了一定的学习稳定性。此外，预训练的LLMs相比于指令调优的LLMs，对快速思考的不稳定性影响较小。另外，我们还研究了在使用快速思考与慢速思考路径训练不同LLMs时，梯度模式能否反映回应的正确性。结果显示，慢速思考的梯度能够区分正确的和无关的推理路径。作为对比，我们在非推理知识学习任务上进行了类似的梯度分析，但简单增加响应长度并不能产生类似慢速思考的行为。我们的研究增强了对LLM训练基本理解，并为其实效性和稳定性的新见解铺平了道路，从而朝着构建一个可泛化的System-2代理迈出一步。我们的代码、数据和梯度统计可以在以下链接找到：\nhttps://github.com/MingLiiii/Layer_Gradient."
    },
    {
        "序号": 40,
        "标题": "GigaCheck: Detecting LLM-generated Content",
        "链接": "http://arxiv.org/abs/2410.23728v1",
        "作者": [
            "Irina Tolstykh",
            "Aleksandra Tsybina",
            "Sergey Yakubson",
            "Aleksandr Gordeev",
            "Vladimir Dokholyan",
            "Maksim Kuprashevich"
        ],
        "摘要": "With the increasing quality and spread of LLM-based assistants, the amount of\nartificially generated content is growing rapidly. In many cases and tasks,\nsuch texts are already indistinguishable from those written by humans, and the\nquality of generation tends to only increase. At the same time, detection\nmethods are developing more slowly, making it challenging to prevent misuse of\nthese technologies.\n  In this work, we investigate the task of generated text detection by\nproposing the GigaCheck. Our research explores two approaches: (i)\ndistinguishing human-written texts from LLM-generated ones, and (ii) detecting\nLLM-generated intervals in Human-Machine collaborative texts. For the first\ntask, our approach utilizes a general-purpose LLM, leveraging its extensive\nlanguage abilities to fine-tune efficiently for the downstream task of\nLLM-generated text detection, achieving high performance even with limited\ndata. For the second task, we propose a novel approach that combines computer\nvision and natural language processing techniques. Specifically, we use a\nfine-tuned general-purpose LLM in conjunction with a DETR-like detection model,\nadapted from computer vision, to localize artificially generated intervals\nwithin text.\n  We evaluate the GigaCheck on five classification datasets with English texts\nand three datasets designed for Human-Machine collaborative text analysis. Our\nresults demonstrate that GigaCheck outperforms previous methods, even in\nout-of-distribution settings, establishing a strong baseline across all\ndatasets.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "11 pages, 1 figure",
        "日期": "2024-10-31T08:30:55+00:00",
        "概述": "该研究旨在识别由语言模型生成的内容，以应对生成内容质量提高和应用范围扩大的问题。研究提出了GigaCheck，采用两种方法：一是通过微调通用语言模型区分人类和模型生成的文本；二是结合计算机视觉和自然语言处理技术，在人机协作文本中定位人工生成的段落。实验结果显示，GigaCheck在多种数据集上表现优于现有方法，特别是在分布外情况下的表现更为突出。",
        "摘要译文": "随着基于LLM的助手的质量和普及率不断提高，人工生成的内容数量正在迅速增长。在很多情况下和任务中，这些文本已经难以与人类编写的内容区分开来，生成的质量也在不断提高。与此同时，检测方法的进步相对较慢，使得防止这些技术被误用变得颇具挑战性。\n\n在本项研究中，我们探讨了生成文本检测的任务，并提出了GigaCheck。我们的研究探索了两种方法：（i）区分人类编写的文本与LLM生成的文本；（ii）在人类-机器协作文本中检测LLM生成的段落。对于第一项任务，我们的方法利用了一种通用的LLM，通过利用其广泛的语言能力高效地微调以适应下游任务——生成文本检测，即使在数据有限的情况下也能实现高性能。对于第二项任务，我们提出了一种新颖的方法，结合了计算机视觉和自然语言处理技术。具体而言，我们使用了一种微调过的通用LLM与一个类似于DETR的检测模型相结合，将该模型从计算机视觉领域适配过来，用于在文本中定位人工生成的段落。\n\n我们使用包含五个英文文本分类数据集和三个用于人类-机器协作文本分析的数据集对GigaCheck进行了评估。我们的结果表明，GigaCheck在各种数据集中的表现优于之前的方法，即使在未知分布场景中也表现出色，为所有数据集建立了一个强大的基线。"
    },
    {
        "序号": 41,
        "标题": "Artificial intelligence to improve clinical coding practice in Scandinavia: a crossover randomized controlled trial",
        "链接": "http://arxiv.org/abs/2410.23725v1",
        "作者": [
            "Taridzo Chomutare",
            "Therese Olsen Svenning",
            "Miguel Ángel Tejedor Hernández",
            "Phuong Dinh Ngo",
            "Andrius Budrionis",
            "Kaisa Markljung",
            "Lill Irene Hind",
            "Torbjørn Torsvik",
            "Karl Øyvind Mikalsen",
            "Aleksandar Babic",
            "Hercules Dalianis"
        ],
        "摘要": "\\textbf{Trial design} Crossover randomized controlled trial. \\textbf{Methods}\nAn AI tool, Easy-ICD, was developed to assist clinical coders and was tested\nfor improving both accuracy and time in a user study in Norway and Sweden.\nParticipants were randomly assigned to two groups, and crossed over between\ncoding complex (longer) texts versus simple (shorter) texts, while using our\ntool versus not using our tool. \\textbf{Results} Based on Mann-Whitney U test,\nthe median coding time difference for complex clinical text sequences was 123\nseconds (\\emph{P}\\textless.001, 95\\% CI: 81 to 164), representing a 46\\%\nreduction in median coding time when our tool is used. There was no significant\ntime difference for simpler text sequences. For coding accuracy, the\nimprovement we noted for both complex and simple texts was not significant.\n\\textbf{Conclusions} This study demonstrates the potential of AI to transform\ncommon tasks in clinical workflows, with ostensible positive impacts on work\nefficiencies for complex clinical coding tasks. Further studies within hospital\nworkflows are required before these presumed impacts can be more clearly\nunderstood.",
        "分类": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.LG"
        ],
        "补充信息": "13 pages, 4 figures, 4 tables",
        "日期": "2024-10-31T08:24:37+00:00",
        "概述": "这项研究通过开发一款名为Easy-ICD的人工智能工具，旨在改善临床编码工作。研究采用交叉随机对照试验设计，在挪威和瑞典进行用户测试，比较使用该工具与不使用该工具在处理复杂和简单文本时的编码时间和准确性。结果显示，使用该工具处理复杂文本时，编码时间显著减少了46%，但对简单文本无显著影响，且在准确性方面没有显著提升。研究认为AI可能有助于提高复杂临床编码任务的工作效率，但未来还需要在医院工作流程中进行更多研究以验证其影响。",
        "摘要译文": "\\textbf{试验设计} 交叉随机对照试验。 \\textbf{方法} 开发了一种AI工具Easy-ICD，以辅助临床编码人员，并在挪威和瑞典进行了用户研究，以提高准确性和效率。参与者被随机分配到两个组，并交替使用该工具编码复杂（较长）文本和简单（较短）文本。 \\textbf{结果} 基于Mann-Whitney U检验，复杂临床文本序列的平均编码时间差异为123秒（P<0.001，95% CI：81至164），使用该工具时中位编码时间减少了46%。对于简单文本序列，没有显著的时间差异。在编码准确性方面，对复杂和简单文本的改进均不具有统计学意义。 \\textbf{结论} 本研究展示了AI在临床工作流中执行常见任务的潜力，尤其是在复杂临床编码任务方面可能对工作效率产生积极影响。在医院工作流程中进行进一步研究，才能更清晰地理解这些假设的影响。"
    },
    {
        "序号": 43,
        "标题": "Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction",
        "链接": "http://arxiv.org/abs/2410.23692v1",
        "作者": [
            "Peizhi Tang",
            "Chuang Yang",
            "Tong Xing",
            "Xiaohang Xu",
            "Renhe Jiang",
            "Kaoru Sezaki"
        ],
        "摘要": "Human mobility prediction plays a critical role in applications such as\ndisaster response, urban planning, and epidemic forecasting. Traditional\nmethods often rely on designing crafted, domain-specific models, and typically\nfocus on short-term predictions, which struggle to generalize across diverse\nurban environments. In this study, we introduce Llama-3-8B-Mob, a large\nlanguage model fine-tuned with instruction tuning, for long-term citywide\nmobility prediction -- in a Q&A manner. We validate our approach using\nlarge-scale human mobility data from four metropolitan areas in Japan, focusing\non predicting individual trajectories over the next 15 days. The results\ndemonstrate that Llama-3-8B-Mob excels in modeling long-term human mobility --\nsurpassing the state-of-the-art on multiple prediction metrics. It also\ndisplays strong zero-shot generalization capabilities -- effectively\ngeneralizing to other cities even when fine-tuned only on limited samples from\na single city. Source codes are available at\nhttps://github.com/TANGHULU6/Llama3-8B-Mob.",
        "分类": [
            "cs.CL",
            "cs.CY"
        ],
        "补充信息": null,
        "日期": "2024-10-31T07:30:38+00:00",
        "概述": "本文研究了使用指令调优的大语言模型Llama-3-8B-Mob在城市尺度长期人口流动性预测方面的应用。传统的短时预测方法难以适应不同城市环境，本文通过调用大规模日本城市的人口流动数据，实现对未来15天个体轨迹的预测。结果表明，Llama-3-8B-Mob在长期预测方面优于现有方法，并展现出较强的零样本泛化能力。",
        "摘要译文": "人类移动性预测在灾害响应、城市规划和传染病预测等应用中发挥着关键作用。传统方法往往依赖于设计特定领域的手工模型，并且通常专注于短期预测，这类预测在面对各种城市环境时难以泛化。在本研究中，我们引入了Llama-3-8B-Mob，这是一种通过指令调优 fine-tuned 的大规模语言模型，用于以问答方式实现城市的长期移动性预测。我们使用来自日本四个大都市区的大规模人类移动性数据进行了验证，重点关注预测接下来15天的个人轨迹。结果表明，Llama-3-8B-Mob 在建模长期人类移动性方面表现出色——在多个预测指标上超越了最新技术。此外，它还展示了强大的零样本泛化能力——即使仅在单一城市有限样本上进行微调，也能有效地泛化到其他城市。源代码可在 https://github.com/TANGHULU6/Llama3-8B-Mob 获取。"
    },
    {
        "序号": 42,
        "标题": "OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models",
        "链接": "http://arxiv.org/abs/2410.23703v1",
        "作者": [
            "Junda Wu",
            "Xintong Li",
            "Ruoyu Wang",
            "Yu Xia",
            "Yuxin Xiong",
            "Jianing Wang",
            "Tong Yu",
            "Xiang Chen",
            "Branislav Kveton",
            "Lina Yao",
            "Jingbo Shang",
            "Julian McAuley"
        ],
        "摘要": "Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge.",
        "分类": [
            "cs.LG",
            "cs.CL"
        ],
        "补充信息": "10 pages",
        "日期": "2024-10-31T07:48:44+00:00",
        "概述": "本文旨在通过Offline Chain-of-thought Evaluation and Alignment框架（OCEAN）优化大型语言模型的链式推理能力。针对链式推理与知识图谱结构间的异构性，提出利用知识图谱和 reinforcement learning（RL）进行实体链接和推理路径对齐，最终通过知识图谱密集概率评分（KG-IPS）评估模型行为并优化政策。实验结果显示，OCEAN能有效提升链式推理路径的质量，而不会影响模型在下游任务中的通用能力或内部知识。",
        "摘要译文": " Offline评估大型语言模型（LLM）对于理解其能力至关重要，尽管当前的方法在现有研究中仍处于未充分探索的状态。在本项工作中，我们专注于LLM的在线评估能力的评估，并展示了如何基于提出的评估方法优化LLM。为了提供丰富的知识和推理路径的离线反馈，我们使用知识图谱（例如，Wikidata5m）来对生成的推理链条提供反馈。由于LLM推理与知识图谱结构之间的异质性，直接从知识图谱对LLM行为进行交互和反馈具有挑战性，这需要准确的实体链接并将LLM生成的推理链条在知识图谱中进行定位。为了解决上述挑战，我们提出了一个离线推理链条评估框架OCEAN，将LLM中的推理链条建模为MDP，并评估策略与知识图谱偏好的一致性。为克服推理异质性和定位问题，我们利用基于策略的知识图谱探索和强化学习来建模一个知识图谱策略，该策略为LLM生成的推理链条生成了token级别概率分布，模拟知识图谱的推理偏好。然后，我们将知识图谱反馈纳入逆倾向得分中，并提出了KG-IPS估算器。从理论上讲，我们证明了提出kg-ips估算器的无偏差性，并提供了其方差的下界。借助离策略评估值函数，我们可以直接启用离策略优化，从而进一步增强推理链条的一致性。我们的实证研究表明，OCEAN可以在不损害LLM在下游任务中的普遍能力和内部知识的情况下，高效优化生成具有更高估计值的推理链条。"
    },
    {
        "序号": 44,
        "标题": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers",
        "链接": "http://arxiv.org/abs/2410.23684v1",
        "作者": [
            "Eugene Jang",
            "Kimin Lee",
            "Jin-Woo Chung",
            "Keuntae Park",
            "Seungwon Shin"
        ],
        "摘要": "Tokenization is a crucial step that bridges human-readable text with\nmodel-readable discrete tokens. However, recent studies have revealed that\ntokenizers can be exploited to elicit unwanted model behaviors. In this work,\nwe investigate incomplete tokens, i.e., undecodable tokens with stray bytes\nresulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize\nthat such tokens are heavily reliant on their adjacent tokens and are fragile\nwhen paired with unfamiliar tokens. To demonstrate this vulnerability, we\nintroduce improbable bigrams: out-of-distribution combinations of incomplete\ntokens designed to exploit their dependency. Our experiments show that\nimprobable bigrams are significantly prone to hallucinatory behaviors.\nSurprisingly, alternative tokenizations of the same phrases result in\ndrastically lower rates of hallucination (93% reduction in Llama3.1). We\ncaution against the potential vulnerabilities introduced by byte-level BPE\ntokenizers, which may impede the development of trustworthy language models.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T07:19:44+00:00",
        "概述": "该研究探讨了字节级BPE分词器中的不完整令牌问题，指出这些令牌对相邻令牌高度依赖且容易受到不熟悉令牌的影响。研究通过设计“不可能双词”来验证这种脆弱性，即旨在利用此依赖性的未见过的令牌组合。实验结果表明，不完整令牌容易诱发幻觉行为，不同分词方法显著降低了幻觉率（如Llama3.1降低了93%）。研究强调了字节级BPE分词器潜在的安全风险，可能会影响语言模型的可靠性。",
        "摘要译文": "分词是将人类可读的文本转换为模型可读的离散令牌的关键步骤。然而，最近的研究揭示了分词器可能被利用以引发不必要的模型行为。在本工作中，我们研究了不完整令牌，即由字节级比特对编码（BPE）分词产生的无法解码的令牌，因其含有多余的字节。我们假设这些令牌高度依赖于其相邻令牌，并且与不熟悉的令牌配对时较为脆弱。为了展示这种脆弱性，我们提出了不可能双词组：设计用于利用其依赖关系的不常用不完整令牌组合。我们的实验表明，不可能双词组会显著引发幻想行为。令人惊讶的是，对相同短语的不同分词方式会导致幻觉率大幅下降（在Llama3.1中降低93%）。我们警告潜在的由字节级BPE分词器引入的安全漏洞，这可能会阻碍可信语言模型的发展。"
    },
    {
        "序号": 45,
        "标题": "Pseudo-Conversation Injection for LLM Goal Hijacking",
        "链接": "http://arxiv.org/abs/2410.23678v1",
        "作者": [
            "Zheng Chen",
            "Buhui Yao"
        ],
        "摘要": "Goal hijacking is a type of adversarial attack on Large Language Models\n(LLMs) where the objective is to manipulate the model into producing a\nspecific, predetermined output, regardless of the user's original input. In\ngoal hijacking, an attacker typically appends a carefully crafted malicious\nsuffix to the user's prompt, which coerces the model into ignoring the user's\noriginal input and generating the target response. In this paper, we introduce\na novel goal hijacking attack method called Pseudo-Conversation Injection,\nwhich leverages the weaknesses of LLMs in role identification within\nconversation contexts. Specifically, we construct the suffix by fabricating\nresponses from the LLM to the user's initial prompt, followed by a prompt for a\nmalicious new task. This leads the model to perceive the initial prompt and\nfabricated response as a completed conversation, thereby executing the new,\nfalsified prompt. Following this approach, we propose three Pseudo-Conversation\nconstruction strategies: Targeted Pseudo-Conversation, Universal\nPseudo-Conversation, and Robust Pseudo-Conversation. These strategies are\ndesigned to achieve effective goal hijacking across various scenarios. Our\nexperiments, conducted on two mainstream LLM platforms including ChatGPT and\nQwen, demonstrate that our proposed method significantly outperforms existing\napproaches in terms of attack effectiveness.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T06:58:34+00:00",
        "概述": "这篇论文针对大型语言模型（LLMs）中的目标劫持攻击，提出了一种新型攻击方法——伪对话注入（Pseudo-Conversation Injection）。动机在于利用LLMs在对话角色识别上的弱点。方法是构造一个由伪造响应和恶意新任务请求组成的后缀，使模型误以为初始对话已完成，从而执行新的虚假请求。实验结果显示，该方法在ChatGPT和Qwen上显著优于现有方法，有效提升了攻击效果。",
        "摘要译文": "目标劫持是一种针对大规模语言模型（LLMs）的 adversarial 攻击类型，其目标是操纵模型产生特定的、预先确定的输出，而不管用户的原始输入是什么。在目标劫持中，攻击者通常会在用户的提示后面添加一个精心构建的恶意后缀，从而使模型忽略用户的原始输入并生成目标响应。在本文中，我们介绍了一种名为伪对话注入的新目标劫持攻击方法，该方法利用了 LLMs 在对话上下文中的角色识别弱点。具体而言，我们通过让用户模型对其初始提示生成伪造响应，然后生成一个恶意的新任务提示来构建后缀。这会使模型将初始提示和伪造的响应视为一个完整的对话，从而执行新的、虚假的提示。依据这一方法，我们提出了三种伪对话构建策略：目标导向的伪对话、通用伪对话和稳健的伪对话。这些策略旨在在各种场景中实现有效的目标劫持。我们在包括 ChatGPT 和 Qwen 的两个主流 LLM 平台上进行的实验表明，与现有方法相比，我们提出的方法在攻击效果上明显更优。"
    },
    {
        "序号": 47,
        "标题": "Morphological Typology in BPE Subword Productivity and Language Modeling",
        "链接": "http://arxiv.org/abs/2410.23656v1",
        "作者": [
            "Iñigo Parra"
        ],
        "摘要": "This study investigates the impact of morphological typology on tokenization\nand language modeling performance. We focus on languages with synthetic and\nanalytical morphological structures and examine their productivity when\ntokenized using the byte-pair encoding (BPE) algorithm. We compare the\nperformance of models trained with similar amounts of data in different\nlanguages. Our experiments reveal that languages with synthetic features\nexhibit greater subword regularity and productivity with BPE tokenization and\nachieve better results in language modeling tasks. We also observe that the\ntypological continuum from linguistic theory is reflected in several\nexperiments. These findings suggest a correlation between morphological\ntypology and BPE tokenization efficiency.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "15 pages, 6 figures",
        "日期": "2024-10-31T06:13:29+00:00",
        "概述": "这项研究旨在探讨形态学类型对基于字节对编码（BPE）的子词分词及语言建模性能的影响。研究聚焦于合成型和分析型语言，并比较了不同语言在相同数据量下使用BPE tokenization的效果。实验结果表明，具有合成型特征的语言在BPE tokenization下展现出更高程度的子词规律性和生产性，并在语言建模任务中表现更优。研究还发现，语言理论中的形态学连续谱在实验中有所体现，暗示形态学类型与BPE tokenization效率之间存在关联。",
        "摘要译文": "这项研究探讨了形态类型学对分词和语言建模性能的影响。我们关注具有合成性和分析性形态结构的语言，并检查它们在使用字节对编码（BPE）算法进行分词时的产效率。我们比较了不同语言中使用相似数据量训练的模型的表现。我们的实验表明，具有合成性特征的语言在使用BPE进行分词时表现出更大的子词规律性和产效率，并在语言建模任务中取得更好的结果。我们还观察到，语言学理论中的类型学连续体在多个实验中得到了反映。这些发现表明形态类型学与BPE分词效率之间存在关联。"
    },
    {
        "序号": 46,
        "标题": "Kernel Looping: Eliminating Synchronization Boundaries for Peak Inference Performance",
        "链接": "http://arxiv.org/abs/2410.23668v1",
        "作者": [
            "David Koeplinger",
            "Darshan Gandhi",
            "Pushkar Nandkar",
            "Nathan Sheeley",
            "Matheen Musaddiq",
            "Leon Zhang",
            "Reid Goodbar",
            "Matthew Shaffer",
            "Han Wang",
            "Angela Wang",
            "Mingran Wang",
            "Raghu Prabhakar"
        ],
        "摘要": "Token generation speed is critical to power the next wave of AI inference\napplications. GPUs significantly underperform during token generation due to\nsynchronization overheads at kernel boundaries, utilizing only 21% of their\npeak memory bandwidth. While recent dataflow architectures mitigate these\noverheads by enabling aggressive fusion of decoder layers into a single kernel,\nthey too leave performance on the table due to synchronization penalties at\nlayer boundaries.\n  This paper presents kernel looping, a specialized global optimization\ntechnique which exploits an optimization opportunity brought by combining the\nunique layer-level fusion possible in modern dataflow architectures with the\nrepeated layer structure found in language models. Kernel looping eliminates\nsynchronization costs between consecutive calls to the same kernel by\ntransforming these calls into a single call to a modified kernel containing a\npipelined outer loop. We evaluate kernel looping on the SambaNova SN40L\nReconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.\nExperiments demonstrate that kernel looping speeds up the decode phase of a\nwide array of powerful open-source models by up to 2.2$\\times$ on SN40L. Kernel\nlooping allows scaling of decode performance over multiple SN40L sockets,\nachieving speedups of up to 2.5$\\times$. Finally, kernel looping enables SN40L\nto achieve over 90% of peak performance on 8 and 16 sockets and achieve a\nspeedup of up to 3.7$\\times$ over DGX H100. Kernel looping, as well as the\nmodels evaluated in this paper, are deployed in production in a commercial AI\ninference cloud.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.AR",
            "D.3.4; C.1.3"
        ],
        "补充信息": null,
        "日期": "2024-10-31T06:32:47+00:00",
        "概述": "本文针对AI推理应用中的令牌生成速度瓶颈，提出了Kernel Looping技术。该技术通过消除 kernel 边界同步开销，提升 GPU 性能。研究通过在现代数据流架构中融合语言模型的层级结构，将多次连续 kernel 调用合并为一次带有流水线循环的调用。实验表明，该技术在 SambaNova SN40L 加速器上将解码阶段的速度提高了2.2倍，并实现了超过90%的峰值性能及高达3.7倍的加速比。",
        "摘要译文": "生成标记的速度对于支撑下一波AI推理应用至关重要。由于在内核边界处的同步开销，GPU在生成标记时显著性能不佳，只能利用其峰值内存带宽的21%。尽管最近的数据流架构通过将解码层合并为单个内核来实现激进融合从而减轻这些开销，但由于在层边界处的同步惩罚，它们依然无法充分利用性能。\n\n本文提出了一种内核循环技术，这是一种专门的全局优化技术，通过结合现代数据流架构中独特的层级融合能力和语言模型中重复的层结构来利用这一优化机会。内核循环通过将连续两次调用同一个内核转变为一个包含流水线外层循环的修改后内核调用来消除同步成本。我们在SambaNova SN40L可重构数据流单元（RDU）上评估了内核循环技术，这是一个商业化的AI数据流加速器。实验表明，内核循环技术可以将多种强大开源模型的解码阶段加速高达2.2倍。内核循环技术还允许多个SN40L插槽间的解码性能扩展，实现高达2.5倍的加速。最后，内核循环技术使SN40L在8和16个插槽上实现了超过90%的峰值性能，并相对于DGX H100实现了高达3.7倍的加速。本文所提出的内核循环技术以及评估的模型已经在一家商业AI推理云中投入生产使用。"
    },
    {
        "序号": 49,
        "标题": "Dynamic Uncertainty Ranking: Enhancing In-Context Learning for Long-Tail Knowledge in LLMs",
        "链接": "http://arxiv.org/abs/2410.23605v1",
        "作者": [
            "Shuyang Yu",
            "Runxue Bao",
            "Parminder Bhatia",
            "Taha Kass-Hout",
            "Jiayu Zhou",
            "Cao Xiao"
        ],
        "摘要": "Large language models (LLMs) can learn vast amounts of knowledge from diverse\ndomains during pre-training. However, long-tail knowledge from specialized\ndomains is often scarce and underrepresented, rarely appearing in the models'\nmemorization. Prior work has shown that in-context learning (ICL) with\nretriever augmentation can help LLMs better capture long-tail knowledge,\nreducing their reliance on pre-trained data. Despite these advances, we observe\nthat LLM predictions for long-tail questions remain uncertain to variations in\nretrieved samples. To take advantage of the uncertainty in ICL for guiding LLM\npredictions toward correct answers on long-tail samples, we propose a\nreinforcement learning-based dynamic uncertainty ranking method for ICL that\naccounts for the varying impact of each retrieved sample on LLM predictions.\nOur approach prioritizes more informative and stable samples while demoting\nmisleading ones, updating rankings based on the feedback from the LLM w.r.t.\neach retrieved sample. To enhance training efficiency and reduce query costs,\nwe introduce a learnable dynamic ranking threshold, adjusted when the model\nencounters negative prediction shifts. Experimental results on various\nquestion-answering datasets from different domains show that our method\noutperforms the best baseline by $2.76\\%$, with a notable $5.96\\%$ boost in\naccuracy on long-tail questions that elude zero-shot inference.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T03:42:17+00:00",
        "概述": "该研究旨在改进大型语言模型（LLMs）在长尾知识领域的学习能力。通过在上下文学习中引入动态不确定性排名方法，该方法利用检索增强，减少对预训练数据的依赖，并通过强化学习优先选择更有信息量且更稳定的检索样本，降权误导样本，从而提升长尾问题的预测准确性。实验结果显示，该方法在多种问答数据集上优于最佳基线模型，尤其是在长尾问题上的准确率提升了5.96%。",
        "摘要译文": "大型语言模型（LLMs）在预训练过程中可以从多种领域中学习大量的知识。然而，来自特定领域的小众知识往往稀缺且在模型的记忆中代表性不足，很少出现。先前的研究表明，通过检索增强的上下文学习（ICL）可以幫助LLMs更好地捕捉小众知识，减少对预训练数据的依赖。尽管取得了这些进展，我们观察到LLMs在处理小众问题时对检索样本变化仍持不确定态度。为了利用ICL中的不确定性来引导LLMs在小众样本上获得正确答案，我们提出了一个基于强化学习的动态不确定性排名方法，该方法考虑了每个检索样本对LLMs预测的不同影响。我们的方法优先考虑更有信息量和更稳定的样本，同时降低误导性样本的排名，根据模型对每个检索样本的反馈更新排名。为了提高训练效率并减少查询成本，我们引入了一个可学习的动态排名阈值，当模型遇到负面预测变化时进行调整。在不同领域的多个问答数据集上的实验结果显示，我们的方法比最好基线提高了2.76%，特别在零样本推理无法解决的小众问题上准确率提高了5.96%。"
    },
    {
        "序号": 48,
        "标题": "On Positional Bias of Faithfulness for Long-form Summarization",
        "链接": "http://arxiv.org/abs/2410.23609v1",
        "作者": [
            "David Wan",
            "Jesse Vig",
            "Mohit Bansal",
            "Shafiq Joty"
        ],
        "摘要": "Large Language Models (LLMs) often exhibit positional bias in long-context\nsettings, under-attending to information in the middle of inputs. We\ninvestigate the presence of this bias in long-form summarization, its impact on\nfaithfulness, and various techniques to mitigate this bias. To consistently\nevaluate faithfulness, we first compile a benchmark of eight human-annotated\nlong-form summarization datasets and perform a meta-evaluation of faithfulness\nmetrics. We show that LLM-based faithfulness metrics, though effective with\nfull-context inputs, remain sensitive to document order, indicating positional\nbias. Analyzing LLM-generated summaries across six datasets, we find a\n\"U-shaped\" trend in faithfulness, where LLMs faithfully summarize the beginning\nand end of documents but neglect middle content. Perturbing document order\nsimilarly reveals models are less faithful when important documents are placed\nin the middle of the input. We find that this behavior is partly due to\nshifting focus with context length: as context increases, summaries become less\nfaithful, but beyond a certain length, faithfulness improves as the model\nfocuses on the end. Finally, we experiment with different generation techniques\nto reduce positional bias and find that prompting techniques effectively direct\nmodel attention to specific positions, whereas more sophisticated approaches\noffer limited improvements. Our data and code are available in\nhttps://github.com/meetdavidwan/longformfact.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "18 pages",
        "日期": "2024-10-31T03:50:15+00:00",
        "概述": "该研究探讨大型语言模型（LLMs）在长文摘要生成中是否存在位置偏见，即过度关注输入开头和结尾而忽视中间信息，并影响摘要的忠实性。研究通过编制八个人标注的长文摘要数据集，评估忠实性指标，发现LLM生成的摘要存在“U”型趋势，开头和结尾忠实性较高，但中间部分被忽略，且文档顺序扰动证实了该偏见。研究还尝试了不同的生成方法，发现提示技术能有效引导模型关注特定位置，而其他高级方法效果有限。",
        "摘要译文": "大型语言模型（LLMs）在长上下文设置中常常表现出位置偏差，对输入中间的信息关注不足。我们调查了这种偏差在长文本总结中的存在情况、对其忠实度的影响，以及各种减轻这种偏差的技术。为了 consistent 地评估忠实度，我们首先汇总了一个包含八个手工标注的长文本总结数据集的基准，并进行了忠实度度量的元评估。我们显示，基于LLM的忠实度度量虽然在完整的上下文中有效，但在对文档顺序仍敏感，表明存在位置偏差。通过分析六个数据集上生成的LLM摘要，我们发现忠实度呈现出“U”形趋势，LLM忠实总结文档的开头和结尾部分，但忽略中间内容。将文档顺序扰动后同样显示，当重要文档放在输入中间时，模型的忠实度较低。我们发现这种行为部分归因于随着上下文长度的变化而改变重点：随着上下文的增加，摘要变得不太忠实，但在某种长度之后，忠实度提高，因为模型专注于结尾。最后，我们尝试使用不同的生成技术来减少位置偏差，发现提示技术有效地将模型的注意力引导到特定位置，而更复杂的策略仅提供有限的改进。我们的数据和代码可在 https://github.com/meetdavidwan/longformfact 获取。"
    },
    {
        "序号": 50,
        "标题": "Using Multimodal Deep Neural Networks to Disentangle Language from Visual Aesthetics",
        "链接": "http://arxiv.org/abs/2410.23603v1",
        "作者": [
            "Colin Conwell",
            "Christopher Hamblin",
            "Chelsea Boccagno",
            "David Mayo",
            "Jesse Cummings",
            "Leyla Isik",
            "Andrei Barbu"
        ],
        "摘要": "When we experience a visual stimulus as beautiful, how much of that\nexperience derives from perceptual computations we cannot describe versus\nconceptual knowledge we can readily translate into natural language?\nDisentangling perception from language in visually-evoked affective and\naesthetic experiences through behavioral paradigms or neuroimaging is often\nempirically intractable. Here, we circumnavigate this challenge by using linear\ndecoding over the learned representations of unimodal vision, unimodal\nlanguage, and multimodal (language-aligned) deep neural network (DNN) models to\npredict human beauty ratings of naturalistic images. We show that unimodal\nvision models (e.g. SimCLR) account for the vast majority of explainable\nvariance in these ratings. Language-aligned vision models (e.g. SLIP) yield\nsmall gains relative to unimodal vision. Unimodal language models (e.g. GPT2)\nconditioned on visual embeddings to generate captions (via CLIPCap) yield no\nfurther gains. Caption embeddings alone yield less accurate predictions than\nimage and caption embeddings combined (concatenated). Taken together, these\nresults suggest that whatever words we may eventually find to describe our\nexperience of beauty, the ineffable computations of feedforward perception may\nprovide sufficient foundation for that experience.",
        "分类": [
            "cs.CV",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T03:37:21+00:00",
        "概述": "这篇文章探讨了美感能力中感知与语言表达的分离问题。研究通过使用多模态深度神经网络模型预测自然图像的美丽评分，发现单模态视觉模型解释了绝大多数评分的可解释性变化，而语言对视觉的影响有限。结果表明，尽管我们可以用语言描述美的体验，但感知过程中不可言说的部分可能已经足够构成了这种体验的基础。",
        "摘要译文": "当我们体验到某个视觉刺激是美丽的时，其中有多少体验源自我们无法描述的感知计算，而非我们可以轻松转化为自然语言的概念知识？通过行为范式或神经成像分离视觉诱发的情感和美学体验中的感知与语言成分往往是不可行的。在此，我们通过在单模态视觉、单模态语言和多模态（语言对齐的）深度神经网络（DNN）模型的已学习表示上进行线性解码，来预测自然图像的人类美丽评分。我们发现，单模态视觉模型（例如SimCLR）能够解释这些评分中的绝大部分可解释变异。语言对齐的视觉模型（例如SLIP）相对于单模态视觉模型仅带来很小的提升。通过视觉嵌入生成标题（例如CLIPCap）的单模态语言模型（例如GPT2）没有进一步的提升。仅使用标题嵌入比同时使用图像和标题嵌入组合的预测更加不准确。综合来看，这些结果表明，无论我们最终找到多少词汇来描述我们的审美体验，前馈感知中的不可言说的计算可能已经为这种体验提供了足够的基础。"
    },
    {
        "序号": 52,
        "标题": "BioNCERE: Non-Contrastive Enhancement For Relation Extraction In Biomedical Texts",
        "链接": "http://arxiv.org/abs/2410.23583v1",
        "作者": [
            "Farshad Noravesh"
        ],
        "摘要": "State-of-the-art models for relation extraction (RE) in the biomedical domain\nconsider finetuning BioBERT using classification, but they may suffer from the\nanisotropy problem. Contrastive learning methods can reduce this anisotropy\nphenomena, and also help to avoid class collapse in any classification problem.\nIn the present paper, a new training method called biological non-contrastive\nrelation extraction (BioNCERE) is introduced for relation extraction without\nusing any named entity labels for training to reduce annotation costs. BioNCERE\nuses transfer learning and non-contrastive learning to avoid full or\ndimensional collapse as well as bypass overfitting. It resolves RE in three\nstages by leveraging transfer learning two times. By freezing the weights\nlearned in previous stages in the proposed pipeline and by leveraging\nnon-contrastive learning in the second stage, the model predicts relations\nwithout any knowledge of named entities. Experiments have been done on SemMedDB\nthat are almost similar to State-of-the-art performance on RE without using the\ninformation of named entities.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": "4 figures, 2 tables, 10 pages",
        "日期": "2024-10-31T02:51:56+00:00",
        "概述": "这项研究提出了一种新的非对比增强学习方法——BioNCERE，用于生物医学文本中的关系抽取，旨在解决传统方法的注解成本高和模型过拟合问题。该方法通过两次迁移学习和非对比学习，减少对比度差异和类别坍缩，从而在不使用命名实体标签的情况下达到接近当前最佳性能的成果。",
        "摘要译文": "最先进的生物医学领域关系提取（RE）模型考虑使用分类微调BioBERT，但它们可能会遭受各向异性问题。对比学习方法可以减少这种各向异性现象，并且也有助于避免在任何分类问题中发生类别坍缩。在本文中，提出了一种新的训练方法，称为生物非对比关系提取（BioNCERE），该方法在不使用任何命名实体标签的情况下进行训练，以降低注释成本。BioNCERE 使用迁移学习和非对比学习来避免维度坍缩，同时绕过过拟合。该方法通过两次利用迁移学习分三个阶段解决关系提取问题。通过在提议的管道中冻结先前阶段学习到的权重，并在第二阶段利用非对比学习，模型可以在没有任何命名实体知识的情况下预测关系。实验在SemMedDB上进行，几乎与不使用命名实体信息的最先进的关系提取性能相当。"
    },
    {
        "序号": 51,
        "标题": "End-to-End Ontology Learning with Large Language Models",
        "链接": "http://arxiv.org/abs/2410.23584v1",
        "作者": [
            "Andy Lo",
            "Albert Q. Jiang",
            "Wenda Li",
            "Mateja Jamnik"
        ],
        "摘要": "Ontologies are useful for automatic machine processing of domain knowledge as\nthey represent it in a structured format. Yet, constructing ontologies requires\nsubstantial manual effort. To automate part of this process, large language\nmodels (LLMs) have been applied to solve various subtasks of ontology learning.\nHowever, this partial ontology learning does not capture the interactions\nbetween subtasks. We address this gap by introducing OLLM, a general and\nscalable method for building the taxonomic backbone of an ontology from\nscratch. Rather than focusing on subtasks, like individual relations between\nentities, we model entire subcomponents of the target ontology by finetuning an\nLLM with a custom regulariser that reduces overfitting on high-frequency\nconcepts. We introduce a novel suite of metrics for evaluating the quality of\nthe generated ontology by measuring its semantic and structural similarity to\nthe ground truth. In contrast to standard metrics, our metrics use deep\nlearning techniques to define more robust distance measures between graphs.\nBoth our quantitative and qualitative results on Wikipedia show that OLLM\noutperforms subtask composition methods, producing more semantically accurate\nontologies while maintaining structural integrity. We further demonstrate that\nour model can be effectively adapted to new domains, like arXiv, needing only a\nsmall number of training examples. Our source code and datasets are available\nat https://github.com/andylolu2/ollm.",
        "分类": [
            "cs.LG",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T02:52:39+00:00",
        "概述": "本文研究了使用大型语言模型（LLMs）进行端到端本体学习的方法，旨在自动化本体构建过程中的部分任务。传统的部分本体学习方法未能捕捉到子任务间的交互，因此提出了OLLM，通过微调LLM并引入自定义正则化器来构建本体的分类骨架。OLLM使用新提出的深度学习技术评估生成本体的质量，从语义和结构层面衡量与真实本体的相似度。实验结果表明，OLLM在Wikipedia和arXiv等领域中均优于任务组合方法，实现了更准确的本体构建。",
        "摘要译文": "本体在以结构化格式表示领域知识方面对于自动机器处理是有用的。然而，构建本体需要大量的手工努力。为了部分自动化这个过程，大型语言模型（LLMs）已被应用于解决本体学习的各种子任务。然而，这种部分本体学习并没有捕捉到子任务之间的交互。我们通过引入OLLM（本体构建机器学习框架）来填补这一缺口，OLLM是一个通用且可扩展的方法，用于从零开始构建本体的分类骨架。我们不是专注于子任务，比如个体实体之间的关系，而是通过使用自定义正则化器对LLM进行微调，以减少对高频概念的过度拟合，从而建模目标本体的整个子组件。我们引入了一套新的评估指标来评估生成本体的质量，通过测量其与真实值的语义和结构相似性。与标准指标不同，我们的指标利用深度学习技术定义了更稳健的图形间距离度量。我们在维基百科上的定量和定性结果都显示，OLLM优于子任务组合方法，能够生成更加语义准确的本体并保持其结构完整性。此外，我们还展示了我们的模型可以有效地适应新领域，比如arXiv，只需要少量的训练样本即可。我们的源代码和数据集可在https://github.com/andylolu2/ollm获取。"
    },
    {
        "序号": 53,
        "标题": "From Context to Action: Analysis of the Impact of State Representation and Context on the Generalization of Multi-Turn Web Navigation Agents",
        "链接": "http://arxiv.org/abs/2410.23555v1",
        "作者": [
            "Nalin Tiwary",
            "Vardhan Dongre",
            "Sanil Arun Chawla",
            "Ashwin Lamani",
            "Dilek Hakkani-Tür"
        ],
        "摘要": "Recent advancements in Large Language Model (LLM)-based frameworks have\nextended their capabilities to complex real-world applications, such as\ninteractive web navigation. These systems, driven by user commands, navigate\nweb browsers to complete tasks through multi-turn dialogues, offering both\ninnovative opportunities and significant challenges. Despite the introduction\nof benchmarks for conversational web navigation, a detailed understanding of\nthe key contextual components that influence the performance of these agents\nremains elusive. This study aims to fill this gap by analyzing the various\ncontextual elements crucial to the functioning of web navigation agents. We\ninvestigate the optimization of context management, focusing on the influence\nof interaction history and web page representation. Our work highlights\nimproved agent performance across out-of-distribution scenarios, including\nunseen websites, categories, and geographic locations through effective context\nmanagement. These findings provide insights into the design and optimization of\nLLM-based agents, enabling more accurate and effective web navigation in\nreal-world applications.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.HC"
        ],
        "补充信息": "10 pages, 3 figures, 5 tables",
        "日期": "2024-10-31T01:51:41+00:00",
        "概述": "该研究旨在分析和优化基于大语言模型的多轮网站导航代理的上下文管理，解决其在复杂实时应用中的性能问题。研究通过考察交互历史和网页表示的影响，提高了代理在未见过的网站、类别和地理区域中的性能。结果表明，有效的上下文管理能提升代理在分布式场景下的表现，为设计和优化大语言模型驱动的代理提供了重要见解。",
        "摘要译文": "近年来，基于大型语言模型（LLM）的框架在扩展其实用功能方面取得了进展，例如交互式的网络导航。这些系统根据用户指令导航网络浏览器，通过多轮对话完成任务，既带来了创新的机会，也带来了重大的挑战。尽管已经提出了对话式网络导航的基准，但这些代理的性能受哪些关键上下文因素影响的详细理解仍然缺乏。本研究旨在通过分析对网络导航代理运行至关重要的各种上下文要素来填补这一空白。我们研究了上下文管理的优化，重点关注交互历史和网页表示的影响。我们的工作突出了通过有效的上下文管理，在不同分布场景中的代理性能的提升，包括未见过的网站、类别和地理位置。这些发现为LLM基础代理的设计和优化提供了见解，使其实现在现实生活中的网络导航更加准确和有效。"
    },
    {
        "序号": 54,
        "标题": "Simulating User Agents for Embodied Conversational-AI",
        "链接": "http://arxiv.org/abs/2410.23535v1",
        "作者": [
            "Daniel Philipov",
            "Vardhan Dongre",
            "Gokhan Tur",
            "Dilek Hakkani-Tür"
        ],
        "摘要": "Embodied agents designed to assist users with tasks must engage in natural\nlanguage interactions, interpret instructions, execute actions, and communicate\neffectively to resolve issues. However, collecting large-scale, diverse\ndatasets of situated human-robot dialogues to train and evaluate such agents is\nexpensive, labor-intensive, and time-consuming. To address this challenge, we\npropose building a large language model (LLM)-based user agent that can\nsimulate user behavior during interactions with an embodied agent in a virtual\nenvironment. Given a user goal (e.g., make breakfast), at each time step, the\nuser agent may observe\" the robot actions or speak\" to either intervene with\nthe robot or answer questions. Such a user agent assists in improving the\nscalability and efficiency of embodied dialogues dataset generation and is\ncritical for enhancing and evaluating the robot's interaction and task\ncompletion ability, as well as for research in reinforcement learning using AI\nfeedback. We evaluate our user agent's ability to generate human-like behaviors\nby comparing its simulated dialogues with the TEACh dataset. We perform three\nexperiments: zero-shot prompting to predict dialogue acts, few-shot prompting,\nand fine-tuning on the TEACh training subset. Results show the LLM-based user\nagent achieves an F-measure of 42% with zero-shot prompting and 43.4% with\nfew-shot prompting in mimicking human speaking behavior. Through fine-tuning,\nperformance in deciding when to speak remained stable, while deciding what to\nsay improved from 51.1% to 62.5%. These findings showcase the feasibility of\nthe proposed approach for assessing and enhancing the effectiveness of robot\ntask completion through natural language communication.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.RO"
        ],
        "补充信息": "8 pages, 5 figures, 4 tables",
        "日期": "2024-10-31T00:56:08+00:00",
        "概述": "本文提出了一种基于大规模语言模型（LLM）的用户代理，用于模拟与虚拟环境中实体代理互动时的用户行为。该方法旨在解决收集大规模、多样化的人机对话数据集的成本高、耗时长的问题。通过三项实验（零样本提示、少量样本提示和微调），验证了该用户代理生成人类行为的能力，结果显示LLM在模拟人类对话行为方面达到了42%-43.4%的F值，并通过微调提高了决策说什么方面的效果。研究表明，该方法能够评估和提升机器人通过自然语言进行任务完成的有效性。",
        "摘要译文": "设计用于协助用户完成任务的具身代理必须进行自然语言交互、解释指令、执行操作并有效沟通以解决问题。然而，收集大规模、多样的 situated 机器人对话数据集来训练和评估这些代理极为昂贵、劳动密集且耗时。为应对这一挑战，我们建议构建一个基于大语言模型（LLM）的用户代理，该代理可以在虚拟环境中模拟用户与具身代理交互时的行为。给定一个用户目标（例如，制作早餐），在每一时间步，用户代理可能会观察机器人行动或“说话”以干预机器人或回答问题。这样的用户代理有助于提高具身对话数据集生成的可扩展性和效率，并对增强和评估机器人交互和任务完成能力至关重要，同时对于使用 AI 反馈进行强化学习的研究也至关重要。我们通过将用户代理生成的对话与 TEACh 数据集进行比较，评估其生成人类行为的能力。我们进行了三项实验：零样本提示以预测对话行为、少量样本提示以及在 TEACh 训练子集上的微调。结果显示，在零样本提示下，LLM 基础的用户代理在模仿人类说话行为方面达到了 42% 的 F 值，在少量样本提示下达到了 43.4%。通过微调，决定何时说话的表现保持稳定，而决定说什么提高了从 51.1% 到 62.5%。这些发现展示了该提议的方法在通过自然语言通信评估和提升机器人任务完成效果方面的可行性。"
    },
    {
        "序号": 56,
        "标题": "LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models",
        "链接": "http://arxiv.org/abs/2410.23526v1",
        "作者": [
            "Hieu Tran",
            "Junda Wang",
            "Yujan Ting",
            "Weijing Huang",
            "Terrence Chen"
        ],
        "摘要": "Large language models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks, yet they often struggle with maintaining\nfactual accuracy, particularly in knowledge-intensive domains like healthcare.\nThis study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,\na novel approach designed to enhance the factual reliability of LLMs, with a\nfocus on medical question answering (QA). LEAF utilizes a dual strategy to\nenhance the factual accuracy of responses from models such as Llama 3 70B\nInstruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,\nimproves Retrieval-Augmented Generation (RAG) by incorporating fact-checking\nresults to guide the retrieval process without updating model parameters. The\nsecond strategy, Learning from Fact-Checks via Self-Training, involves\nsupervised fine-tuning (SFT) on fact-checked responses or applying Simple\nPreference Optimization (SimPO) with fact-checking as a ranking mechanism, both\nupdating LLM parameters from supervision. These findings suggest that\nintegrating fact-checked responses whether through RAG enhancement or\nself-training enhances the reliability and factual correctness of LLM outputs,\noffering a promising solution for applications where information accuracy is\ncrucial.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "22 pages, 9 figures",
        "日期": "2024-10-31T00:18:05+00:00",
        "概述": "这项研究旨在提高大型语言模型（LLMs）在医学问答等知识密集型领域的事实准确性。研究提出LEAF方法，通过事实核查增强学习和评估，采用Fact-Check-Then-RAG和Learning from Fact-Checks via Self-Training两大策略。前者通过事实核查改进检索增强生成(RAG)，后者通过监督微调或简单偏好优化(SimPO)更新模型参数。实验结果显示，这两种策略均能有效提高LLM输出的事实准确性和可靠性，特别是在信息准确性至关重要的应用中。",
        "摘要译文": "大型语言模型（LLMs）在各种自然语言处理任务中展现出了出色的能力，但它们在保持事实准确性方面常常遇到困难，特别是在像医疗保健这样知识密集的领域。本研究引入了LEAF：事实核查增强的学习与评估方法，这是一种旨在提高LLMs事实可靠性的新方法，特别关注于医疗问答（QA）。LEAF 采用双重策略来提高模型（如 Llama 3 70B Instruct 和 Llama 3 8B Instruct）生成响应的事实准确性。第一种策略是“先核查后检索增强”（Fact-Check-Then-RAG），通过将事实核查结果纳入检索过程来改进检索增强生成（RAG），而不更新模型参数。第二种策略是通过自我训练学习事实核查，通过监督微调（SFT）来调优已核查响应，或使用事实核查作为排名机制的简单偏好优化（SimPO），这两种方法都会更新LLM的参数。这些 findings 表明，无论是通过增强RAG还是自我训练，将事实核查结果纳入事实核查响应可以提高LLM输出的可靠性和事实准确性，为信息准确性至关重要的应用提供了有前景的解决方案。"
    },
    {
        "序号": 57,
        "标题": "Neural spell-checker: Beyond words with synthetic data generation",
        "链接": "http://dx.doi.org/10.1007/978-3-031-70563-2_7",
        "作者": [
            "Matej Klemen",
            "Martin Božič",
            "Špela Arhar Holdt",
            "Marko Robnik-Šikonja"
        ],
        "摘要": "Spell-checkers are valuable tools that enhance communication by identifying\nmisspelled words in written texts. Recent improvements in deep learning, and in\nparticular in large language models, have opened new opportunities to improve\ntraditional spell-checkers with new functionalities that not only assess\nspelling correctness but also the suitability of a word for a given context. In\nour work, we present and compare two new spell-checkers and evaluate them on\nsynthetic, learner, and more general-domain Slovene datasets. The first\nspell-checker is a traditional, fast, word-based approach, based on a\nmorphological lexicon with a significantly larger word list compared to\nexisting spell-checkers. The second approach uses a language model trained on a\nlarge corpus with synthetically inserted errors. We present the training data\nconstruction strategies, which turn out to be a crucial component of neural\nspell-checkers. Further, the proposed neural model significantly outperforms\nall existing spell-checkers for Slovene in both precision and recall.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "Camera-ready version. Accepted to TSD 2024",
        "日期": "2024-10-30T23:51:01+00:00",
        "概述": "本文研究了基于神经网络的拼写检查器，旨在改进传统的拼写检查工具。通过比较两种方法：一种基于大型词库的传统词基方法，另一种使用带有合成错误的大规模语言模型，作者发现后者在斯洛文尼亚语上的精确度和召回率显著优于现有拼写检查器。研究强调了训练数据构建策略的重要性，并展示了神经模型在处理拼写检查任务上的优势。",
        "摘要译文": "拼写检查器是提高交流的宝贵工具，它们能够识别文中错拼的单词。近年来，特别是在大规模语言模型方面的深度学习进步，为改进传统的拼写检查器提供了新的机会。这些新的功能不仅评估拼写正确性，还能评估单词是否适合特定的上下文。在我们的研究中，我们介绍了两种新的拼写检查器并进行了比较，这些检查器被应用于合成数据、学习者数据以及更通用的斯洛文尼亚语数据集。第一个拼写检查器是一种传统的快速单词级方法，基于一个比现有拼写检查器大得多的形态学词典。第二个方法是使用在大量包含合成错误的语料库上训练的语言模型。我们介绍了训练数据构建策略，这些策略被认为是神经拼写检查器的关键组成部分。此外，我们提出的神经模型在准确率和召回率方面显著优于所有现有的斯洛文尼亚语拼写检查器。"
    },
    {
        "序号": 55,
        "标题": "Large Language Models for Patient Comments Multi-Label Classification",
        "链接": "http://arxiv.org/abs/2410.23528v1",
        "作者": [
            "Hajar Sakai",
            "Sarah S. Lam",
            "Mohammadsadegh Mikaeili",
            "Joshua Bosire",
            "Franziska Jovin"
        ],
        "摘要": "Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4o-Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4o-Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-31T00:29:52+00:00",
        "概述": "这项研究旨在利用大型语言模型（LLMs）进行住院患者反馈的多标签分类，以提高医院的服务质量和声誉。研究解决了非结构化患者反馈数据难以用传统监督学习方法处理的问题。研究采用GPT-4o-Turbo进行分类，并通过 PHI 检测框架确保数据安全。实验结果显示，GPT-4o-Turbo 在零样本和少量样本设置下均优于传统方法和预训练语言模型，F1分数分别达到76.12%和73.61%。研究增强了多标签分类，帮助医疗从业者更深入地了解患者反馈并作出及时响应。",
        "摘要译文": "患者体验和护理质量对于医院的可持续性和声誉至关重要。患者反馈的分析为了解患者满意度和结果提供了宝贵的信息。然而，这些评论的非结构化性质给传统的监督学习方法带来了挑战。这是因为缺乏标记数据和这些文本所包含的细微差别。本研究探讨了在进行住院患者评论的多标签文本分类（MLTC）时利用大型语言模型（LLMs）的可能性。GPT-4o-Turbo 被用来进行分类。然而，由于患者评论的敏感性，在将数据喂入LLM之前，通过一个保护健康信息（PHI）检测框架引入了一层安全性，以确保患者的信息去识别。此外，通过提示工程框架，实验了零样本学习、上下文学习和链式思考提示。结果显示，无论是在零样本还是少数样本设置下，GPT-4o-Turbo 都优于传统方法和预训练语言模型（PLMs），并且以 F1 得分为 76.12% 和加权 F1 得分为 73.61% 达到了最佳性能，其次是非样本学习的结果。随后，还分析了结果与患者体验的其他结构变量（例如评分）之间的关系。该研究通过应用LLMs提高了MLTC的效果，为医疗保健从业者提供了一种高效的方法，以深入理解患者反馈并提供及时、适当的响应。"
    },
    {
        "序号": 59,
        "标题": "Tiny Transformers Excel at Sentence Compression",
        "链接": "http://arxiv.org/abs/2410.23510v1",
        "作者": [
            "Peter Belcak",
            "Roger Wattenhofer"
        ],
        "摘要": "It is staggering that words of the English language, which are on average\nrepresented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served\nto large language models. We show that there is room for more information in\nevery token embedding. We demonstrate that 1--3-layer transformers are capable\nof encoding and subsequently decoding standard English sentences into as little\nas a single 3-kilobyte token. Our work implies that even small networks can\nlearn to construct valid English sentences and suggests the possibility of\noptimising large language models by moving from sub-word token embeddings\ntowards larger fragments of text.",
        "分类": [
            "cs.LG",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T23:34:45+00:00",
        "概述": "本文研究动机在于减少语言模型处理大量冗余信息的需求，方法是利用1-3层的小型变压器对英语句子进行压缩，结果表明小型变压器能够将标准英语句子压缩至3KB左右。这表明小型网络可以学习构建有效英文句子，并暗示通过从子词token嵌入转向更大文本片段可能优化大型语言模型。",
        "摘要译文": "令人震惊的是，平均用5到6个ASCII字节表示的英语词汇，在传递给大型语言模型时需要多达24千字节。我们展示了每个词嵌入中仍存在更多的信息空间。我们证明，一层到三层的变压器能够将标准英语句子编码并随后解码为仅一个3千字节的词。我们的研究暗示即使是小型网络也能学习构建有效的英语句子，并提出了通过从子词嵌入向更大文本片段过渡来优化大型语言模型的可能性。"
    },
    {
        "序号": 58,
        "标题": "Dynamic Strategy Planning for Efficient Question Answering with Large Language Models",
        "链接": "http://arxiv.org/abs/2410.23511v1",
        "作者": [
            "Tanmay Parekh",
            "Pradyot Prakash",
            "Alexander Radovic",
            "Akshay Shekher",
            "Denis Savenkov"
        ],
        "摘要": "Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),\nplanning (e.g., SelfAsk), and retrieval augmented generation strategies to\nimprove the performance of Large Language Models (LLMs) on various tasks, such\nas question answering. However, using a single fixed strategy to answer\ndifferent kinds of questions is suboptimal in performance and inefficient in\nterms of generated output tokens and performed retrievals. In our work, we\npropose a novel technique DyPlan, to induce a dynamic strategy selection\nprocess in LLMs, to improve performance and reduce costs in question-answering.\nDyPlan incorporates an initial decision step to select the most suitable\nstrategy conditioned on the input question and guides the LLM's response\ngeneration accordingly. We extend DyPlan to DyPlan-verify, adding an internal\nverification and correction process to further enrich the generated answer.\nExperiments on three prominent multi-hop question answering (MHQA) datasets\nreveal how DyPlan can improve model performance by 7-13% while reducing the\ncost by 11-32% relative to the best baseline model.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "补充信息": "Under review at ACL Rolling Review",
        "日期": "2024-10-30T23:35:21+00:00",
        "概述": "该研究针对大型语言模型在问答任务中的表现不佳和效率问题，提出了一种动态策略选择技术DyPlan。DyPlan能够在接收到问题时选择合适的推理、规划或检索策略，并增强生成答案的过程，从而在三个多跳问答数据集上将模型性能提高7-13%，降低成本11-32%。",
        "摘要译文": "研究显示，在各种任务（如问答）中，推理（例如，Chain-of-Thought）、规划（例如，SelfAsk）和检索增强生成策略能够提升大型语言模型（LLM）的效果。然而，使用单一固定策略来回答不同类型的问答问题是性能上的不足，且在生成输出令牌和执行检索方面效率低下。在我们的工作中，我们提出了一种名为DyPlan的新技术，以在LLM中诱导动态策略选择过程，从而提高问答性能并降低成本。DyPlan结合了一个初始决策步骤，根据输入的问题选择最合适的策略，并相应地指导LLM的响应生成。我们进一步将DyPlan扩展为DyPlan-verify，在生成答案过程中增加了内部验证和修正过程，以进一步丰富生成的答案。在三个主流的多跳问答（MHQA）数据集上的实验结果显示，与最佳基线模型相比，DyPlan可以提高模型性能7-13%，同时将成本降低11-32%。"
    },
    {
        "序号": 60,
        "标题": "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts",
        "链接": "http://arxiv.org/abs/2410.23507v1",
        "作者": [
            "Muhammad Reza Qorib",
            "Alham Fikri Aji",
            "Hwee Tou Ng"
        ],
        "摘要": "Error type information has been widely used to improve the performance of\ngrammatical error correction (GEC) models, whether for generating corrections,\nre-ranking them, or combining GEC models. Combining GEC models that have\ncomplementary strengths in correcting different error types is very effective\nin producing better corrections. However, system combination incurs a high\ncomputational cost due to the need to run inference on the base systems before\nrunning the combination method itself. Therefore, it would be more efficient to\nhave a single model with multiple sub-networks that specialize in correcting\ndifferent error types. In this paper, we propose a mixture-of-experts model,\nMoECE, for grammatical error correction. Our model successfully achieves the\nperformance of T5-XL with three times fewer effective parameters. Additionally,\nour model produces interpretable corrections by also identifying the error type\nduring inference.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "Findings of EMNLP 2024",
        "日期": "2024-10-30T23:27:54+00:00",
        "概述": "本文提出了一种名为MoECE的混合专家模型，用于语法错误修正。该模型旨在高效且可解释地修正语法错误。研究人员发现，通过组合专门针对不同错误类型的子网络，可以减少计算开销并提高性能。实验结果显示，MoECE在参数量减少三分之一的情况下仍能达到T5-XL的性能，并能生成可解释的修正结果。",
        "摘要译文": "错误类型信息已被广泛用于提高语法错误修正（GEC）模型的性能，无论是生成修正、重新排名还是结合GEC模型。将具有不同错误类型修正互补优势的GEC模型结合在一起，非常有效地产生了更好的修正。然而，系统结合需要在运行组合方法之前先对基系统进行推理，这导致了高计算成本。因此，一个具有多个专门修正不同错误类型的子网络的单个模型将更为高效。在本文中，我们提出了一种专家混合模型MoECE，用于语法错误修正。我们的模型成功地实现了T5-XL的性能，但其有效参数仅为T5-XL的三分之一。此外，我们的模型在推理过程中也能生成可解释的修正，并识别出错误类型。"
    },
    {
        "序号": 62,
        "标题": "All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling",
        "链接": "http://arxiv.org/abs/2410.23501v1",
        "作者": [
            "Emanuele Marconato",
            "Sébastien Lachapelle",
            "Sebastian Weichwald",
            "Luigi Gresele"
        ],
        "摘要": "We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors.",
        "分类": [
            "stat.ML",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-30T23:19:29+00:00",
        "概述": "本文分析了可辨别性作为语言模型中普遍存在线性性质（如“easy”和“easiest”与“lucky”和“luckiest”之间的向量差平行）的一种可能解释。研究证明了一个可辨别性结果，以表征分布等价的下一个令牌预测器，并引入了关系线性的新视角。结果表明，在满足一定条件时，这些线性性质要么在所有分布等价的下一个令牌预测器中存在，要么在没有分布等价的下一个令牌预测器中不存在。",
        "摘要译文": "我们分析可识别性是否可以解释语言模型中普遍存在线性性质的现象，例如，“easy”和“easiest”的表示向量差与“lucky”和“luckiest”的表示向量差是平行的。为此，我们提出一个问题：如果在一个模型中找到了一个线性性质，那么任何诱导相同分布的模型是否也具有这种性质？为此，我们首先证明了一个可识别性结果，以表征与分布等价的下一个令牌预测器，提升了一些先前结果的多样性要求。其次，基于对关系线性性的进一步细化[佩卡纳罗和汉金顿, 2001；赫南德兹等人, 2024]，我们展示了有多少线性性质可以适用于我们的分析。最后，我们在适当的条件下证明了这些线性性质要么在所有的、要么在没有任何与分布等价的下一个令牌预测器中成立。"
    },
    {
        "序号": 61,
        "标题": "Learning to Achieve Goals with Belief State Transformers",
        "链接": "http://arxiv.org/abs/2410.23506v1",
        "作者": [
            "Edward S. Hu",
            "Kwangjun Ahn",
            "Qinghua Liu",
            "Haoran Xu",
            "Manan Tomar",
            "Ada Langford",
            "Dinesh Jayaraman",
            "Alex Lamb",
            "John Langford"
        ],
        "摘要": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T23:26:06+00:00",
        "概述": "该研究提出“信念状态变换器”，一种结合前缀和后缀的下一个标记预测模型，旨在预测前缀的下一个标记和后缀的前一个标记。该模型在不依赖特定领域的情况下有效解决传统单向变换器难以处理的挑战性问题。通过学习紧凑的信念状态来捕获所有必要信息以进行准确预测。实验表明，该模型在故事写作任务中即使在未知目标场景下也优于中间填充方法，提高了目标条件解码的效率和测试时的推理能力。",
        "摘要译文": "我们介绍了“信念状态变换器”（Belief State Transformer），这是一种同时接受前缀和后缀作为输入的下一个令牌预测器，其具有新颖的目标，即预测前缀的下一个令牌和后缀的前一个令牌。信念状态变换器能够在不依赖特定领域的情况下，有效学习解决传统单向变换器难以处理的挑战性问题。关键在于学习一个紧凑的信念状态，能够捕捉到所有对准确预测至关重要的相关信息。实证消融实验表明，在标准变换器难以应对的复杂场景中，模型的每个组成部分都是必不可少的。对于带有已知前缀和后缀的故事情节写作任务，我们的方法在实现已知目标方面优于中间填充方法，并且即使目标未知时也能表现出更好的性能。总体而言，信念状态变换器能够更高效地进行目标条件解码，提高测试时推理能力和在小规模问题上生成高质量的文本表示。"
    },
    {
        "序号": 64,
        "标题": "Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs",
        "链接": "http://arxiv.org/abs/2410.23478v1",
        "作者": [
            "Sireesh Gururaja",
            "Yueheng Zhang",
            "Guannan Tang",
            "Tianhao Zhang",
            "Kevin Murphy",
            "Yu-Tsen Yi",
            "Junwon Seo",
            "Anthony Rollett",
            "Emma Strubell"
        ],
        "摘要": "Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science.",
        "分类": [
            "cs.CL",
            "cs.HC"
        ],
        "补充信息": null,
        "日期": "2024-10-30T22:00:34+00:00",
        "概述": "这篇文章介绍了Collage工具，旨在解决科学PDF文档信息提取中模型难以比较和调试的问题。Collage支持快速原型设计、可视化和评估不同信息提取模型，并提供可扩展的软件接口以加速新模型的实验。此外，该工具允许用户和开发者通过查看处理过程的中间状态来检查、调试和更好地理解模型管道。研究团队在材料科学文献回顾中展示了Collage的应用。",
        "摘要译文": "近年来，在自然语言处理（NLP）领域持续开发了针对科学文献的专业信息提取工具，同时越来越多的多模态预训练变换器模型也被发布。尽管非NLP领域的科学家们在评估和应用这些系统来应用于自身领域的机会从未如此明确，但这些模型却难以比较：它们接受不同的输入格式，往往是一个黑盒，很少提供有关处理失败的见解，并且很少处理PDF格式文件，这是科学出版物中最常见的格式。在本项工作中，我们介绍了Collage，一个旨在快速原型设计、可视化和评估不同信息提取模型在科学PDF上的工具。Collage 可以无缝使用和评估任何HuggingFace标记分类器、几种语言模型，以及多个其他任务特定模型，并提供了可扩展的软件接口，以加速对新模型的实验。此外，我们通过提供对处理过程中间状态的详细视图，使NLP工具的开发者和使用者能够检查、调试和更好地理解建模管道。我们以材料科学中的文献回顾为例，展示了该系统的实用性。"
    },
    {
        "序号": 63,
        "标题": "Smaller Large Language Models Can Do Moral Self-Correction",
        "链接": "http://arxiv.org/abs/2410.23496v1",
        "作者": [
            "Guangliang Liu",
            "Zhiyu Xue",
            "Rongrong Wang",
            "Kristen Marie Johnson"
        ],
        "摘要": "Self-correction is one of the most amazing emerging capabilities of Large\nLanguage Models (LLMs), enabling LLMs to self-modify an inappropriate output\ngiven a natural language feedback which describes the problems of that output.\nMoral self-correction is a post-hoc approach correcting unethical generations\nwithout requiring a gradient update, making it both computationally lightweight\nand capable of preserving the language modeling ability. Previous works have\nshown that LLMs can self-debias, and it has been reported that small models,\ni.e., those with less than 22B parameters, are not capable of moral\nself-correction. However, there is no direct proof as to why such smaller\nmodels fall short of moral self-correction, though previous research\nhypothesizes that larger models are skilled in following instructions and\nunderstanding abstract social norms. In this paper, we empirically validate\nthis hypothesis in the context of social stereotyping, through meticulous\nprompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs\nwith proper safety alignment fine-tuning can achieve very good moral\nself-correction performance, highlighting the significant effects of safety\nalignment; and (ii) small LLMs are indeed weaker than larger-scale models in\nterms of comprehending social norms and self-explanation through CoT, but all\nscales of LLMs show bad self-correction performance given unethical\ninstructions.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T22:58:57+00:00",
        "概述": "这项研究旨在探讨小规模语言模型是否能够实现道德自纠行为。动机源于小模型不能进行道德自纠的假设，方法包括对大规模和小规模语言模型进行安全对齐微调，并通过社会 stereotypeprompt进行实验。结果显示，3.8B参数的模型在适当的安全对齐微调后可以很好地实现道德自纠，表明安全对齐的重要性。同时，研究发现小模型在理解社会规范和通过CoT解释自纠方面确实不如大模型，但所有规模的模型在道德故障指令下自纠效果不佳。",
        "摘要译文": "自我修正是一种最大的新兴能力之一，使大型语言模型（LLMs）能够在自然语言反馈描述其输出问题的情况下，自我修改不合适的输出。道德自我修正是一种后验方法，能够在不要求梯度更新的情况下纠正非伦理生成，使其在计算上轻量级，并且能够保留语言建模能力。先前的研究表明，大型语言模型可以自我去偏见，而且据报道，参数少于220亿的小型模型不具备道德自我修正的能力。然而，没有直接证据证明为什么较小型的模型无法实现道德自我修正，尽管先前的研究假设较大的模型更擅长遵循指令并理解抽象的社会规范。在本文中，我们通过细致的提示，在社会刻板印象的背景下实证验证了这一假设。我们的实验结果表明：(i) 惊人的是，经过适当安全对齐微调的38亿参数的LLMs能够实现非常好的道德自我修正性能，突显了安全对齐的显著影响；(ii) 小型语言模型在通过解释性推理理解社会规范方面确实比大型模型薄弱，但在面对非伦理指令时，所有规模的LLMs都表现出不好的自我修正性能。"
    },
    {
        "序号": 65,
        "标题": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following",
        "链接": "http://arxiv.org/abs/2410.23463v1",
        "作者": [
            "Gabrielle Kaili-May Liu",
            "Bowen Shi",
            "Avi Caciularu",
            "Idan Szpektor",
            "Arman Cohan"
        ],
        "摘要": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nchallenges, such as managing inter-document dependencies, redundancy, and\nincoherent structures. We introduce MDCure, a scalable and effective\nfine-tuning pipeline to enhance the MD capabilities of LLMs without the\ncomputational cost of pre-training or reliance on human annotated data. MDCure\nis based on generation of high-quality synthetic MD instruction data from sets\nof related articles via targeted prompts. We further introduce MDCureRM, a\nmulti-objective reward model which filters generated data based on their\ntraining utility for MD settings. With MDCure, we fine-tune a variety of LLMs,\nfrom the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in\nsize. Extensive evaluations on a wide range of MD and long-context benchmarks\nspanning various tasks show MDCure consistently improves performance over\npre-trained baselines and over corresponding base models by up to 75.5%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-30T21:08:07+00:00",
        "概述": "MDCure 是一种用于增强语言模型处理多文档任务能力的可扩展微调管道，无需预训练或依赖人工标注数据。它通过生成高质量的合成多文档指令数据来解决多文档上下文中的挑战，如文档间依赖、冗余和不一致结构。MDCureRM 是一个多目标奖励模型，用于筛选训练数据，以提升多文档场景下的效果。实验结果表明，MDCure 在多种多文档和长上下文基准测试上的性能均优于预训练模型和基础模型，提升幅度高达 75.5%。",
        "摘要译文": "多文档（MD）处理对于LLM处理包括摘要和跨大量文档进行问答在内的实际任务至关重要。尽管LLM在处理长输入方面有所改进，但在MD上下文方面仍存在挑战，例如管理文档间的相互依赖性、冗余性和不一致结构。我们引入了MDCure，这是一种可扩展且有效的微调管道，能够在不增加预训练计算成本或依赖人工标注数据的情况下提升LLM的MD能力。MDCure基于通过目标提示生成的相关文章集的高质量合成MD指令数据集生成。我们还引入了MDCureRM，这是一种多目标奖励模型，根据其在MD设置中的训练 utility 进行生成数据过滤。借助MDCure，我们对FlanT5、Qwen2和LLAMA3.1等多个模型系列进行了微调，参数量高达700亿。广泛的任务和长上下文基准测试的大量评估表明，MDCure在各种MD和长上下文基准测试中始终优于预训练基线和相应的基模型，性能提升最高可达75.5%。我们的代码、数据集和模型可在https://github.com/yale-nlp/MDCure获取。"
    },
    {
        "序号": 66,
        "标题": "Graph-Augmented Relation Extraction Model with LLMs-Generated Support Document",
        "链接": "http://arxiv.org/abs/2410.23452v1",
        "作者": [
            "Vicky Dong",
            "Hao Yu",
            "Yao Chen"
        ],
        "摘要": "This study introduces a novel approach to sentence-level relation extraction\n(RE) that integrates Graph Neural Networks (GNNs) with Large Language Models\n(LLMs) to generate contextually enriched support documents. By harnessing the\npower of LLMs to generate auxiliary information, our approach crafts an\nintricate graph representation of textual data. This graph is subsequently\nprocessed through a Graph Neural Network (GNN) to refine and enrich the\nembeddings associated with each entity ensuring a more nuanced and\ninterconnected understanding of the data. This methodology addresses the\nlimitations of traditional sentence-level RE models by incorporating broader\ncontexts and leveraging inter-entity interactions, thereby improving the\nmodel's ability to capture complex relationships across sentences. Our\nexperiments, conducted on the CrossRE dataset, demonstrate the effectiveness of\nour approach, with notable improvements in performance across various domains.\nThe results underscore the potential of combining GNNs with LLM-generated\ncontext to advance the field of relation extraction.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": null,
        "日期": "2024-10-30T20:48:34+00:00",
        "概述": "本文提出了一种结合图神经网络（GNN）和大型语言模型（LLMs）的新型句子级关系抽取方法。通过利用LLMs生成辅助信息，该方法构建了一个复杂的文本数据图表示，并通过GNN进一步优化每个实体的嵌入，从而更深入地理解数据间的复杂关系。该研究解决了传统句子级关系抽取模型局限于局部上下文的问题，实验结果表明，该方法在跨RE数据集上表现出显著性能提升，验证了结合GNN与LLM生成的上下文对关系抽取领域的潜在促进作用。",
        "摘要译文": "这项研究介绍了一种将图神经网络（GNN）与大型语言模型（LLM）结合用于句级关系提取（RE）的新方法，以生成上下文丰富的支持文档。通过利用LLM生成辅助信息的强大能力，该方法构建了一种复杂的文本数据图形表示。该图随后通过图神经网络（GNN）处理，以改进和丰富与每个实体相关的嵌入，确保对数据有更细腻和互联的理解。这种方法通过引入更广泛的上下文并利用实体之间的相互作用，解决了传统句级RE模型的局限性，从而增强了模型在句子间捕捉复杂关系的能力。我们在CrossRE数据集上的实验表明了该方法的有效性，在各个领域均实现了性能上的显著提升。结果突显了将GNN与LLM生成的上下文结合以推进关系提取领域发展的潜力。"
    },
    {
        "序号": 67,
        "标题": "Learning and Transferring Sparse Contextual Bigrams with Linear Transformers",
        "链接": "http://arxiv.org/abs/2410.23438v1",
        "作者": [
            "Yunwei Ren",
            "Zixuan Wang",
            "Jason D. Lee"
        ],
        "摘要": "Transformers have excelled in natural language modeling and one reason behind\nthis success is their exceptional ability to combine contextual informal and\nglobal knowledge. However, the theoretical basis remains unclear. In this\npaper, first we introduce the Sparse Contextual Bigram (SCB), a natural\nextension of the classical bigram model, where the next token's generation\ndepends on a sparse set of earlier positions determined by the last token. We\nthen analyze the training dynamics and sample complexity of learning SCB using\na one-layer linear transformer with a gradient-based algorithm. We show that\nwhen trained from scratch, the training process can be split into an initial\nsample-intensive stage where the correlation is boosted from zero to a\nnontrivial value, followed by a more sample-efficient stage of further\nimprovement. Additionally, we prove that, provided a nontrivial correlation\nbetween the downstream and pretraining tasks, finetuning from a pretrained\nmodel allows us to bypass the initial sample-intensive stage. We also\nempirically demonstrate that our algorithm can outperform SGD in this setting\nand discuss its relationship with the usual softmax-based transformers.",
        "分类": [
            "cs.LG",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T20:29:10+00:00",
        "概述": "这篇论文研究了如何利用线性变压器学习和迁移稀疏上下文二元组（SCB），这是一种自然语言处理中常见的模型扩展。作者分析了一层线性变压器在基于梯度算法学习SCB时的训练动态和样本复杂度。结果表明，从零开始训练需要大量样本来提升相关性，之后则更高效；并且从预训练模型微调可以跳过初期的样本密集阶段。实验还显示该算法在某些情况下优于SGD。",
        "摘要译文": "变压器在自然语言建模方面表现出色，其中一个原因是它们在结合上下文局部知识和全局知识方面具有卓越的能力。然而，其理论基础依然不够清晰。在这篇论文中，我们首先引入了稀疏上下文二元组（SCB），这是一种经典二元模型的自然扩展，其中下一个标记的生成依赖于由最后一个标记确定的稀疏早期位置集。然后，我们通过一层线性变压器和基于梯度的算法来分析学习SCB的训练动态和样本复杂度。我们证明，在从头训练时，训练过程可以分为一个初始的样本密集阶段，在此阶段中相关性从零提高到一个非平凡值，随后是一个更加样本高效的改进阶段。此外，我们证明，在下游任务与预训练任务之间存在非平凡相关性的情况下，从预训练模型微调可以跳过初始的样本密集阶段。我们还通过实验展示了我们的算法在这种情况下可以超越SGD，并讨论了它与通常的softmax基变压器之间的关系。"
    },
    {
        "序号": 68,
        "标题": "Mind the Gap: A Generalized Approach for Cross-Modal Embedding Alignment",
        "链接": "http://arxiv.org/abs/2410.23437v1",
        "作者": [
            "Arihan Yadav",
            "Alan McMillan"
        ],
        "摘要": "Retrieval-Augmented Generation (RAG) systems enhance text generation by\nincorporating external knowledge but often struggle when retrieving context\nacross different text modalities due to semantic gaps. We introduce a\ngeneralized projection-based method, inspired by adapter modules in transfer\nlearning, that efficiently bridges these gaps between various text types, such\nas programming code and pseudocode, or English and French sentences. Our\napproach emphasizes speed, accuracy, and data efficiency, requiring minimal\nresources for training and inference. By aligning embeddings from heterogeneous\ntext modalities into a unified space through a lightweight projection network,\nour model significantly outperforms traditional retrieval methods like the\nOkapi BM25 algorithm and models like Dense Passage Retrieval (DPR), while\napproaching the accuracy of Sentence Transformers. Extensive evaluations\ndemonstrate the effectiveness and generalizability of our method across\ndifferent tasks, highlighting its potential for real-time, resource-constrained\napplications.",
        "分类": [
            "cs.LG",
            "cs.CL",
            "cs.IR",
            "H.3.3; I.2.7; I.2.6"
        ],
        "补充信息": "18 pages, 3 figures",
        "日期": "2024-10-30T20:28:10+00:00",
        "概述": "本文介绍了一种通用的投影基方法，旨在解决检索增强生成（RAG）系统在不同文本模态（如编程代码和伪代码）之间检索上下文时出现的语义差距问题。该方法借鉴了迁移学习中的适配器模块，通过轻量级的投影网络将异构文本模态的嵌入统一到一个空间中，从而显著提高了生成质量，优于传统的检索方法（如Okapi BM25和DPR模型），接近于句向量变换器的准确性。实验证明该方法在多种任务上表现出色，适用于实时、资源受限的应用。",
        "摘要译文": "检索增强生成（RAG）系统通过结合外部知识来提升文本生成能力，但在不同文本模态间检索上下文时常常遇到语义鸿沟而表现不佳。我们提出了一种基于通用投影的方法，灵感来自迁移学习中的适配器模块，能够高效地连接不同类型的文本，如编程代码和伪代码，或英语和法语句子之间的差距。我们的方法强调速度、准确性和数据效率，训练和推理所需资源极少。通过轻量级的投影网络将异构文本模态的嵌入统一到同一空间中，我们的模型在多项传统检索方法（如Okapi BM25算法）和密集段落检索模型（如DPR）中显著超越，并接近Sentence Transformers的准确度。广泛评估表明，该方法在不同任务上具有有效性和通用性，显示出其在实时、资源受限应用场景中的潜力。"
    },
    {
        "序号": 69,
        "标题": "Social Science Meets LLMs: How Reliable Are Large Language Models in Social Simulations?",
        "链接": "http://arxiv.org/abs/2410.23426v1",
        "作者": [
            "Yue Huang",
            "Zhengqing Yuan",
            "Yujun Zhou",
            "Kehan Guo",
            "Xiangqi Wang",
            "Haomin Zhuang",
            "Weixiang Sun",
            "Lichao Sun",
            "Jindong Wang",
            "Yanfang Ye",
            "Xiangliang Zhang"
        ],
        "摘要": "Large Language Models (LLMs) are increasingly employed for simulations,\nenabling applications in role-playing agents and Computational Social Science\n(CSS). However, the reliability of these simulations is under-explored, which\nraises concerns about the trustworthiness of LLMs in these applications. In\nthis paper, we aim to answer ``How reliable is LLM-based simulation?'' To\naddress this, we introduce TrustSim, an evaluation dataset covering 10\nCSS-related topics, to systematically investigate the reliability of the LLM\nsimulation. We conducted experiments on 14 LLMs and found that inconsistencies\npersist in the LLM-based simulated roles. In addition, the consistency level of\nLLMs does not strongly correlate with their general performance. To enhance the\nreliability of LLMs in simulation, we proposed Adaptive Learning Rate Based\nORPO (AdaORPO), a reinforcement learning-based algorithm to improve the\nreliability in simulation across 7 LLMs. Our research provides a foundation for\nfuture studies to explore more robust and trustworthy LLM-based simulations.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T20:09:37+00:00",
        "概述": "本文旨在探讨大型语言模型（LLMs）在社会模拟中的可靠性，提出了TrustSim评估数据集来系统研究LLM模拟的可靠性。研究发现，LLM在模拟角色时存在不一致性，且模型的性能与其一致性无强关联。为此，提出了一种基于强化学习的AdaORPO算法，以提高多个模型在模拟中的可靠性。研究为未来开发更稳健和可信的LLM社会模拟奠定了基础。",
        "摘要译文": "大型语言模型（LLMs）越来越多地被用于模拟，从而在角色扮演代理和计算社会科学（CSS）等领域得到应用。然而，这些模拟的可靠性尚未得到充分探索，这引发了人们对其在这些应用中的可信度的担忧。在本文中，我们旨在回答“基于LLM的模拟有多可靠？”为了解决这一问题，我们引入了TrustSim，一个涵盖10个CSS相关主题的评估数据集，以系统地考察LLM模拟的可靠性。我们在14个LLM上进行了实验，并发现LLM基于模拟的角色仍然存在不一致之处。此外，LLM的一致性水平并不强烈地与它们的一般性能相关。为了提高LLM在模拟中的可靠性，我们提出了基于强化学习的自适应学习率ORPO（AdaORPO）算法，以在7个LLM上提高模拟的可靠性。我们的研究为未来的研究探索更稳健和可信的LLM基于模拟奠定了基础。"
    },
    {
        "序号": 71,
        "标题": "Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists",
        "链接": "http://arxiv.org/abs/2410.23331v1",
        "作者": [
            "Michał Pietruszka",
            "Łukasz Borchmann",
            "Aleksander Jędrosz",
            "Paweł Morawiecki"
        ],
        "摘要": "We present a benchmark for large language models designed to tackle one of\nthe most knowledge-intensive tasks in data science: writing feature engineering\ncode, which requires domain knowledge in addition to a deep understanding of\nthe underlying problem and data structure. The model is provided with a dataset\ndescription in a prompt and asked to generate code transforming it. The\nevaluation score is derived from the improvement achieved by an XGBoost model\nfit on the modified dataset compared to the original data. By an extensive\nevaluation of state-of-the-art models and comparison to well-established\nbenchmarks, we demonstrate that the FeatEng of our proposal can cheaply and\nefficiently assess the broad capabilities of LLMs, in contrast to the existing\nmethods.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T17:59:01+00:00",
        "概述": "该研究旨在评估大语言模型（LLM）在数据科学中进行特征工程的能力。通过自动生成特征工程代码，模型能够利用领域知识和数据理解。评价方法基于xgboost模型在修改后的数据集上的表现与原始数据集的比较。研究结果表明，该方法可以经济高效地评估LLM的广泛能力，超越现有方法。",
        "摘要译文": "我们提出了一项基准测试，旨在解决数据科学中知识密集度最高的一项任务：编写特征工程代码，这不仅需要领域知识，还需要对底层问题和数据结构有深刻的理解。模型会根据数据集描述生成代码进行转化。评估分数来自于使用修改后数据集拟合的XGBoost模型相较于原始数据所取得的改进。通过广泛评估最先进的模型并将我们的方法与现有的基准进行比较，我们证明了本文提案中的特征工程可以廉价且高效地评估大型语言模型的广泛能力，而现有方法则不具备这一优势。"
    },
    {
        "序号": 70,
        "标题": "Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric Vehicles",
        "链接": "http://arxiv.org/abs/2410.23371v1",
        "作者": [
            "Keiichi Namikoshi",
            "David A. Shamma",
            "Rumen Iliev",
            "Jingchao Fang",
            "Alexandre Filipowicz",
            "Candice L Hogan",
            "Charlene Wu",
            "Nikos Arechiga"
        ],
        "摘要": "Behavior change interventions are important to coordinate societal action\nacross a wide array of important applications, including the adoption of\nelectrified vehicles to reduce emissions. Prior work has demonstrated that\ninterventions for behavior must be personalized, and that the intervention that\nis most effective on average across a large group can result in a backlash\neffect that strengthens opposition among some subgroups. Thus, it is important\nto target interventions to different audiences, and to present them in a\nnatural, conversational style. In this context, an important emerging\napplication domain for large language models (LLMs) is conversational\ninterventions for behavior change. In this work, we leverage prior work on\nunderstanding values motivating the adoption of battery electric vehicles. We\nleverage new advances in LLMs, combined with a contextual bandit, to develop\nconversational interventions that are personalized to the values of each study\nparticipant. We use a contextual bandit algorithm to learn to target values\nbased on the demographics of each participant. To train our bandit algorithm in\nan offline manner, we leverage LLMs to play the role of study participants. We\nbenchmark the persuasive effectiveness of our bandit-enhanced LLM against an\nunaided LLM generating conversational interventions without\ndemographic-targeted values.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T18:15:43+00:00",
        "概述": "本文研究动机在于促进电池电动汽车的普及，以减少排放。它解决了行为干预必须个性化的问题，并且常规平均有效的干预措施可能会强化某些子群体的反对情绪。因此，文章利用语言模型和上下文臂部算法开发了个性化干预措施，以便根据不同参与者的价值观进行定制。研究使用上下文臂部算法基于参与者的 demographic 对目标价值观进行学习，并通过语言模型模拟研究参与者进行离线训练。结果表明，结合臂部算法的语言模型在说服效果上优于仅使用语言模型生成干预措施的方法。",
        "摘要译文": "行为改变干预对于协调广泛重要应用的社会行动至关重要，包括采用电动车辆以减少排放。先前的研究表明，行为干预必须个性化，针对大规模人群最有效的干预措施可能会在某些子群体中引发反效果，从而增强其反对情绪。因此，重要的是将干预措施针对不同的受众，并以自然、对话的方式呈现。在此背景下，大型语言模型（LLMs）的一个重要新兴应用领域是对行为改变的对话式干预。在这项工作中，我们借鉴了关于理解推动电池电动汽车采用的价值观的先前研究成果。我们利用最新的LLM进展，结合上下文多臂_bandit_算法，开发个性化的对话式干预措施，使其符合每位研究参与者的价值观。我们使用上下文多臂_bandit_算法，根据每位参与者的人口统计学特征来学习目标价值观。为了以离线方式训练我们的多臂_bandit_算法，我们利用LLM扮演研究参与者的角色。我们将我们的多臂_bandit_增强的LLM说服有效性与其未借助人口统计学目标价值观生成对话式干预措施的LLM进行了基准测试。"
    },
    {
        "序号": 72,
        "标题": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
        "链接": "http://arxiv.org/abs/2410.23277v1",
        "作者": [
            "Yining Hong",
            "Beide Liu",
            "Maxine Wu",
            "Yuanhao Zhai",
            "Kai-Wei Chang",
            "Lingjie Li",
            "Kevin Lin",
            "Chung-Ching Lin",
            "Jianfeng Wang",
            "Zhengyuan Yang",
            "Yingnian Wu",
            "Lijuan Wang"
        ],
        "摘要": "Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io",
        "分类": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.RO"
        ],
        "补充信息": null,
        "日期": "2024-10-30T17:55:52+00:00",
        "概述": "该研究旨在解决长视频生成中长期帧间不一致的问题。通过引入SlowFast-VGen，该系统利用慢速学习模块捕捉世界动态，快速学习模块更新局部记忆参数，实现 episodic 记忆存储。研究提出了慢速-快速学习循环算法，将局部快速学习嵌入全局慢速学习，增强多场景理解。实验表明，与基线相比，SlowFast-VGen 的 FVD 得分更低，长视频一致性更高，且在长期规划任务上表现出色。",
        "摘要译文": "人类拥有互补的学习系统，将缓慢的学习世界动力学与从新体验中快速存储 episodic 记忆相结合。然而，之前的视频生成模型主要侧重于通过大规模数据预训练来进行缓慢学习，忽视了对于 episodic 记忆存储至关重要的快速学习阶段。这种忽视导致在生成较长视频时，时间上相隔较远的帧之间出现了不一致，因为这些帧超出了模型的上下文窗口。为此，我们提出了一种新的双速学习系统 SlowFast-VGen，用于基于动作的长视频生成。我们的方法结合了掩码条件视频弥散模型，用于缓慢学习世界动力学，以及以时间 LoRA 模块为基础的推理时快速学习策略。具体来说，快速学习过程根据局部输入和输出更新其时间 LoRA 参数，从而在其参数中高效地存储 episodic 记忆。我们还提出了一种慢速-快速学习循环算法，无缝地将内部快速学习循环集成到外部缓慢学习循环中，使模型能够在上下文感知环境中回忆起先前的多回目经历，从而进行技能学习。为了促进对近似世界模型的缓慢学习，我们收集了一个包含 200,000 个视频的大规模数据集，并带有语言动作注释，涵盖了多种场景。广泛的实验表明，SlowFast-VGen 在多个视频生成指标上优于基线模型，FVD 得分为 514，而基线模型为 782，并且在更长的视频中保持一致性，平均场景剪辑数为 0.37 与 0.89。慢速-快速学习循环算法还显著提升了长期规划任务的表现。\n项目网站：https://slowfast-vgen.github.io"
    },
    {
        "序号": 75,
        "标题": "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources",
        "链接": "http://arxiv.org/abs/2410.23261v1",
        "作者": [
            "Apoorv Khandelwal",
            "Tian Yun",
            "Nihal V. Nayak",
            "Jack Merullo",
            "Stephen H. Bach",
            "Chen Sun",
            "Ellie Pavlick"
        ],
        "摘要": "Pre-training is notoriously compute-intensive and academic researchers are\nnotoriously under-resourced. It is, therefore, commonly assumed that academics\ncan't pre-train models. In this paper, we seek to clarify this assumption. We\nfirst survey academic researchers to learn about their available compute and\nthen empirically measure the time to replicate models on such resources. We\nintroduce a benchmark to measure the time to pre-train models on given GPUs and\nalso identify ideal settings for maximizing training speed. We run our\nbenchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on\nour experiments. Our results reveal a brighter picture for academic\npre-training: for example, although Pythia-1B was originally trained on 64 GPUs\nfor 3 days, we find it is also possible to replicate this model (with the same\nhyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude\nwith a cost-benefit analysis to help clarify the trade-offs between price and\npre-training time. We believe our benchmark will help academic researchers\nconduct experiments that require training larger models on more data. We fully\nrelease our codebase at: https://github.com/apoorvkh/academic-pretraining.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-30T17:46:20+00:00",
        "概述": "本文旨在探讨学术研究人员在有限计算资源下进行模型预训练的可能性。研究通过调查学术资源可用性、实验测量训练时间，并建立基准来评估不同GPU上模型预训练所需时间。结果显示，某些模型在较少的GPU天数内即可训练完成，如Pythia-1B模型在原有64 GPUs 3天基础上，可在4 GPUs 18天内复制完成。研究最后进行了成本效益分析，帮助学术研究人员权衡价格与训练时间的trade-off，该基准将有助于学术研究人员更高效地进行实验。",
        "摘要译文": "预训练 notoriously 计算密集型，而学术研究人员 notoriously 资源有限。因此，普遍认为学术研究人员无法进行预训练。在本文中，我们旨在澄清这一假设。我们首先调查了学术研究人员可用的计算资源，然后实证测量在这些资源上复制模型所需的时间。我们引入了一个基准，用于衡量在给定 GPU 上预训练模型所需的时间，并确定了最大化训练速度的最佳设置。我们在各种模型和学术 GPU 上运行了我们的基准测试，总共花费了 2000 个 GPU 小时进行实验。我们的结果显示了学术预训练更为乐观的前景：例如，尽管 Pythia-1B 原始训练使用了 64 个 GPU，耗时 3 天，但我们发现使用相同的超参数，也可以在 3 倍少的 GPU 日（即 4 个 GPU，18 天）上复制此模型。最后，我们进行了一项成本效益分析，以帮助明确价格与预训练时间之间的权衡。我们相信我们的基准测试将有助于学术研究人员开展需要在更多数据上训练更大模型的实验。我们的代码库已完全开源：https://github.com/apoorvkh/academic-pretraining。"
    },
    {
        "序号": 74,
        "标题": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "链接": "http://arxiv.org/abs/2410.23262v1",
        "作者": [
            "Jyh-Jing Hwang",
            "Runsheng Xu",
            "Hubert Lin",
            "Wei-Chih Hung",
            "Jingwei Ji",
            "Kristy Choi",
            "Di Huang",
            "Tong He",
            "Paul Covington",
            "Benjamin Sapp",
            "James Guo",
            "Dragomir Anguelov",
            "Mingxing Tan"
        ],
        "摘要": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.",
        "分类": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.RO"
        ],
        "补充信息": "Blog post: https://waymo.com/blog/2024/10/introducing-emma/",
        "日期": "2024-10-30T17:46:31+00:00",
        "概述": "EMMA是一种用于自主驾驶的端到端多模态模型，它基于大规模语言模型构建，直接将相机传感器数据映射为规划轨迹、感知对象和道路图元等驾驶特定输出。通过将所有非传感器输入和输出表示为自然语言文本，EMMA能够在统一的语言空间中处理各种驾驶任务，并通过任务特定提示生成输出。实验结果显示，EMMA在nuScenes运动规划和Waymo Open Motion Dataset（WOMD）中达到最先进的性能，并在Waymo Open Dataset（WOD）中的3D物体检测任务中取得竞争力结果。然而，EMMA存在处理图像帧量少、不包含准确的3D传感模态以及计算成本高等局限性。",
        "摘要译文": "我们引入了EMMA，这是一种端到端的多模态模型，用于自动驾驶。\n基于多模态大语言模型的基础，EMMA 直接将原始相机传感器数据映射到各种驾驶特定的输出，包括规划轨迹、感知对象和道路图元素。EMMA 通过将所有非传感器输入（例如导航指令和车辆状态）和输出（例如轨迹和三维位置）表示为自然语言文本，最大化了预训练大语言模型的世界知识的利用价值。这种做法使得EMMA可以在统一的语言空间中联合处理各种驾驶任务，使用特定任务的提示生成每个任务的输出。实验表明，EMMA 在 nuScenes 的运动规划方面取得了领先性能，并在 Waymo 开放运动数据集（WOMD）上取得了竞争力的结果。EMMA 在 Waymo 开放数据集（WOD）上的相机为主三维对象检测任务也表现出竞争力的结果。我们表明，通过协同训练规划轨迹、物体检测和道路图任务，EMMA 在这三个领域均表现出改进，突显了EMMA作为自动驾驶应用通用模型的潜力。然而，EMMA 也存在某些局限性：它只能处理少量的图片帧，不包含如 LiDAR 或雷达等准确的三维传感模态，并且计算成本较高。我们希望我们的结果能够激发进一步的研究来缓解这些问题，并进一步推动自动驾驶模型架构的发展状态。"
    },
    {
        "序号": 73,
        "标题": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models",
        "链接": "http://arxiv.org/abs/2410.23266v1",
        "作者": [
            "Ziyao Shangguan",
            "Chuhan Li",
            "Yuxuan Ding",
            "Yanan Zheng",
            "Yilun Zhao",
            "Tesca Fitzgerald",
            "Arman Cohan"
        ],
        "摘要": "Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality.",
        "分类": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T17:50:23+00:00",
        "概述": "这篇文章旨在评估多模态基础模型在视频理解中的视觉时间推理能力。现有基准可能高估了模型的时间推理能力，因此作者提出了TOMATO（Temporal Reasoning Multimodal Evaluation）评估框架，通过三个原则（多帧增益、帧顺序敏感性和帧信息差异）和1484个精心标注的问题，系统地评估模型的时间推理能力。结果显示，最优秀的模型与人类的表现差距为57.3%，揭示了当前多模态模型在将独立帧视为连续序列进行解释上的局限性。",
        "摘要译文": "现有的基准通常强调了多模态基础模型（MFMs）在利用时间上下文进行视频理解方面取得的出色性能。然而，这些模型在视觉时序推理方面真正表现如何？我们的研究显示，这些基准中MFMs的能力可能被高估了，因为许多问题可以通过使用单一帧、少量帧或顺序错乱的帧来解决。为系统地检查当前的视觉时序推理任务，我们提出了三个原则及其对应的度量标准：（1）多帧增益、（2）帧序敏感性和（3）帧信息差异。遵循这些原则，我们引入了TOMATO——时序推理多模态评估，这是一种新型基准，旨在严格评估MFMs在视频理解中的时序推理能力。TOMATO包含1484个精心策划、由人类标注的问题，覆盖了六个任务（即动作计数、方向、旋转、形状与趋势、速度与频率、以及视觉线索），应用到了1417个视频，包括805个自我录制和生成的视频，这些视频涵盖了人类为中心的实际场景以及模拟场景。全面的评估显示，最佳模型与人类的表现差距为57.3%。此外，我们的深度分析还揭示了当前MFMs超出这一差距的更根本的局限性。虽然它们能够准确识别孤立帧中的事件，但在将这些帧视为连续序列时却不能进行解释。我们相信，TOMATO将成为评估下一代MFMs的关键测试平台，并呼吁社区开发能够通过视频模态理解人类世界动态的AI系统。"
    },
    {
        "序号": 76,
        "标题": "Evaluating Cultural and Social Awareness of LLM Web Agents",
        "链接": "http://arxiv.org/abs/2410.23252v1",
        "作者": [
            "Haoyi Qiu",
            "Alexander R. Fabbri",
            "Divyansh Agarwal",
            "Kung-Hsiang Huang",
            "Sarah Tan",
            "Nanyun Peng",
            "Chien-Sheng Wu"
        ],
        "摘要": "As large language models (LLMs) expand into performing as agents for\nreal-world applications beyond traditional NLP tasks, evaluating their\nrobustness becomes increasingly important. However, existing benchmarks often\noverlook critical dimensions like cultural and social awareness. To address\nthese, we introduce CASA, a benchmark designed to assess LLM agents'\nsensitivity to cultural and social norms across two web-based tasks: online\nshopping and social discussion forums. Our approach evaluates LLM agents'\nability to detect and appropriately respond to norm-violating user queries and\nobservations. Furthermore, we propose a comprehensive evaluation framework that\nmeasures awareness coverage, helpfulness in managing user queries, and the\nviolation rate when facing misleading web content. Experiments show that\ncurrent LLMs perform significantly better in non-agent than in web-based agent\nenvironments, with agents achieving less than 10% awareness coverage and over\n40% violation rates. To improve performance, we explore two methods: prompting\nand fine-tuning, and find that combining both methods can offer complementary\nadvantages -- fine-tuning on culture-specific datasets significantly enhances\nthe agents' ability to generalize across different regions, while prompting\nboosts the agents' ability to navigate complex tasks. These findings highlight\nthe importance of constantly benchmarking LLM agents' cultural and social\nawareness during the development cycle.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "Work in progress",
        "日期": "2024-10-30T17:35:44+00:00",
        "概述": "这篇论文探讨了大型语言模型（LLM）在作为web代理应用时的文化和社会意识评估。研究动机是鉴于LLM在传统NLP任务之外的应用日益增多，现有基准往往忽视文化和社会意识维度。为此，作者提出了CASA基准，评估LLM代理在在线购物和社交讨论论坛任务中的文化和社会规范敏感性。实验结果显示，当前LLM在web代理环境中的表现远逊于非代理环境，覆盖度不足10%，违例率超过40%。通过组合微调和提示的方法，可以提升LLM代理的性能。",
        "摘要译文": "随着大型语言模型（LLMs）扩展到执行超越传统自然语言处理任务的真实世界应用，评估其稳健性变得越来越重要。然而，现有的基准常常忽视了诸如文化和社会意识等关键维度。为了解决这些问题，我们提出了CASA基准，这是一个旨在评估LLM代理在跨两个基于网络的任务（在线购物和社会讨论论坛）中对文化和社会规范敏感性的基准。我们的方法评估了LLM代理检测和适当响应规范违反的用户查询和观察的能力。此外，我们提出了一种全面的评估框架，该框架衡量意识覆盖率、管理用户查询的有用性以及面对误导性网络内容时的违规率。实验结果显示，当前的LLM在非代理环境中的表现明显优于基于网络的代理环境，代理仅实现了不到10%的意识覆盖率，而违规率超过40%。为了提高性能，我们探索了两种方法：提示和微调，并发现结合这两种方法可以提供互补的优势——在特定文化数据集上进行微调显著增强了代理跨不同地区的泛化能力，而提示则增强了代理处理复杂任务的能力。这些发现突显了在开发过程中不断基准测试LLM代理的文化和社会意识的重要性。"
    },
    {
        "序号": 79,
        "标题": "Reliability of Topic Modeling",
        "链接": "http://arxiv.org/abs/2410.23186v1",
        "作者": [
            "Kayla Schroeder",
            "Zach Wood-Doughty"
        ],
        "摘要": "Topic models allow researchers to extract latent factors from text data and\nuse those variables in downstream statistical analyses. However, these\nmethodologies can vary significantly due to initialization differences,\nrandomness in sampling procedures, or noisy data. Reliability of these methods\nis of particular concern as many researchers treat learned topic models as\nground truth for subsequent analyses. In this work, we show that the standard\npractice for quantifying topic model reliability fails to capture essential\naspects of the variation in two widely-used topic models. Drawing from a\nextensive literature on measurement theory, we provide empirical and\ntheoretical analyses of three other metrics for evaluating the reliability of\ntopic models. On synthetic and real-world data, we show that McDonald's\n$\\omega$ provides the best encapsulation of reliability. This metric provides\nan essential tool for validation of topic model methodologies that should be a\nstandard component of any topic model-based research.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T16:42:04+00:00",
        "概述": "这篇论文探讨了主题模型的可靠性问题，指出现有方法在捕捉不同初始化和随机抽样等因素导致的变异性方面存在不足。作者借鉴测量理论，提出了三种新的可靠性评估指标，并通过实证分析表明McDonald's $\\omega$是最佳选择。研究结果强调了在基于主题模型的研究中采用McDonald's $\\omega$的重要性。",
        "摘要译文": "主题模型允许研究人员从文本数据中提取潜在因素，并将这些变量用于下游统计分析。然而，这些方法由于初始化差异、采样过程中的随机性或噪声数据等原因可能会有很大差异。由于许多研究人员将学习到的主题模型视为后续分析中的事实真相，这些方法的可靠性尤为重要。在本工作中，我们展示了标准的主题模型可靠性的量化方法未能捕捉到两种广泛使用主题模型中关键的变异方面。借鉴测量理论的广泛文献，我们提供了三种评估主题模型可靠性的其他指标的实证和理论分析。在合成和真实数据上，我们表明麦考登的$\\omega$系数提供了对可靠性的最佳概括。这一指标为验证主题模型方法提供了一个必不可少的工具，而这种工具应成为任何基于主题模型的研究的标准组成部分。"
    },
    {
        "序号": 77,
        "标题": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences",
        "链接": "http://arxiv.org/abs/2410.23223v1",
        "作者": [
            "Yixin Liu",
            "Argyris Oikonomou",
            "Weiqiang Zheng",
            "Yang Cai",
            "Arman Cohan"
        ],
        "摘要": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.GT"
        ],
        "补充信息": null,
        "日期": "2024-10-30T17:13:02+00:00",
        "概述": "该研究针对现有方法如RLHF无法充分捕捉人类偏好范围的问题，提出了一种新的元算法COMAL。COMAL将对齐问题建模为两人零和博弈，旨在找到纳什均衡策略，确保在对抗任何其他策略时维持50%的胜率。研究证明该算法在最后一次迭代中可精确收敛到纳什策略，并且易于与现有方法集成。实验结果表明，结合COMAL和现有偏好策略优化方法可以有效提升对齐效果。",
        "摘要译文": "许多对齐方法，包括从人类反馈中强化学习（RLHF），依赖于布雷德利-特利（Bradley-Terry）奖励假设，但该假设无法完全捕捉人类的广泛偏好。为了实现对广泛偏好的鲁棒对齐，我们将对齐问题建模为两人零和博弈，其中纳什均衡策略在与任何竞争策略对抗时确保胜率为50%。然而，之前用于寻找纳什策略的算法要么发散，要么在修改后的游戏中收敛到一个纳什策略，即使在简单合成设置中也是如此，从而无法保持对所有其他策略的50%胜率保证。我们提出了一种元算法——收敛元对齐算法（COMAL）——用于具有广泛偏好的语言模型对齐，该算法灵感来源于博弈论中的收敛算法。\n理论上，我们证明了我们的元算法在最后一次迭代中收敛到精确的纳什策略。此外，我们的元算法简单，可以最小化修改地与许多现有针对RLHF和偏好优化的方法结合使用。实验结果证明了在与现有偏好策略优化方法结合使用时，该框架的有效性。"
    },
    {
        "序号": 78,
        "标题": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "链接": "http://arxiv.org/abs/2410.23218v1",
        "作者": [
            "Zhiyong Wu",
            "Zhenyu Wu",
            "Fangzhi Xu",
            "Yian Wang",
            "Qiushi Sun",
            "Chengyou Jia",
            "Kanzhi Cheng",
            "Zichen Ding",
            "Liheng Chen",
            "Paul Pu Liang",
            "Yu Qiao"
        ],
        "摘要": "Existing efforts in building GUI agents heavily rely on the availability of\nrobust commercial Vision-Language Models (VLMs) such as GPT-4o and\nGeminiProVision. Practitioners are often reluctant to use open-source VLMs due\nto their significant performance lag compared to their closed-source\ncounterparts, particularly in GUI grounding and Out-Of-Distribution (OOD)\nscenarios. To facilitate future research in this area, we developed OS-Atlas -\na foundational GUI action model that excels at GUI grounding and OOD agentic\ntasks through innovations in both data and modeling. We have invested\nsignificant engineering effort in developing an open-source toolkit for\nsynthesizing GUI grounding data across multiple platforms, including Windows,\nLinux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing\nthe largest open-source cross-platform GUI grounding corpus to date, which\ncontains over 13 million GUI elements. This dataset, combined with innovations\nin model training, provides a solid foundation for OS-Atlas to understand GUI\nscreenshots and generalize to unseen interfaces. Through extensive evaluation\nacross six benchmarks spanning three different platforms (mobile, desktop, and\nweb), OS-Atlas demonstrates significant performance improvements over previous\nstate-of-the-art models. Our evaluation also uncovers valuable insights into\ncontinuously improving and scaling the agentic capabilities of open-source\nVLMs.",
        "分类": [
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "补充信息": null,
        "日期": "2024-10-30T17:10:19+00:00",
        "概述": "该研究旨在克服开源视觉-语言模型在GUI对接和OOD场景下的性能短板，通过创新数据和建模方法开发OS-Atlas，一种基础GUI操作模型。研究团队开发了一套开源工具，集成跨平台的GUI数据合成，构建了迄今最大的跨平台GUI数据集，包含逾1300万GUI元素。实验结果显示，OS-Atlas在六项跨平台基准测试中表现优于现有模型，展现出在开源视觉-语言模型中的先进能力。",
        "摘要译文": "现有的GUI代理构建工作绝大部分依赖于稳健的商业视觉-语言模型（如GPT-4o和GeminiProVision）。由于开源视觉-语言模型与闭源模型相比在性能上存在显著差距，特别是在GUI定位和离群分布（OOD）场景中，实践者通常不愿意使用开源模型。为了促进该领域的未来研究，我们开发了OS-Atlas——一种基础的GUI动作模型，通过数据和模型上的创新，在GUI定位和OOD代理任务方面表现出色。我们投入了大量的工程努力，开发了一个开源工具包，用于在Windows、Linux、MacOS、Android和网页等多个平台合成GUI定位数据。利用这一工具包，我们发布了迄今为止最大的跨平台开源GUI定位语料库，包含超过1300万个GUI元素。结合模型训练上的创新，OS-Atlas为理解GUI屏幕截图并将这些知识应用于未见过的界面奠定了坚实基础。通过在三个不同平台（移动、桌面和网页）的六个基准测试中的广泛评估，OS-Atlas在以前的最先进的模型上展示了显著的性能提升。我们的评估还揭示了不断改进和扩展开源视觉-语言模型代理能力的宝贵见解。"
    },
    {
        "序号": 80,
        "标题": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
        "链接": "http://arxiv.org/abs/2410.23182v1",
        "作者": [
            "Zhichao Hou",
            "Weizhi Gao",
            "Yuchen Shen",
            "Feiyi Wang",
            "Xiaorui Liu"
        ],
        "摘要": "Transformer-based architectures have dominated various areas of machine\nlearning in recent years. In this paper, we introduce a novel robust attention\nmechanism designed to enhance the resilience of transformer-based\narchitectures. Crucially, this technique can be integrated into existing\ntransformers as a plug-and-play layer, improving their robustness without the\nneed for additional training or fine-tuning. Through comprehensive experiments\nand ablation studies, we demonstrate that our ProTransformer significantly\nenhances the robustness of transformer models across a variety of prediction\ntasks, attack mechanisms, backbone architectures, and data domains. Notably,\nwithout further fine-tuning, the ProTransformer consistently improves the\nperformance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT,\nALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler\nattack. Furthermore, ProTransformer shows promising resilience in large\nlanguage models (LLMs) against prompting-based attacks, improving the\nperformance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing\nVicuna by an average of 10.4% against the Jailbreaking attack. Beyond the\nlanguage domain, ProTransformer also demonstrates outstanding robustness in\nboth vision and graph domains.",
        "分类": [
            "cs.LG",
            "cs.CL",
            "cs.CR"
        ],
        "补充信息": null,
        "日期": "2024-10-30T16:38:09+00:00",
        "概述": "本文介绍了一种名为ProTransformer的新颖鲁棒注意力机制，旨在增强基于Transformer架构的模型的鲁棒性，且可以无缝集成到现有Transformer中。研究表明，ProTransformer在多种任务、攻击机制、基础架构和数据领域中显著提升了模型的鲁棒性。实验证实，ProTransformer在对抗TextFooler攻击时，分别将BERT、ALBERT、DistilBERT和RoBERTa的性能提升19.5%、28.3%、16.1%和11.4%。此外，在面对大型语言模型的提示攻击时，ProTransformer也展示了良好的鲁棒性，如将T5和LLaMA的性能分别提升24.8%和17.8%，并平均提升Vicuna在Jailbreaking攻击下的性能10.4%。",
        "摘要译文": "基于Transformer的架构近年来在机器学习的各个领域占据主导地位。在本文中，我们介绍了一种新颖的鲁棒注意力机制，旨在增强基于Transformer架构的鲁棒性。 crucial 地，该技术可以无缝整合到现有的Transformer中，作为即插即用层，而不需额外的训练或微调即可提高其鲁棒性。通过全面的实验和消融研究，我们证明我们的ProTransformer显着增强了Transformer模型在各种预测任务、攻击机制、骨干架构和数据领域的鲁棒性。值得注意的是，在古典TextFooler攻击下，ProTransformer在不需进一步微调的情况下，分别提高了BERT、ALBERT、DistilBERT和RoBERTa的性能，提升幅度分别为19.5%、28.3%、16.1%和11.4%。此外，ProTransformer在大型语言模型（LLMs）中也展现出对提示攻击的强大鲁棒性，分别提高了T5和LLaMA的性能24.8%和17.8%，并平均提高了Vicuna在Jailbreaking攻击下的性能10.4%。此外，ProTransformer还在视觉和图论领域展现了卓越的鲁棒性。"
    },
    {
        "序号": 82,
        "标题": "The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in Lie Detection",
        "链接": "http://arxiv.org/abs/2410.23143v1",
        "作者": [
            "Haimanti Bhattacharya",
            "Subhasish Dugar",
            "Sanchaita Hazra",
            "Bodhisattwa Prasad Majumder"
        ],
        "摘要": "We investigate how low-quality AI advisors, lacking quality disclosures, can\nhelp spread text-based lies while seeming to help people detect lies.\nParticipants in our experiment discern truth from lies by evaluating\ntranscripts from a game show that mimicked deceptive social media exchanges on\ntopics with objective truths. We find that when relying on low-quality advisors\nwithout disclosures, participants' truth-detection rates fall below their own\nabilities, which recovered once the AI's true effectiveness was revealed.\nConversely, high-quality advisor enhances truth detection, regardless of\ndisclosure. We discover that participants' expectations about AI capabilities\ncontribute to their undue reliance on opaque, low-quality advisors.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.CY",
            "cs.HC",
            "cs.LG"
        ],
        "补充信息": "Order of the authors are in alphabetical order of their last names.\n  All authors contributed equally. The manuscript is under review. 74 Pages,\n  including appendices and references",
        "日期": "2024-10-30T15:58:05+00:00",
        "概述": "该研究调查了低质量AI顾问在不提供质量披露的情况下如何助 falsified信息扩散，并让人误以为这些顾问能帮助辨别虚假信息。研究通过实验让参与者评估游戏节目中模仿社交媒体上的虚假交流的剧本来辨别真伪。结果发现，依赖低质量无披露的AI顾问会使鉴定准确率下降，但高质量顾问即便不披露也能提升鉴定准确率。研究指出，人们对AI能力的期望导致了对其不透明、低质量顾问的过度依赖。",
        "摘要译文": "我们研究了缺乏质量披露的低质量AI顾问如何帮助传播文本形式的谎言，同时似乎还能帮助人们检测谎言。在我们的实验中，参与者通过评估模仿 deceiving 社交媒体交流的真人秀节目的对话转录来辨别事实与谎言，这些话题具有客观真伪。我们发现，当依赖低质量且无披露的顾问时，参与者的事实检测率会低于他们自身的水平，而在揭示AI的真实有效性后，这一情况得以恢复。相反，高质量的顾问无论是否有披露都能增强事实检测的能力。我们发现，参与者对AI能力的期望促进了他们对不透明、低质量顾问的过度依赖。"
    },
    {
        "序号": 81,
        "标题": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
        "链接": "http://arxiv.org/abs/2410.23166v1",
        "作者": [
            "Wenxiao Wang",
            "Lihui Gu",
            "Liye Zhang",
            "Yunxiang Luo",
            "Yi Dai",
            "Chen Shen",
            "Liang Xie",
            "Binbin Lin",
            "Xiaofei He",
            "Jieping Ye"
        ],
        "摘要": "The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.IR",
            "cs.LG"
        ],
        "补充信息": "25 pages, 5 figures, 19 tables",
        "日期": "2024-10-30T16:18:22+00:00",
        "概述": "这项研究表明，随着知识的快速增长和跨学科研究的复杂性增加，研究人员面临着信息过载和创新想法探索的挑战。SciPIP 是一种基于大语言模型的科学论文创意提案工具，通过文献检索和利用大语言模型生成创意，帮助研究人员提出新颖且可行的科学论文想法。该方法构建了一个包含多维信息的文献检索数据库，并提出了一种基于语义、实体和引用共现的检索方法。此外，还引入了双路径创意生成策略，结合检索和模型自动生成创意，以平衡创新性和实用性。实验表明，SciPIP 能够检索到类似于顶级会议论文的引文，并生成大量与它们一致的想法，验证了方法的有效性。",
        "摘要译文": "知识的指数增长和跨学科研究的不断增加的复杂性给研究人员带来了重大挑战，包括信息过载和探索新颖想法的困难。大型语言模型（LLMs）的发展，如GPT-4，展示了增强想法提案的巨大潜力，但如何有效利用这些大型模型进行合理的想法提案尚未得到充分探索。本文提出了一种科学论文想法提案器（SciPIP）。基于用户提供的研究背景，SciPIP从文献数据库中检索有用的论文，并利用LLMs的能力生成更多新颖和可行的想法。为此，\n1) 我们构建了一个文献检索数据库，提取了许多论文的多维度信息以实现快速访问。然后，提出了一种基于语义、实体和引用共现的文献检索方法，根据用户提供的背景从多个方面检索相关文献。\n2) 在文献检索之后，我们引入了双路径想法提案策略，其中一条路径从检索到的文献中推断解决方案，另一条路径通过模型头脑风暴生成原创想法。然后将两者结合起来，在可行性和原创性之间取得良好的平衡。通过在自然语言处理（NLP）领域的广泛实验，我们证明SciPIP可以检索到与现有顶级会议论文类似的引用，并生成许多与其一致的想法。此外，我们使用大型语言模型评估了SciPIP生成的其他想法的原创性，进一步验证了我们提出方法的有效性。代码和数据库已发布在https://github.com/cheerss/SciPIP。"
    },
    {
        "序号": 83,
        "标题": "Crowdsourcing Lexical Diversity",
        "链接": "http://arxiv.org/abs/2410.23133v1",
        "作者": [
            "Hadi Khalilia",
            "Jahna Otterbacher",
            "Gabor Bella",
            "Rusma Noortyani",
            "Shandy Darma",
            "Fausto Giunchiglia"
        ],
        "摘要": "Lexical-semantic resources (LSRs), such as online lexicons or wordnets, are\nfundamental for natural language processing applications. In many languages,\nhowever, such resources suffer from quality issues: incorrect entries,\nincompleteness, but also, the rarely addressed issue of bias towards the\nEnglish language and Anglo-Saxon culture. Such bias manifests itself in the\nabsence of concepts specific to the language or culture at hand, the presence\nof foreign (Anglo-Saxon) concepts, as well as in the lack of an explicit\nindication of untranslatability, also known as cross-lingual \\emph{lexical\ngaps}, when a term has no equivalent in another language. This paper proposes a\nnovel crowdsourcing methodology for reducing bias in LSRs. Crowd workers\ncompare lexemes from two languages, focusing on domains rich in lexical\ndiversity, such as kinship or food. Our LingoGap crowdsourcing tool facilitates\ncomparisons through microtasks identifying equivalent terms, language-specific\nterms, and lexical gaps across languages. We validated our method by applying\nit to two case studies focused on food-related terminology: (1) English and\nArabic, and (2) Standard Indonesian and Banjarese. These experiments identified\n2,140 lexical gaps in the first case study and 951 in the second. The success\nof these experiments confirmed the usability of our method and tool for future\nlarge-scale lexicon enrichment tasks.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T15:45:09+00:00",
        "概述": "该研究旨在解决自然语言处理中语言资源（如词典和语义网络）存在的偏见问题，特别是对非英语语言和文化不利。研究人员通过众包工具LingoGap，让工作者比较两种语言中的词项，特别是在亲缘关系和食物等词汇丰富的领域。通过微任务的方式，工具帮助识别等同词、语言特定词和词汇空洞。研究在英语与阿拉伯语、印尼语与邦贾雷语之间进行了验证，发现大量词汇空洞，证明了方法的有效性和工具的实用性。",
        "摘要译文": "词汇语义资源（LSRs），例如在线词典或单词网络，是自然语言处理应用的基础。然而，在许多语言中，这类资源存在质量问题：错误的条目、不完整的内容，但很少注意到的问题是偏向英语语言和盎格鲁-撒克逊文化的偏差。这种偏差体现在特定于语言或文化的专有概念缺失、外来（盎格鲁-撒克逊）概念的存在，以及缺乏对不可翻译性（即跨语言的语义缺口）的明示说明，当一个术语在另一种语言中没有对应词时，这种情况也被称为跨语言的\\emph{词汇缺口}。本文提出了一种新颖的众包方法，以减少LSRs中的偏差。众包工作者比较两种语言的词项，重点关注像亲属关系或食品这样的词汇多样性丰富的领域。我们的LingoGap众包工具通过微任务来简化跨语言等同术语、特定语言术语和语义缺口的比较。我们通过两个专注于食品相关术语的研究案例验证了该方法：（1）英语和阿拉伯语，以及（2）印尼标准语和班j土语。这些实验分别发现了2140个语义缺口和951个语义缺口。这些实验的成功证实了我们方法的可实用性，并验证了该工具在未来大规模词典丰富任务中的应用潜力。"
    },
    {
        "序号": 85,
        "标题": "Teaching a Language Model to Distinguish Between Similar Details using a Small Adversarial Training Set",
        "链接": "http://arxiv.org/abs/2410.23118v1",
        "作者": [
            "Chris Achard"
        ],
        "摘要": "Language models can achieve high accuracy on natural language tasks such as\nNLI, but performance suffers on manually created adversarial examples. We\ninvestigate the performance of a language model trained on the Stanford Natural\nLanguage Inference (SNLI) corpus on a manually created adversarial test set. We\nthen improve the model's performance by fine tuning the model on a small,\nmanually created adversarial training set, designed to help the language model\nto learn to differentiate between similar words and phrases in the data. We\nshow an increase in accuracy on the adversarial test set (+ 13%) while still\nmaintaining good performance on the original NLI task. We also show an increase\nin accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI\ntest set (as judged by cosine similarity).",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": null,
        "日期": "2024-10-30T15:27:55+00:00",
        "概述": "该研究旨在提升语言模型在区分相似细节方面的准确性，尤其是在处理手动构造的对抗性样本时。通过使用一个小规模的手动创建的对抗训练集，模型在对抗测试集上的准确性提高了13%，同时仍然保持在原始自然语言推理任务上的良好性能。实验结果还显示，模型在SNLI测试集中的最相似矛盾上的准确率从91.2%提高到92.9%。",
        "摘要译文": "语言模型在诸如自然语言推理（NLI）等自然语言任务上可以达到很高的准确率，但在人工创建的对抗性示例上的性能会下降。我们研究了在斯坦福自然语言推理（SNLI）语料库上训练的语言模型在人工创建的对抗性测试集上的表现。然后，我们通过在一小部分手动创建的对抗性训练集上微调该模型来提高其性能，该训练集旨在帮助语言模型学习区分数据中相似的单词和短语。我们在对抗性测试集上的准确率提高了13%，同时仍然保持在原始NLI任务上的良好性能。我们还展示了SNLI测试集中最相似的矛盾样本（根据余弦相似度判断）的准确率从91.2%提高到92.9%。"
    },
    {
        "序号": 84,
        "标题": "On Memorization of Large Language Models in Logical Reasoning",
        "链接": "http://arxiv.org/abs/2410.23123v1",
        "作者": [
            "Chulin Xie",
            "Yangsibo Huang",
            "Chiyuan Zhang",
            "Da Yu",
            "Xinyun Chen",
            "Bill Yuchen Lin",
            "Bo Li",
            "Badih Ghazi",
            "Ravi Kumar"
        ],
        "摘要": "Large language models (LLMs) achieve good performance on challenging\nreasoning benchmarks, yet could also make basic reasoning mistakes. This\ncontrasting behavior is puzzling when it comes to understanding the mechanisms\nbehind LLMs' reasoning capabilities. One hypothesis is that the increasingly\nhigh and nearly saturated performance on common reasoning benchmarks could be\ndue to the memorization of similar problems. In this paper, we systematically\ninvestigate this hypothesis with a quantitative measurement of memorization in\nreasoning tasks, using a dynamically generated logical reasoning benchmark\nbased on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate\nthe training puzzles (achieving near-perfect accuracy) after fine-tuning, yet\nfail when those puzzles are slightly perturbed, suggesting that the models\nheavily rely on memorization to solve those training puzzles. On the other\nhand, we show that while fine-tuning leads to heavy memorization, it also\nconsistently improves generalization performance. In-depth analyses with\nperturbation tests, cross difficulty-level transferability, probing model\ninternals, and fine-tuning with wrong answers suggest that the LLMs learn to\nreason on K&K puzzles despite training data memorization. This phenomenon\nindicates that LLMs exhibit a complex interplay between memorization and\ngenuine reasoning abilities. Finally, our analysis with per-sample memorization\nscore sheds light on how LLMs switch between reasoning and memorization in\nsolving logical puzzles. Our code and data are available at\nhttps://memkklogic.github.io.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T15:31:54+00:00",
        "概述": "该研究探讨了大型语言模型在逻辑推理任务中是否通过记忆来达到高性能。动机源于大型语言模型在推理基准测试中表现出色但出现基本推理错误的矛盾现象。研究通过动态生成基于骑士和混蛋（K&K）谜题的逻辑推理基准，发现模型在微调后能记住训练谜题并达到近乎完美的准确性，但在稍作改动后则无法解决，表明模型高度依赖记忆。微调虽然导致了记忆，但同时提高了泛化性能。研究结果揭示了大型语言模型在推理和记忆之间的复杂关系。",
        "摘要译文": "大型语言模型（LLMs）在复杂的推理基准任务上取得了良好的性能，但也可能犯基本的推理错误。这种矛盾的行为在理解LLMs推理能力背后的机制时令人困惑。一种假设是，LLMs在常见推理基准任务上的逐渐高且几乎饱和的性能可能是由于对类似问题的记忆。在这篇论文中，我们通过使用基于骑士与无赖（K&K）谜题的动态生成逻辑推理基准，系统地研究了这一假设，并通过定性测量记忆在推理任务中的程度进行了探讨。我们发现，经过微调后，LLMs能够内插训练谜题（几乎达到完美精度），但在这些谜题稍微变化后会出错，这表明模型在解决训练谜题时严重依赖记忆。另一方面，我们展示虽然微调会导致严重记忆，但它也一致地提高了泛化性能。通过扰动测试、跨难度级别的转移性分析、探查模型内部以及使用错误答案进行微调的深入分析，我们表明LLMs即使在训练数据记忆的情况下，也能学会在K&K谜题上进行推理。这一现象表明，LLMs在记忆和真正推理能力之间表现出复杂的相互作用。最后，我们关于每个样本记忆得分的分析揭示了LLMs在解决逻辑谜题时如何在推理和记忆之间切换。我们的代码和数据可在https://memkklogic.github.io 获取。"
    },
    {
        "序号": 87,
        "标题": "Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning",
        "链接": "http://arxiv.org/abs/2410.23099v1",
        "作者": [
            "Dong Shu",
            "Mengnan Du"
        ],
        "摘要": "In-context learning can help Large Language Models (LLMs) to adapt new tasks\nwithout additional training. However, this performance heavily depends on the\nquality of the demonstrations, driving research into effective demonstration\nselection algorithms to optimize this process. These algorithms assist users in\nselecting the best $k$ input-label pairs (demonstration examples) based on a\ngiven test input, enabling LLMs to in-context learn the relationship between\nthe provided examples and the test inputs. Despite all the proposed\ndemonstration selection algorithms, their efficiency and effectiveness remain\nunclear. This lack of clarity make it difficult to apply these algorithms in\nreal-world scenarios and poses challenges for future research aimed at\ndeveloping improved methods. This paper revisits six proposed algorithms,\nevaluating them on five datasets from both efficiency and effectiveness\nperspectives. Our experiments reveal significant variations in algorithm\nperformance across different tasks, with some methods struggling to outperform\nrandom selection in certain scenarios. We also find that increasing the number\nof demonstrations does not always lead to better performance, and that there\nare often trade-offs between accuracy and computational efficiency. Our code is\navailable at https://github.com/Tizzzzy/Demonstration_Selection_Overview.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "补充信息": "6 pages, 4 figures",
        "日期": "2024-10-30T15:11:58+00:00",
        "概述": "本文旨在评估大型语言模型（LLMs）在上下文学习中的示范选择算法，以优化新任务的适应能力。研究六个现有算法在五个数据集上的效率和效果，发现不同任务间算法性能差异显著，部分方法在特定情况下甚至不如随机选择。研究还表明，增加示范数量并不总能提升性能，且准确性与计算效率之间存在权衡。",
        "摘要译文": "上下文学习可以帮助大规模语言模型（LLMs）在无需额外训练的情况下适应新任务。然而，这种性能在很大程度上依赖于演示的质量，驱使研究人员开发有效的演示选择算法以优化这一过程。这些算法帮助用户根据给定的测试输入选择最佳的$k$个输入-标签对（演示示例），使LLMs能够在上下文中学习所给示例与测试输入之间的关系。尽管已经提出了许多演示选择算法，但它们的效率和有效性仍不清楚。这种不确定性使得这些算法难以在实际场景中应用，并为未来旨在开发改进方法的研究带来了挑战。本文重新审视了六种提出的算法，并从效率和有效性两个方面在五个数据集上进行了评估。我们的实验揭示了这些算法在不同任务上的表现存在显著差异，某些方法在某些情况下甚至无法超越随机选择。我们还发现，增加演示示例的数量并不总是能提高性能，而且准确性与计算效率之间常常存在权衡。我们的代码可在https://github.com/Tizzzzy/Demonstration_Selection_Overview找到。"
    },
    {
        "序号": 86,
        "标题": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models",
        "链接": "http://arxiv.org/abs/2410.23114v1",
        "作者": [
            "Junjie Wu",
            "Tsz Ting Chung",
            "Kai Chen",
            "Dit-Yan Yeung"
        ],
        "摘要": "Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE.",
        "分类": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": "18 pages, 8 figures",
        "日期": "2024-10-30T15:25:06+00:00",
        "概述": "本文旨在评估大型视觉语言模型（LVLMs）在视觉语言推理中的幻觉问题，特别是关系幻觉。现有研究表明，LVLMs在处理对象级幻觉方面已有一定研究，但关于对象间关系的幻觉问题仍被忽视。为解决这一问题，论文提出了一种统一框架，通过评估（对象，关系，对象）三元组来同时测量对象和关系幻觉，从而适用于不同视觉语言任务。基于此框架，引入了Tri-HE数据集，全面评估不同LVLMs的关系幻觉问题，并提出一种简单的训练免费方法来缓解幻觉问题，实验结果表明该方法在Tri-HE上优于现有开源模型。",
        "摘要译文": "尽管大型视觉语言模型（LVLMs）在视觉语言推理方面表现卓越，但它们生成的幻觉内容可能并不存在于给定的图像中。目前大多数现有的LVLM幻觉基准主要集中在评估对象相关的幻觉。然而，两种对象之间的关系幻觉，即关系幻觉，尚未受到充分研究。为解决这一问题，本文设计了一个统一框架，同时测量LVLM中的对象和关系幻觉。我们的框架核心思想是对LVLM响应中提取的（对象，关系，对象）三元组进行幻觉评估，从而使框架能够容易地应用于不同的视觉语言任务。基于我们的框架，我们进一步引入了Tri-HE，这是一个新颖的三元组级幻觉评估基准，可用于同时研究对象和关系幻觉。我们在Tri-HE上进行了全面评估，并观察到，现有的LVLM中关系幻觉问题比对象幻觉问题更为严重，突显出一个之前被忽视的可靠LVLM问题。此外，根据我们的发现，我们设计了一个简单有效的无训练方法，以减轻LVLM中的幻觉，使用该方法，我们在Tri-HE上超过了所有开源的竞争对手，并且性能与强大的GPT-4V相当。我们的数据集和用于重复我们实验的代码已公开发布在https://github.com/wujunjie1998/Tri-HE。"
    },
    {
        "序号": 88,
        "标题": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
        "链接": "http://arxiv.org/abs/2410.23090v1",
        "作者": [
            "Yiruo Cheng",
            "Kelong Mao",
            "Ziliang Zhao",
            "Guanting Dong",
            "Hongjin Qian",
            "Yongkang Wu",
            "Tetsuya Sakai",
            "Ji-Rong Wen",
            "Zhicheng Dou"
        ],
        "摘要": "Retrieval-Augmented Generation (RAG) has become a powerful paradigm for\nenhancing large language models (LLMs) through external knowledge retrieval.\nDespite its widespread attention, existing academic research predominantly\nfocuses on single-turn RAG, leaving a significant gap in addressing the\ncomplexities of multi-turn conversations found in real-world applications. To\nbridge this gap, we introduce CORAL, a large-scale benchmark designed to assess\nRAG systems in realistic multi-turn conversational settings. CORAL includes\ndiverse information-seeking conversations automatically derived from Wikipedia\nand tackles key challenges such as open-domain coverage, knowledge intensity,\nfree-form responses, and topic shifts. It supports three core tasks of\nconversational RAG: passage retrieval, response generation, and citation\nlabeling. We propose a unified framework to standardize various conversational\nRAG methods and conduct a comprehensive evaluation of these methods on CORAL,\ndemonstrating substantial opportunities for improving existing approaches.",
        "分类": [
            "cs.IR",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T15:06:32+00:00",
        "概述": "CORAL 是一个大型基准测试集，旨在评估多轮对话检索增强生成（RAG）系统在真实世界多轮对话中的表现。它填补了现有研究主要集中在单轮 RAG 而忽视多轮对话复杂性的空白。CORAL 包含从维基百科自动提取的多样化信息查询对话，并解决了开放域覆盖、知识密集度、自由形式响应和话题转换等关键问题。研究提出了统一框架来标准化各种对话 RAG 方法，并对 CORAL 进行了全面评估，展示了现有方法改进的巨大潜力。",
        "摘要译文": "检索增强生成（RAG）已成为通过外部知识检索增强大型语言模型（LLMs）的一种强大范式。尽管它受到了广泛关注，但现有的学术研究主要集中在单一回合的RAG上，从而在处理真实世界应用中发现的复杂多回合对话方面留下了显著的空白。为了填补这一空白，我们介绍了CORAL，这是一个大规模基准，旨在在现实世界的多回合对话环境中评估RAG系统。CORAL 包含从Wikipedia自动衍生出的多样信息查询对话，并解决了一些关键挑战，如跨域覆盖、知识密集度、自由形式的回应以及话题转换。它支持对话RAG的三大核心任务：段落检索、回应生成和引用标注。我们提出了一种统一框架，以标准化各种对话RAG方法，并在CORAL上对这些方法进行了全面评估，展示了改进现有方法的巨大潜力。"
    },
    {
        "序号": 89,
        "标题": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference",
        "链接": "http://arxiv.org/abs/2410.23079v1",
        "作者": [
            "Junqi Zhao",
            "Zhijin Fang",
            "Shu Li",
            "Shaohui Yang",
            "Shichao He"
        ],
        "摘要": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": null,
        "日期": "2024-10-30T14:53:37+00:00",
        "概述": "本文提出了BUZZ算法，旨在解决大型语言模型（LLM）在推理速度和计算效率上的问题，通过减少缓存记忆使用量并提高推理速度来支持实时部署。BUZZ采用蜂巢结构的稀疏缓存，并结合滑动窗口和历史分段策略，动态关注重要信息，从而在保持高准确率的同时大幅减少缓存占用。实验结果显示，BUZZ在多项实际数据集上取得了显著效果，相比现有方法，在保证99%以上准确率的前提下将缓存占用减少2.5倍，且在多文档问答任务中提高了7.69%的性能，同时兼具log(n)的时间复杂性。",
        "摘要译文": "大型语言模型（LLMs）在自然语言处理中至关重要，但往往在推理速度和计算效率方面存在问题，限制了其实时部署。键值（KV）缓存机制在变压器模型中减少了计算开销，但保持上下文理解方面仍面临挑战。在本文中，我们提出了一种名为BUZZ的新型KV缓存算法，该算法利用结构化的上下文信息，同时减少缓存内存使用，提高推理速度。BUZZ采用蜂巢结构的稀疏缓存，并结合滑动窗口捕获最近的信息，动态将历史标记分割成块，优先处理局部区域中的重要标记。我们在四个现实世界的数据集上评估了BUZZ：CNN/Daily Mail、XSUM、Wikitext 和 10-QA。我们的结果显示，与LLM推理相比，BUZZ（1）节省了2.5倍的缓存内存使用，同时在长文本摘要中保持超过99%的准确性；（2）在相同内存限制下，多文档问答任务超越最先进的性能达到7.69%的优势，而全缓存方法则遇到内存不足的问题。此外，BUZZ实现了显著的推理速度提升，具有$\\log{n}$的时间复杂度。代码可在https://github.com/JunqiZhao888/buzz-llm 上获取。"
    },
    {
        "序号": 90,
        "标题": "Multi-Programming Language Sandbox for LLMs",
        "链接": "http://arxiv.org/abs/2410.23074v1",
        "作者": [
            "Shihan Dou",
            "Jiazheng Zhang",
            "Jianxiang Zang",
            "Yunbo Tao",
            "Haoxiang Jia",
            "Shichun Liu",
            "Yuming Yang",
            "Shenxi Wu",
            "Shaoqing Zhang",
            "Muling Wu",
            "Changze Lv",
            "Limao Xiong",
            "Wenyu Zhan",
            "Lin Zhang",
            "Rongxiang Weng",
            "Jingang Wang",
            "Xunliang Cai",
            "Yueming Wu",
            "Ming Wen",
            "Rui Zheng",
            "Tao Ji",
            "Yixin Cao",
            "Tao Gui",
            "Xipeng Qiu",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "摘要": "We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox\ndesigned to provide unified and comprehensive feedback from compiler and\nanalysis tools for Large Language Models (LLMs). It can automatically identify\nthe programming language of the code, compiling and executing it within an\nisolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox\nalso integrates both traditional and LLM-based code analysis tools, providing a\ncomprehensive analysis of generated code. MPLSandbox can be effortlessly\nintegrated into the training and deployment of LLMs to improve the quality and\ncorrectness of their generated code. It also helps researchers streamline their\nworkflows for various LLM-based code-related tasks, reducing the development\ncost. To validate the effectiveness of MPLSandbox, we integrate it into\ntraining and deployment approaches, and also employ it to optimize workflows\nfor a wide range of real-world code-related tasks. Our goal is to enhance\nresearcher productivity on LLM-based code-related tasks by simplifying and\nautomating workflows through delegation to MPLSandbox.",
        "分类": [
            "cs.SE",
            "cs.CL"
        ],
        "补充信息": "25 pages, 14 figures",
        "日期": "2024-10-30T14:46:43+00:00",
        "概述": "MPLSandbox 是一种多编程语言沙箱，旨在为大型语言模型（LLMs）提供统一且全面的反馈。它能自动识别代码的编程语言，并在隔离的子沙箱中编译和运行代码，确保安全性和稳定性。此外，MPLSandbox 结合了传统和基于 LLM 的代码分析工具，提供全面的代码分析。该工具可轻松集成到 LLM 的训练和部署中，提高生成代码的质量和正确性，帮助研究人员简化和自动化各种 LLM 编码任务的工作流程，降低成本并提升生产力。",
        "摘要译文": "我们引入了MPLSandbox，这是一种开箱即用的多编程语言沙箱，旨在为大型语言模型（LLM）提供统一和全面的编译器和分析工具反馈。它可以自动识别代码的编程语言，并在隔离的子沙箱中编译和执行代码，以确保安全性和稳定性。此外，MPLSandbox还整合了传统和基于LLM的代码分析工具，提供对生成代码的全面分析。MPLSandbox可以轻松集成到LLM的训练和部署过程中，从而提高其生成代码的质量和正确性。它还有助于研究人员简化各种基于LLM的代码相关任务的工作流程，降低开发成本。为了验证MPLSandbox的有效性，我们将其集成到训练和部署方法中，并将其应用于优化广泛的实际代码相关任务的工作流程。我们的目标是通过将工作流程委派给MPLSandbox来简化和自动化基于LLM的代码相关任务的研究人员工作效率。"
    },
    {
        "序号": 92,
        "标题": "Controlling Language and Diffusion Models by Transporting Activations",
        "链接": "http://arxiv.org/abs/2410.23054v1",
        "作者": [
            "Pau Rodriguez",
            "Arno Blaas",
            "Michal Klein",
            "Luca Zappella",
            "Nicholas Apostoloff",
            "Marco Cuturi",
            "Xavier Suau"
        ],
        "摘要": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "68T07, 49Q22",
            "I.2.6; I.2.7; I.4.8"
        ],
        "补充信息": null,
        "日期": "2024-10-30T14:21:33+00:00",
        "概述": "这篇文章提出了一种名为Activation Transport（AcT）的新框架，通过最优传输理论引导模型激活，实现对大型生成模型（如语言和扩散模型）生成内容的精细控制。该方法不需要特定模态知识，具有较低的计算成本，且不影响模型的生成能力。实验结果显示，对于大型语言模型（LLM），AcT能够有效减轻毒性内容、引入任意概念并提升真实性；对于文本到图像生成模型（T2I），AcT能够实现细粒度的风格控制和概念反向。",
        "摘要译文": "随着大型生成模型的能力不断增强及其日益广泛的部署，人们对这些模型的可靠性、安全性和潜在滥用的关注也日益增加。为应对这些挑战，近期的研究提出通过引导模型激活来控制模型生成，以有效地诱导或防止生成输出中概念或行为的出现。本文介绍了激活传输（AcT），这是一种基于最优传输理论引导的泛化框架，可广泛应用于多种激活引导工作。AcT 不依赖于模态类型，能够以极小的计算开销提供对模型行为的精细控制，同时对模型能力的影响也最小。我们通过在大型语言模型（LLMs）和文本到图像扩散模型（T2Is）中解决关键挑战，实验展示了我们方法的有效性和 versatility。对于 LLMs，我们展示了 AcT 可以有效缓解毒性、诱导任意概念以及提高其真实性。在 T2Is 中，我们展示了 AcT 如何实现精细的风格控制和概念否定。"
    },
    {
        "序号": 91,
        "标题": "Don't Just Pay Attention, PLANT It: Transfer L2R Models to Fine-tune Attention in Extreme Multi-Label Text Classification",
        "链接": "http://arxiv.org/abs/2410.23066v1",
        "作者": [
            "Debjyoti Saharoy",
            "Javed A. Aslam",
            "Virgil Pavlu"
        ],
        "摘要": "State-of-the-art Extreme Multi-Label Text Classification (XMTC) models rely\nheavily on multi-label attention layers to focus on key tokens in input text,\nbut obtaining optimal attention weights is challenging and resource-intensive.\nTo address this, we introduce PLANT -- Pretrained and Leveraged AtteNTion -- a\nnovel transfer learning strategy for fine-tuning XMTC decoders. PLANT surpasses\nexisting state-of-the-art methods across all metrics on mimicfull, mimicfifty,\nmimicfour, eurlex, and wikiten datasets. It particularly excels in few-shot\nscenarios, outperforming previous models specifically designed for few-shot\nscenarios by over 50 percentage points in F1 scores on mimicrare and by over 36\npercentage points on mimicfew, demonstrating its superior capability in\nhandling rare codes. PLANT also shows remarkable data efficiency in few-shot\nscenarios, achieving precision comparable to traditional models with\nsignificantly less data. These results are achieved through key technical\ninnovations: leveraging a pretrained Learning-to-Rank model as the planted\nattention layer, integrating mutual-information gain to enhance attention,\nintroducing an inattention mechanism, and implementing a stateful-decoder to\nmaintain context. Comprehensive ablation studies validate the importance of\nthese contributions in realizing the performance gains.",
        "分类": [
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-30T14:41:23+00:00",
        "概述": "本文针对极端多标签文本分类（XMTC）中注意力机制难以优化的问题，提出了一种名为PLANT的新颖迁移学习策略，通过利用预训练的Learning-to-Rank模型作为植入的注意力层，并结合互信息增益、引入注意力缺失机制和使用状态解码器等方式，极大地提升了模型在少量数据场景下的性能，尤其在罕见标签分类上的F1分值提升超过50个百分点，展示了其在处理稀有标签方面的优越能力。",
        "摘要译文": "最先进的极端多标签文本分类（XMTC）模型大量依赖于多标签注意力层来聚焦输入文本中的关键token，但获得最优注意力权重是一项具有挑战性和资源密集的任务。为了解决这一问题，我们引入了PLANT——预训练和利用注意力（Pretrained and Leveraged AtteNTion）——这一新颖的迁移学习策略来进行XMTC解码器的微调。PLANT在mimicfull、mimicfifty、mimicfour、eurlex和wikiten数据集的所有指标上都超越了现有最先进的方法。它在少量样本场景中尤为突出，在mimicrare上的F1分数上比专门为少量样本场景设计的模型高出超过50个百分点，在mimicfew上则高出超过36个百分点，展示了其在处理稀有代码方面的卓越能力。PLANT还在少量样本场景中表现出显著的数据效率，在少量数据的情况下实现了与传统模型相当的精度。这些结果是通过关键技术革新实现的：利用一个预训练的排序学习模型作为植入的注意力层，集成互信息增益以增强注意力，引入不注意机制，并实现一种状态解码器以保持上下文。全面的消融研究验证了这些贡献对于实现性能提升的重要性。"
    },
    {
        "序号": 94,
        "标题": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall",
        "链接": "http://arxiv.org/abs/2410.23000v2",
        "作者": [
            "Zehan Qi",
            "Rongwu Xu",
            "Zhijiang Guo",
            "Cunxiang Wang",
            "Hao Zhang",
            "Wei Xu"
        ],
        "摘要": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "Accepted to EMNLP'24 (Findings). Camera-ready version",
        "日期": "2024-10-31T03:04:28+00:00",
        "概述": "该研究旨在评估长时间上下文检索与生成（RAG）的能力，通过Long$^2$RAG基准和关键点召回（KPR）指标来解决现有评估方法的不足，包括缺乏有效处理长上下文检索的数据集及评估长响应生成利用检索信息的能力。Long$^2$RAG包含280个跨10个领域的问题和8个类别，每个问题关联5份平均2444词的文档。KPR衡量LLM在生成回答时对文档关键点的利用程度，提供更精细的评估。",
        "摘要译文": "检索增强生成（RAG）是一种有前途的方法，可以解决大型语言模型（LLMs）中固定知识的限制。然而，当前用于评估RAG系统的基准测试存在两个关键缺陷：（1）由于缺乏反映检索文档特性的数据集，它们未能充分衡量LLMs处理长上下文检索的能力；（2）它们缺乏一种全面的方法来评估LLMs生成长格式响应的能力，这些响应能有效利用检索到的信息。为了解决这些不足，我们引入了Long$^2$RAG基准测试和关键点召回（KPR）指标。Long$^2$RAG包含涵盖10个领域、8个问题类别共计280个问题，每个问题类别都与5篇平均长度为2,444词的检索文档相关联。KPR评估LLMs在其生成的响应中整合从检索文档中提取的关键点的程度，从而提供对其利用检索信息能力的更细致的评估。"
    },
    {
        "序号": 93,
        "标题": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback",
        "链接": "http://arxiv.org/abs/2410.23022v1",
        "作者": [
            "Qinqing Zheng",
            "Mikael Henaff",
            "Amy Zhang",
            "Aditya Grover",
            "Brandon Amos"
        ],
        "摘要": "Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon).",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.RO"
        ],
        "补充信息": null,
        "日期": "2024-10-30T13:52:43+00:00",
        "概述": "本文提出了一种新的在线方法ONI，通过利用大语言模型（LLMs）的反馈，同时学习强化学习（RL）策略和内在奖励函数。该方法通过异步LLM服务器对代理收集的经验进行标注，并提炼成内在奖励模型。ONI解决了现有方法在大规模环境样本、奖励函数表达、数据集多样性等方面的局限性，实现了对NetHack Learning Environment等挑战性稀疏奖励任务的优秀性能，仅依赖于代理收集的经验，无需额外数据集和源代码。",
        "摘要译文": "从自然语言描述自动生成密集奖励是强化学习（RL）中一个有前途的范式，适用于稀疏奖励问题、开放探索和层次技能设计。近期的一些工作通过利用大规模语言模型（LLMs）的先验知识取得了令人鼓舞的进展。然而，这些方法仍然存在重要局限性：它们要么无法扩展到需要数十亿环境样本的问题；要么仅限于可以通过紧凑代码表达的奖励函数，这可能需要源代码并且难以捕捉到细微的语义；要么需要多样化的离线数据集，但这些数据集可能不存在或难以收集。在这项工作中，我们通过算法和系统层面的贡献来解决这些局限性。我们提出了ONI，一种分布式架构，该架构同时学习一个RL策略和一个内在奖励函数，利用LLM反馈进行学习。我们的方法通过异步LLM服务器标注代理收集的经验，然后将其蒸馏为一个内在奖励模型。我们探索了不同类型奖励建模算法的选择，包括哈希、分类和排序模型，并通过研究它们的相对权衡，照亮了内在奖励设计中的问题。我们的方法仅使用代理收集的经验，在简单的统一过程中实现了NetHack Learning Environment中一系列具有挑战性的、稀疏奖励任务的最先进性能，无需使用外部数据集或源代码。我们将在网址\\url{URL}（即将上线）处开源我们的代码。"
    },
    {
        "序号": 96,
        "标题": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based Encoder For Legal Violation Detection and Resolution",
        "链接": "http://arxiv.org/abs/2410.22977v1",
        "作者": [
            "Shikha Bordia"
        ],
        "摘要": "In this work, we present two systems -- Named Entity Resolution (NER) and\nNatural Language Inference (NLI) -- for detecting legal violations within\nunstructured textual data and for associating these violations with potentially\naffected individuals, respectively. Both these systems are lightweight DeBERTa\nbased encoders that outperform the LLM baselines. The proposed NER system\nachieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which\nfocuses on identifying violations. The proposed NLI system achieved an F1 score\nof 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving\nthese violations by matching them with pre-existing legal complaints of class\naction cases. Our NER system ranked sixth and NLI system ranked fifth on the\nLegalLens leaderboard. We release the trained models and inference scripts.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T12:42:38+00:00",
        "概述": "本文介绍了两个系统——命名实体识别（NER）和自然语言推理（NLI），用于检测未结构化文本中的法律违规行为，并将这些违规行为与潜在受影响个体关联。两个系统均采用轻量级DeBERTa编码器，性能优于LLM基线。NER系统在LegalLens挑战赛子任务A中取得60.01%的F1分数，NLI系统在子任务B中取得84.73%的F1分数，分别排名第6和第5。本文发布了训练模型和推理脚本。",
        "摘要译文": "在本文中，我们提出了两个系统——命名实体消解（NER）和自然语言推理（NLI），用于检测未结构化文本数据中的法律违规行为，并将这些违规行为与可能受到影响的个人关联起来。这两个系统都是基于DeBERTa的轻量级编码器，其性能优于LLM基线。提出的NER系统在LegalLens挑战赛的子任务A中（专注于识别违规行为）获得了60.01%的F1分数。提出的NLI系统在LegalLens挑战赛的子任务B中（专注于通过将违规行为与现有的集体诉讼案件的法律投诉匹配来解决这些违规行为）获得了84.73%的F1分数。我们的NER系统在LegalLens排行榜上位列第六，NLI系统位列第五。我们发布了训练模型和推理脚本。"
    },
    {
        "序号": 95,
        "标题": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning",
        "链接": "http://arxiv.org/abs/2410.22995v1",
        "作者": [
            "Jingkun Ma",
            "Runzhe Zhan",
            "Derek F. Wong",
            "Yang Li",
            "Di Sun",
            "Hou Pong Chan",
            "Lidia S. Chao"
        ],
        "摘要": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process.",
        "分类": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": "58 pages, 28 figures",
        "日期": "2024-10-30T13:19:44+00:00",
        "概述": "这篇论文提出了VisAidMath基准测试，旨在评估大型语言模型和多模态模型在处理视觉辅助数学问题解决（MPS）上的能力。研究针对现有模型在处理视觉信息方面的不足，通过严谨的数据收集和标注过程，收集了1200个具有挑战性的数学问题，并对包括GPT-4V在内的10种主流模型进行了评估，结果显示这些模型在视觉辅助推理任务上的准确率普遍较低，主要原因是模型在隐含的视觉推理过程中存在幻觉现象。",
        "摘要译文": "尽管以往对大规模语言模型（LLMs）和大规模多模态模型（LMMs）的研究系统地探索了视觉情境下的数学问题解决（MPS），但在问题解决过程中这些模型处理视觉信息的方式分析仍不够充分。为解决这一问题，我们提出了VisAidMath，这是一个用于评估与视觉信息相关的MPS过程的基准。我们采用了一套严格的数据整理管道，结合了自动化处理和手动注释，以确保数据质量与可靠性。因此，该基准包括了来自不同数学分支、视觉辅助表达形式和不同难度级别的1,200个具有挑战性的数学问题，这些问题来源于多种不同的来源，如教材、考试试卷和奥林匹克竞赛题目。基于提出的基准，我们对该十种主流LLM和LMM进行了全面评估，突显了视觉辅助推理过程中的不足之处。例如，GPT-4V在视觉辅助推理任务中的准确率仅为45.33%，即使提供了正确的视觉辅助信息，准确率也仅提升2个百分点。深入分析表明，这些不足的主要原因在于对隐含的视觉推理过程的幻觉，这为未来视觉辅助MPS过程的研究指明了方向。"
    },
    {
        "序号": 97,
        "标题": "Private Synthetic Text Generation with Diffusion Models",
        "链接": "http://arxiv.org/abs/2410.22971v1",
        "作者": [
            "Sebastian Ochs",
            "Ivan Habernal"
        ],
        "摘要": "How capable are diffusion models of generating synthetics texts? Recent\nresearch shows their strengths, with performance reaching that of\nauto-regressive LLMs. But are they also good in generating synthetic data if\nthe training was under differential privacy? Here the evidence is missing, yet\nthe promises from private image generation look strong. In this paper we\naddress this open question by extensive experiments. At the same time, we\ncritically assess (and reimplement) previous works on synthetic private text\ngeneration with LLMs and reveal some unmet assumptions that might have led to\nviolating the differential privacy guarantees. Our results partly contradict\nprevious non-private findings and show that fully open-source LLMs outperform\ndiffusion models in the privacy regime. Our complete source codes, datasets,\nand experimental setup is publicly available to foster future research.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T12:38:49+00:00",
        "概述": "该研究探讨了在差分隐私条件下，扩散模型生成合成文本的能力。动机是验证扩散模型是否能在确保隐私的前提下生成高质量的合成数据，这与之前使用LLMs的研究存在争议。通过广泛实验，研究发现开源的LLMs在隐私保护环境下优于扩散模型。此外，研究还批评并重实施了之前的工作，揭示了一些未满足的假设。研究结果为未来该领域的研究提供了代码、数据集和实验设置。",
        "摘要译文": "扩散模型在生成合成文本方面有多强？近期研究显示，它们的表现已达到与自回归大语言模型相当的水平。但如果训练过程中采用了差异隐私，它们生成合成数据的能力如何？目前证据不足，但私有图像生成的前景看起来很强。在本文中，我们通过广泛的实验证明了这一开放性问题。同时，我们批判性地评估（并重新实现）了之前关于使用大语言模型生成私有合成文本的研究，并揭示了一些未被满足的假设，这些假设可能导致了大量的隐私担保被违反。我们的结果部分否定了之前的非隐私研究发现，并表明在隐私环境中，完全开源的大语言模型优于扩散模型。我们的全部源代码、数据集和实验设置均已公开，以促进未来的研究。"
    },
    {
        "序号": 98,
        "标题": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
        "链接": "http://arxiv.org/abs/2410.22944v1",
        "作者": [
            "Tom A. Lamb",
            "Adam Davies",
            "Alasdair Paren",
            "Philip H. S. Torr",
            "Francesco Pinto"
        ],
        "摘要": "Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": "28pages, 14 figures",
        "日期": "2024-10-30T12:01:48+00:00",
        "概述": "本文旨在解决大语言模型（LLMs）在新环境中部署时表现出的非期望行为，如利用偏见特征。为此，作者提出了Focus Instruction Tuning（FIT）方法，通过在特定特征上聚焦而在其他特征上忽略，引导模型根据不同特征输出不同行为。实验结果显示，FIT可以在推理时动态调整模型行为，例如，通过关注任务因果特征而非噪声特征提高鲁棒性，忽略人口统计类别减轻社会偏见，并在新环境下泛化，促进更稳健、公平和可控的LLM应用。",
        "摘要译文": "尽管指令调优（IT）在训练大规模语言模型（LLMs）以执行任意用户指定的任务方面取得了成功，但在新的上下文中部署这些模型时，它们仍然会利用其训练数据中学到的虚假或有偏的特征，导致不期望的行为。在本文中，我们引入了焦点指令调优（FIT），该方法训练LLMs在响应时专注于特定特征而忽略其他特征，从而在什么特征被指定的情况下产生不同的行为。在多个实验设置中，我们展示了焦点调优模型可以在推理时根据不同的特征进行可适应的引导：例如，通过专注于任务因果特征并忽略虚假特征可以提高鲁棒性，通过忽略人口统计类别可以缓解社会偏见。此外，FIT可以在新的上下文中引导行为，在分布转移下进行泛化，并在推理时对未见过的新特征进行引导，从而在真实环境中促进更鲁棒、更公平和更可控的大规模语言模型应用。"
    },
    {
        "序号": 100,
        "标题": "Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration",
        "链接": "http://arxiv.org/abs/2410.22916v1",
        "作者": [
            "Yanchu Guan",
            "Dong Wang",
            "Yan Wang",
            "Haiqing Wang",
            "Renen Sun",
            "Chenyi Zhuang",
            "Jinjie Gu",
            "Zhixuan Chu"
        ],
        "摘要": "Autonomous mobile app interaction has become increasingly important with\ngrowing complexity of mobile applications. Developing intelligent agents that\ncan effectively navigate and interact with mobile apps remains a significant\nchallenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent\n(EBC-LLMAgent), a novel approach that combines large language models (LLMs)\nwith behavior cloning by learning demonstrations to create intelligent and\nexplainable agents for autonomous mobile app interaction. EBC-LLMAgent consists\nof three core modules: Demonstration Encoding, Code Generation, and UI Mapping,\nwhich work synergistically to capture user demonstrations, generate executable\ncodes, and establish accurate correspondence between code and UI elements. We\nintroduce the Behavior Cloning Chain Fusion technique to enhance the\ngeneralization capabilities of the agent. Extensive experiments on five popular\nmobile applications from diverse domains demonstrate the superior performance\nof EBC-LLMAgent, achieving high success rates in task completion, efficient\ngeneralization to unseen scenarios, and the generation of meaningful\nexplanations.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "20 pages",
        "日期": "2024-10-30T11:14:33+00:00",
        "概述": "本文提出了一种名为EBC-LLMAgent的新颖方法，结合了大型语言模型和行为克隆技术，通过演示学习来创建可自主导航和交互的智能移动应用代理。该方法包括三个核心模块：演示编码、代码生成和UI映射，能够捕获用户演示、生成可执行代码，并准确对应代码和UI元素。通过引入行为克隆链融合技术，EBC-LLMAgent在五个不同领域的移动应用中表现优秀，实现了高效的任务完成和对未见场景的良好泛化，并能生成有意义的解释。",
        "摘要译文": "随着移动应用程序复杂性的增加，自主移动应用互动变得越来越重要。开发能够有效导航和与移动应用互动的智能代理仍然是一个重大挑战。在本文中，我们提出了一种名为可解释行为克隆大语言模型代理（EBC-LLMAgent）的新方法，该方法结合了大语言模型（LLMs）和行为克隆，通过学习示范来创建用于自主移动应用互动的智能且可解释的代理。EBC-LLMAgent 包含三个核心模块：示范编码、代码生成和UI映射，这些模块协同工作以捕获用户示范、生成可执行代码并建立代码与UI元素之间的准确对应关系。我们引入了行为克隆链融合技术以增强代理的泛化能力。在五个不同领域的流行移动应用上的广泛实验表明，EBC-LLMAgent 的性能更优越，实现了高任务完成成功率、对未见过的场景的有效泛化以及生成有意义的解释。"
    },
    {
        "序号": 101,
        "标题": "From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes",
        "链接": "http://arxiv.org/abs/2410.22906v1",
        "作者": [
            "Zébulon Goriely",
            "Richard Diehl Martinez",
            "Andrew Caines",
            "Lisa Beinborn",
            "Paula Buttery"
        ],
        "摘要": "Language models are typically trained on large corpora of text in their\ndefault orthographic form. However, this is not the only option; representing\ndata as streams of phonemes can offer unique advantages, from deeper insights\ninto phonological language acquisition to improved performance on sound-based\ntasks. The challenge lies in evaluating the impact of phoneme-based training,\nas most benchmarks are also orthographic. To address this, we develop a\npipeline to convert text datasets into a continuous stream of phonemes. We\napply this pipeline to the 100-million-word pre-training dataset from the\nBabyLM challenge, as well as to standard language and grammatical benchmarks,\nenabling us to pre-train and evaluate a model using phonemic input\nrepresentations. Our results show that while phoneme-based training slightly\nreduces performance on traditional language understanding tasks, it offers\nvaluable analytical and practical benefits.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T11:05:01+00:00",
        "概述": "这项研究旨在探索使用连续音素流预训练语言模型的可行性。传统上，语言模型使用文本数据训练，但研究认为音素流表示法在语音语言习得和基于声音的任务上具有优势。研究开发了一种将文本数据转化为连续音素流的管道，并应用于BabyLM挑战的1亿词预训练数据集及标准语言和语法基准，以评估音素训练模型的效果。结果表明，虽然音素训练在传统语言理解任务中表现略逊，但提供了宝贵的分析和实用价值。",
        "摘要译文": "语言模型通常使用大量的文本数据进行训练，这些文本数据保持其默认的拼写形式。然而，这不是唯一的选择；将数据表示为音素流可以提供独特的优势，从更深入地了解语音语言习得，到在基于声音的任务上获得更好的性能。挑战在于评估基于音素的训练的影响，因为大多数基准测试也是基于拼写的。为了解决这个问题，我们开发了一个流水线，将文本数据集转换为连续的音素流。我们将这个流水线应用于BabyLM挑战中的1亿单词预训练数据集，还应用于标准的语言和语法基准测试，从而使我们能够使用音素输入表示来预训练和评估模型。我们的结果显示，尽管基于音素的训练在传统语言理解任务上的性能略有下降，但它提供了宝贵的分析和实用益处。"
    },
    {
        "序号": 99,
        "标题": "Multi-Agent Large Language Models for Conversational Task-Solving",
        "链接": "http://arxiv.org/abs/2410.22932v1",
        "作者": [
            "Jonas Becker"
        ],
        "摘要": "In an era where single large language models have dominated the landscape of\nartificial intelligence for years, multi-agent systems arise as new\nprotagonists in conversational task-solving. While previous studies have\nshowcased their potential in reasoning tasks and creative endeavors, an\nanalysis of their limitations concerning the conversational paradigms and the\nimpact of individual agents is missing. It remains unascertained how\nmulti-agent discussions perform across tasks of varying complexity and how the\nstructure of these conversations influences the process. To fill that gap, this\nwork systematically evaluates multi-agent systems across various discussion\nparadigms, assessing their strengths and weaknesses in both generative tasks\nand question-answering tasks. Alongside the experiments, I propose a taxonomy\nof 20 multi-agent research studies from 2022 to 2024, followed by the\nintroduction of a framework for deploying multi-agent LLMs in conversational\ntask-solving. I demonstrate that while multi-agent systems excel in complex\nreasoning tasks, outperforming a single model by leveraging expert personas,\nthey fail on basic tasks. Concretely, I identify three challenges that arise:\n1) While longer discussions enhance reasoning, agents fail to maintain\nconformity to strict task requirements, which leads to problem drift, making\nshorter conversations more effective for basic tasks. 2) Prolonged discussions\nrisk alignment collapse, raising new safety concerns for these systems. 3) I\nshowcase discussion monopolization through long generations, posing the problem\nof fairness in decision-making for tasks like summarization. This work uncovers\nboth the potential and challenges that arise with multi-agent interaction and\nvarying conversational paradigms, providing insights into how future research\ncould improve the efficiency, performance, and safety of multi-agent LLMs.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T11:38:13+00:00",
        "概述": "本文研究了多智能体大型语言模型在对话任务解决中的表现，旨在填补多智能体对话在复杂性和对话结构影响方面的研究空白。通过系统评估多个智能体模型在生成任务和问答任务中的优缺点，作者发现多智能体系统在复杂推理任务中优于单模型，但在基本任务中表现不佳。研究发现多智能体讨论存在三个主要挑战：较长讨论可能导致任务偏离和安全问题，对话垄断引发公平性问题，以及难以维持严格任务要求。研究提出了20个研究模型的分类，并引入了部署多智能体LLM的框架，为未来研究提供了见解。",
        "摘要译文": "在单一大型语言模型在过去几年主导人工智能领域的情况下，多智能体系统作为对话任务解决中的新主角浮现。虽然之前的研究展示了它们在推理任务和创造性工作中的潜力，但它们在对话范式方面的局限性以及个体智能体的影响分析仍缺位。多智能体讨论在不同复杂性的任务中表现如何，以及这些对话结构如何影响过程仍未明了。为了填补这一空白，本研究系统地评估了多智能体系统在各种讨论范式中的表现，评估它们在生成任务和问答任务中的优势和劣势。除了实验之外，我还提出了2022年至2024年间的20项多智能体研究的分类法，并引入了在对话任务解决中部署多智能体语言模型的框架。我展示，虽然多智能体系统在复杂推理任务中胜过单一模型，通过利用专家角色，但它们在基本任务中表现逊色。具体来说，我确定了三个挑战：1）虽然更长的讨论可以增强推理，但智能体难以保持严格任务要求的一致性，导致问题转移，因此较短的讨论更适合基本任务。2）长时间讨论增加了对齐失败的风险，为这些系统提出了新的安全问题。3）我展示了通过长时间生成导致讨论垄断的现象，提出了任务总结中决策公平性的问题。本研究揭示了多智能体互动及其不同对话范式带来的潜力和挑战，为未来研究如何提升多智能体语言模型的效率、性能和安全性提供了洞察。"
    },
    {
        "序号": 102,
        "标题": "Combining psychoanalysis and computer science: an empirical study of the relationship between emotions and the Lacanian discourses",
        "链接": "http://arxiv.org/abs/2410.22895v1",
        "作者": [
            "Minas Gadalla",
            "Sotiris Nikoletseas",
            "José Roberto de A. Amazonas"
        ],
        "摘要": "This research explores the interdisciplinary interaction between\npsychoanalysis and computer science, suggesting a mutually beneficial exchange.\nIndeed, psychoanalytic concepts can enrich technological applications involving\nunconscious, elusive aspects of the human factor, such as social media and\nother interactive digital platforms. Conversely, computer science, especially\nArtificial Intelligence (AI), can contribute quantitative concepts and methods\nto psychoanalysis, identifying patterns and emotional cues in human expression.\nIn particular, this research aims to apply computer science methods to\nestablish fundamental relationships between emotions and Lacanian discourses.\nSuch relations are discovered in our approach via empirical investigation and\nstatistical analysis, and are eventually validated in a theoretical\n(psychoanalytic) way. It is worth noting that, although emotions have been\nsporadically studied in Lacanian theory, to the best of our knowledge a\nsystematic, detailed investigation of their role is missing. Such fine-grained\nunderstanding of the role of emotions can also make the identification of\nLacanian discourses more effective and easy in practise. In particular, our\nmethods indicate the emotions with highest differentiation power in terms of\ncorresponding discourses; conversely, we identify for each discourse the most\ncharacteristic emotions it admits. As a matter of fact, we develop a method\nwhich we call Lacanian Discourse Discovery (LDD), that simplifies (via\nsystematizing) the identification of Lacanian discourses in texts. Although the\nmain contribution of this paper is inherently theoretical (psychoanalytic), it\ncan also facilitate major practical applications in the realm of interactive\ndigital systems. Indeed, our approach can be automated through Artificial\nIntelligence methods that effectively identify emotions (and corresponding\ndiscourses) in texts.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T10:49:33+00:00",
        "概述": "这篇论文旨在探索psychoanalysis和computer science的跨学科互动，特别是通过计算机科学方法来揭示情感与拉康式话语之间的关系。研究动机在于填补拉康理论中对情感角色研究的系统性和详细性空白。该研究通过实证调查和统计分析发现，某些情感具有最强的分化力，对应特定的拉康式话语；同时，每种话语也有其最特色的反映情感。研究提出了“拉康式话语发现”（LDD）方法，简化了文本中拉康式话语的识别。虽然研究主要具有理论意义，但其方法可以被AI自动化应用，识别文本中的情感和相应话语，从而促进交互数字系统的实际应用。",
        "摘要译文": "这项研究探讨了精神分析与计算机科学的跨学科互动，建议它们之间可以实现互惠互利的交换。的确，精神分析的概念可以丰富涉及人类因素潜意识、难以捉摸方面（如社交媒体和其他交互式数字平台）的技术应用。相反，计算机科学，尤其是人工智能（AI），可以为精神分析提供定量的概念和方法，帮助识别人类表达中的模式和情绪线索。特别地，这项研究旨在应用计算机科学方法来建立情绪与拉康话语之间基本的关系。我们通过实证调查和统计分析发现了这些关系，并最终从理论（精神分析）的角度进行了验证。值得注意的是，尽管情绪偶尔在拉康理论中有所研究，据我们所知，系统而详细的探讨它们的作用仍然缺失。这样的细致理解情绪的作用也可以使识别拉康话语更加有效和易于实践。具体来说，我们的方法表明，在对应的话语中情绪具有最强的区分力；相反，我们为每一话语识别出它最典型的对应情绪。事实上，我们开发了一种称为拉康话语发现（LDD）的方法，通过系统化简化了在文本中识别拉康话语的过程。虽然本文的主要贡献本质上是理论（精神分析）性的，但它也可以在交互数字系统领域促进主要的实际应用。确实，我们的方法可以通过有效识别文本中的情绪（及其对应的话语）的人工智能方法实现自动化。"
    },
    {
        "序号": 103,
        "标题": "VPO: Leveraging the Number of Votes in Preference Optimization",
        "链接": "http://arxiv.org/abs/2410.22891v1",
        "作者": [
            "Jae Hyeon Cho",
            "Minkyung Park",
            "Byung-Jun Lee"
        ],
        "摘要": "Direct Preference Optimization (DPO) trains a language model using human\npreference data, bypassing the explicit reward modeling phase of Reinforcement\nLearning from Human Feedback (RLHF). By iterating over sentence pairs in a\npreference dataset, DPO enhances generation quality by increasing the\nlikelihood of producing preferred sentences over less favored ones. Preference\ndatasets are typically created by selecting preferred sentences through a\nvoting process involving multiple individuals, as opinions can vary due to the\nsubjective nature of human preferences. While the number of votes offers\ninsight into whether a sentence pair is clearly preferable or controversial,\ncurrent methods do not fully leverage this information. In this paper, we\nintroduce a technique that leverages user voting data to better align with\ndiverse subjective preferences. We employ the Bayesian Minimum Mean Square\nError (Bayesian MMSE) estimator to model the probability that one generation is\npreferable to another. Using this estimated probability as a target, we develop\nthe Vote-based Preference Optimization (VPO) framework, which incorporates the\nnumber of votes on both sides to distinguish between controversial and obvious\ngeneration pairs. We show that previous algorithms, such as DPO and Identity\nPreference Optimization (IPO), can be extended using the proposed framework,\ntermed VDPO and VIPO. Our experiments demonstrate that these proposed\nalgorithms outperform various existing methods, including their base\nalgorithms.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T10:39:34+00:00",
        "概述": "该研究旨在改进直接偏好优化（DPO）技术，使其更好地利用用户投票数据。目前的方法并未充分利用投票数量信息，而该研究通过引入基于贝叶斯最小均方误差（Bayesian MMSE）的Vote-based Preference Optimization（VPO）框架，解决了这一问题。该框架使用估计的概率作为目标，区分有争议和明显句子对并优化生成质量。实验结果表明，新的VPO框架在多种场景中均优于现有方法。",
        "摘要译文": "直接偏好优化（DPO）使用人类偏好数据训练语言模型，绕过了强化学习从人类反馈（RLHF）中显式的奖励建模阶段。通过迭代偏好数据集中的句子对，DPO通过增加产生偏好句子而非不太受欢迎的句子的可能性来提高生成质量。偏好数据集通常通过多人投票选出偏好句子的方式创建，因为人类偏好的主观性会导致意见有所不同。虽然投票数量可以提供判断句子对是否明显更优或存在争议的线索，但当前的方法并未充分利用这些信息。在本文中，我们引入了一种技术，利用用户投票数据更好地与多样化的主观偏好对齐。我们采用贝叶斯最小均方误差（Bayesian MMSE）估计器来建模一个生成比另一个生成更优的概率。利用这种估计的概率作为目标，我们开发了基于投票的偏好优化（VPO）框架，该框架结合了双方的投票数量来区分有争议和明显不同的生成对。我们证明了先前的算法，如DPO和身份偏好优化（IPO），可以通过提出的框架扩展，称为VDPO和VIPO。我们的实验显示，这些提出的算法在各种现有方法（包括其基础算法）中表现更优。"
    },
    {
        "序号": 105,
        "标题": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies",
        "链接": "http://arxiv.org/abs/2410.22886v1",
        "作者": [
            "Suchir Salhan",
            "Richard Diehl Martinez",
            "Zébulon Goriely",
            "Paula Buttery"
        ],
        "摘要": "Curriculum Learning has been a popular strategy to improve the cognitive\nplausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.\nHowever, it has not led to considerable improvements over non-curriculum\nmodels. We assess whether theoretical linguistic acquisition theories can be\nused to specify more fine-grained curriculum learning strategies, creating\nage-ordered corpora of Child-Directed Speech for four typologically distant\nlanguage families to implement SSLMs and acquisition-inspired curricula\ncross-lingually. Comparing the success of three objective curricula (Growing,\nInwards and MMM) that precisely replicate the predictions of acquisition\ntheories on a standard SSLM architecture, we find fine-grained\nacquisition-inspired curricula can outperform non-curriculum baselines and\nperformance benefits of curricula strategies in SSLMs can be derived by\nspecifying fine-grained language-specific curricula that precisely replicate\nlanguage acquisition theories.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "BabyLM Shared Task 2024 (Accepted, Poster), co-located in EMNLP 2024",
        "日期": "2024-10-30T10:31:54+00:00",
        "概述": "该研究旨在通过使用认知合理的课程学习策略来改进小型语言模型（SSLMs）的认知合理性。作者基于语言习得理论，为四种不同语系的语言创建了按年龄排序的儿童定向演讲语料库，以实施跨语言的SSLMs和习得启发式课程。研究发现，精准复制语言习得理论预测的细粒度习得启发式课程能够超越非课程基线，证明了以语言特定理论为基础的细致调整课程策略在SSLMs中可以提高性能。",
        "摘要译文": "在BabyLM挑战中， Curriculum Learning 是一个流行的战略，用于提高小型语言模型（SSLMs）的认知合理性。然而，它并没有显著超过非Curriculum模型的效果。我们评估是否可以利用理论语言习得理论来指定更为精细的Curriculum Learning策略，在四个类型学上不同的语言家族中创建按年龄段排序的儿童定向言语语料库，以实施跨语言的SSLMs和基于习得的语言激励性Curriculum。通过在标准SSLM架构上比较三种目标Curriculum（Growing、Inwards和MMM）的成功，我们发现基于精细习得的语言激励性Curriculum可以超越非Curriculum基线，并且SSLMs中Curriculum策略的表现改进可以通过指定精确复制语言习得理论的精细语言特定Curriculum来实现。"
    },
    {
        "序号": 104,
        "标题": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector",
        "链接": "http://arxiv.org/abs/2410.22888v1",
        "作者": [
            "Youcheng Huang",
            "Fengbin Zhu",
            "Jingkun Tang",
            "Pan Zhou",
            "Wenqiang Lei",
            "Jiancheng Lv",
            "Tat-Seng Chua"
        ],
        "摘要": "Visual Language Models (VLMs) are vulnerable to adversarial attacks,\nespecially those from adversarial images, which is however under-explored in\nliterature. To facilitate research on this critical safety problem, we first\nconstruct a new laRge-scale Adervsarial images dataset with Diverse hArmful\nResponses (RADAR), given that existing datasets are either small-scale or only\ncontain limited types of harmful responses. With the new RADAR dataset, we\nfurther develop a novel and effective iN-time Embedding-based AdveRSarial Image\nDEtection (NEARSIDE) method, which exploits a single vector that distilled from\nthe hidden states of VLMs, which we call the attacking direction, to achieve\nthe detection of adversarial images against benign ones in the input. Extensive\nexperiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the\neffectiveness, efficiency, and cross-model transferrability of our proposed\nmethod. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE",
        "分类": [
            "cs.CV",
            "cs.CL",
            "cs.CR"
        ],
        "补充信息": null,
        "日期": "2024-10-30T10:33:10+00:00",
        "概述": "该研究针对视觉语言模型（VLMs）易受对抗攻击的问题，尤其是来自对抗图像的攻击，构建了一个大规模且包含多样化有害反应的新数据集RADAR。基于RADAR，提出了NEARSIDE方法，该方法利用从VLMs隐藏状态中提取的单一向量（攻击方向）来实时检测对抗图像。实验表明NEARSIDE方法在两个模型（LLaVA和MiniGPT-4）上具有有效性、高效性和跨模型可迁移性。",
        "摘要译文": "视觉语言模型（VLMs）容易受到对抗攻击的影响，特别是在来自对抗图像的攻击方面，但这一问题在文献中研究较少。为促进对此关键安全问题的研究，我们首先构建了一个新的大规模对抗图像数据集——具有多样化有害响应的RADAR数据集（Diverse hArmful Responses），因为现有的数据集要么规模较小，要么只包含有限类型的有害响应。借助新的RADAR数据集，我们进一步开发了一种新颖且有效的实时嵌入式对抗图像检测方法（NEARSIDE），该方法利用从视觉语言模型隐藏状态中提炼出的单一向量，即攻击方向，来在输入中检测对抗图像与良性图像。使用两个受害者模型LLaVA和MiniGPT-4的广泛实验表明，我们提出的方法具有有效性、高效性和跨模型可转移性。我们的代码可从 <https://github.com/mob-scu/RADAR-NEARSIDE> 获取。"
    },
    {
        "序号": 106,
        "标题": "Stealing User Prompts from Mixture of Experts",
        "链接": "http://arxiv.org/abs/2410.22884v1",
        "作者": [
            "Itay Yona",
            "Ilia Shumailov",
            "Jamie Hayes",
            "Nicholas Carlini"
        ],
        "摘要": "Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities.",
        "分类": [
            "cs.CR",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "补充信息": null,
        "日期": "2024-10-30T10:25:35+00:00",
        "概述": "这篇论文揭示了Mixture-of-Experts（MoE）模型中存在一种安全漏洞。动机是提高密集语言模型的效率和可扩展性。研究者发现，通过操纵查询批次，攻击者可以完全揭露受害者的输入提示。方法是利用`torch.topk` CUDA 实现的处理方式，成功地在两层Mixtral模型上执行此攻击。结果表明，可以使用$O({VM}^2)$查询（词汇量为$V$，提示长度为$M$）或每个提示词平均100次查询来提取完整提示。这是首次利用架构缺陷窃取用户提示的攻击，揭示了大型语言模型的新漏洞类别。",
        "摘要译文": "混合专家（MoE）模型通过将每个令牌路由到每层中的一小部分专家来提高密集语言模型的效率和可扩展性。在本文中，我们展示了如何通过让攻击者安排其查询与受害者的查询出现在同一个批处理中，利用专家选择路由来完全披露受害者的提示。我们成功地在两层Mixtral模型中演示了这种攻击的有效性，利用了torch.topk CUDA 实现中的并列处理行为。我们的结果表明，在词汇量大小为 \\(V\\) 和提示长度为 \\(M\\) 的情况下，我们可以通过 \\(O({VM}^2)\\) 个查询提取整个提示，或者在我们考虑的设置中平均每令牌需要100个查询。这是首次利用架构缺陷来提取用户提示的攻击，引入了一类新的语言模型漏洞。"
    },
    {
        "序号": 110,
        "标题": "How Well Do Large Language Models Disambiguate Swedish Words?",
        "链接": "http://arxiv.org/abs/2410.22827v1",
        "作者": [
            "Richard Johansson"
        ],
        "摘要": "We evaluate a battery of recent large language models on two benchmarks for\nword sense disambiguation in Swedish. At present, all current models are less\naccurate than the best supervised disambiguators in cases where a training set\nis available, but most models outperform graph-based unsupervised systems.\nDifferent prompting approaches are compared, with a focus on how to express the\nset of possible senses in a given context. The best accuracies are achieved\nwhen human-written definitions of the senses are included in the prompts.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "SLTC 2024 extended abstract",
        "日期": "2024-10-30T09:10:41+00:00",
        "概述": "本文评价了多种大型语言模型在瑞典语词义消歧任务上的表现。研究通过两个瑞典语词义消歧基准测试，探讨了当前大型语言模型与监督式和无监督式系统的性能差异。结果显示，尽管现代模型在有训练集的情况下仍不如最佳监督式系统，但在缺乏训练集时，大多数模型优于基于图的无监督系统。通过对比不同提示方法，研究发现包含人类编写的感觉定义的提示能实现最佳准确性。",
        "摘要译文": "我们评估了近期的大型语言模型在瑞典语词义消歧两个基准上的表现。目前，所有当前模型在有训练集的情况下都不如最好监督消歧系统准确，但大多数模型优于基于图的无监督系统。本文比较了不同的提示方法，重点关注如何在给定上下文中表达可能的词义集合。在提示中包含人工撰写的词义定义时，能达到最佳准确率。"
    },
    {
        "序号": 107,
        "标题": "Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations",
        "链接": "http://arxiv.org/abs/2410.22874v1",
        "作者": [
            "Leonardo Ranaldi",
            "Marco Valentino",
            "Andrè Freitas"
        ],
        "摘要": "Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": null,
        "日期": "2024-10-30T10:11:53+00:00",
        "概述": "这篇论文旨在解决大语言模型在检索增强生成（RAG）机制中难以批判性分析上下文信息的问题，可能会导致错误推断和幻觉。研究通过提出Contrastive-RAG（C-RAG）框架，实现对比解释，该框架包括检索相关文档、选择并举例说明相关段落、生成解释以对比段落的相关性并支持最终答案。实验结果表明，C-RAG能显著提升RAG模型性能，同时需要较少的提示和示例，并对检索到的文档微调具有鲁棒性。",
        "摘要译文": "检索增强生成（RAG）已成为当前自然语言处理（NLP）中一种关键机制，用于支持大规模语言模型（LLMs）系统地访问更丰富的事实背景。然而，RAG机制的集成带来了其固有的挑战，因为LLMs需要处理潜在的噪声上下文。近期研究表明，LLMs仍然难以批判性地分析基于RAG的上下文信息，这一局限可能导致错误的推理和幻觉。在本文中，我们研究了如何通过对比性解释在RAG中引发批判性推理。特别地，我们提出了一种名为对比性RAG（C-RAG）的框架，该框架（i）根据查询检索相关文档，（ii）选择并举例说明相关段落，（iii）生成明确对比段落相关性的解释，以（iv）支持最终答案。我们展示了C-RAG如何通过从LLMs构建对比性推理示范来指导较小的模型进行检索增强任务。广泛的实验表明，C-RAG在（a）显著减少提示和示范数量以及（b）对检索到的文档扰动具有鲁棒性方面改进了最先进的RAG模型。"
    },
    {
        "序号": 109,
        "标题": "Danoliteracy of Generative, Large Language Models",
        "链接": "http://arxiv.org/abs/2410.22839v1",
        "作者": [
            "Søren Vejlgaard Holm",
            "Lars Kai Hansen",
            "Martin Carsten Nielsen"
        ],
        "摘要": "The language technology moonshot moment of Generative, Large Language Models\n(GLLMs) was not limited to English: These models brought a surge of\ntechnological applications, investments and hype to low-resource languages as\nwell. However, the capabilities of these models in languages such as Danish\nwere until recently difficult to verify beyond qualitative demonstrations due\nto a lack of applicable evaluation corpora. We present a GLLM benchmark to\nevaluate Danoliteracy, a measure of Danish language and cultural competency,\nacross eight diverse scenarios such Danish citizenship tests and abstractive\nsocial media question answering. This limited-size benchmark is found to\nproduce a robust ranking that correlates to human feedback at $\\rho \\sim 0.8$\nwith GPT-4 and Claude Opus models achieving the highest rankings. Analyzing\nthese model results across scenarios, we find one strong underlying factor\nexplaining $95\\%$ of scenario performance variance for GLLMs in Danish,\nsuggesting a $g$ factor of model consistency in language adaption.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "I.2.7"
        ],
        "补充信息": "16 pages, 13 figures, submitted to: NoDaLiDa/Baltic-HLT 2025",
        "日期": "2024-10-30T09:18:31+00:00",
        "概述": "这篇论文旨在评估大型生成语言模型（GLLMs）在丹麦语（Danoliteracy）方面的能力，尤其是在缺乏相关评价数据的情况下验证其在低资源语言上的性能。研究通过设计一个包含八种不同场景的基准测试，评估了这些模型在涵盖丹麦公民考试和社会媒体问答等领域的表现。结果显示，GPT-4和Claude Opus模型表现最佳，取得了较高排名，并发现语言适应一致性是模型性能的首要因素。",
        "摘要译文": "生成型大规模语言模型（GLLMs）的语言技术飞跃时刻并不局限于英语：这些模型为低资源语言带来了大量的技术应用、投资和 hype。然而，直到 recently，由于缺乏适用的评价语料库，这些模型在丹麦语等语言上的能力难以通过定量验证。我们提出了一项针对 Danoliteracy 的 GLLM 基准测试，该基准测试评估了丹麦语言和文化能力在八种不同场景中的表现，如丹麦公民资格测试和抽象的社交媒体问答。这项有限规模的基准测试被发现能够产生与人类反馈高度相关（系数约为 0.8）的排名，GPT-4 和 Claude Opus 模型的排名最高。通过对这些模型结果在不同场景中的分析，我们发现一个强大的潜在因素可以解释 GLLMs 在丹麦的表现差异的 95%，这表明模型在语言适应性方面的一致性可以作为其 g 因子。"
    },
    {
        "序号": 108,
        "标题": "Exploiting Phonological Similarities between African Languages to achieve Speech to Speech Translation",
        "链接": "http://arxiv.org/abs/2410.23323v1",
        "作者": [
            "Peter Ochieng",
            "Dennis Kaburu"
        ],
        "摘要": "This paper presents a pilot study on direct speech-to-speech translation\n(S2ST) by leveraging linguistic similarities among selected African languages\nwithin the same phylum, particularly in cases where traditional data annotation\nis expensive or impractical. We propose a segment-based model that maps speech\nsegments both within and across language phyla, effectively eliminating the\nneed for large paired datasets. By utilizing paired segments and guided\ndiffusion, our model enables translation between any two languages in the\ndataset. We evaluate the model on a proprietary dataset from the Kenya\nBroadcasting Corporation (KBC), which includes five languages: Swahili, Luo,\nKikuyu, Nandi, and English. The model demonstrates competitive performance in\nsegment pairing and translation quality, particularly for languages within the\nsame phylum. Our experiments reveal that segment length significantly\ninfluences translation accuracy, with average-length segments yielding the\nhighest pairing quality. Comparative analyses with traditional cascaded ASR-MT\ntechniques show that the proposed model delivers nearly comparable translation\nperformance. This study underscores the potential of exploiting linguistic\nsimilarities within language groups to perform efficient S2ST, especially in\nlow-resource language contexts.",
        "分类": [
            "eess.AS",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T09:44:52+00:00",
        "概述": "该研究旨在利用非洲同系语言之间的音韵相似性进行直接语音到语音翻译（S2ST），特别在数据标注成本高昂时。研究提出了一种基于段落的模型，无需大量配对数据集，通过利用配对段落和引导扩散进行翻译。研究在肯尼亚广播公司（KBC）的私有数据集上评估了该模型，包括五种语言：斯瓦希里语、洛乌语、基库尤语、纳迪语和英语。结果显示，长度适中的段落翻译效果最佳，并且该方法接近传统级联ASR-MT技术的性能。研究证明了利用语言组内的语言相似性进行高效S2ST的潜力，特别是在低资源语言环境中。",
        "摘要译文": "本文介绍了通过利用同一语系中选定非洲语言之间的语言相似性进行直接语音到语音翻译（S2ST）的试点研究，特别是在传统数据标注成本高或不实用的情况下。我们提出了一种基于片段的模型，该模型可以映射同一语系内及不同语系之间的语音片段，有效消除了对大规模配对数据集的需求。通过利用配对片段和引导扩散，我们的模型能够在数据集中的任何两种语言之间进行翻译。我们使用肯尼亚广播公司（KBC）的专有数据集进行了评估，该数据集包括五种语言：斯瓦希里语、卢奥语、基库尤语、纳迪语和英语。该模型在片段配对和翻译质量方面表现出竞争力，尤其是对于同一语系中的语言。我们的实验表明，片段长度显著影响翻译准确性，平均长度的片段生成的配对质量最高。与传统的级联ASR-MT技术相比，提出的模型达到了几乎相当的翻译性能。这项研究强调了在低资源语言环境中利用语言组内的语言相似性进行高效S2ST的潜力。"
    },
    {
        "序号": 115,
        "标题": "Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model",
        "链接": "http://arxiv.org/abs/2410.22736v1",
        "作者": [
            "Keito Sasagawa",
            "Koki Maeda",
            "Issa Sugiura",
            "Shuhei Kurita",
            "Naoaki Okazaki",
            "Daisuke Kawahara"
        ],
        "摘要": "To develop high-performing Visual Language Models (VLMs), it is essential to\nprepare multimodal resources, such as image-text pairs, interleaved data, and\ninstruction data. While multimodal resources for English are abundant, there is\na significant lack of corresponding resources for non-English languages, such\nas Japanese. To address this problem, we take Japanese as a non-English\nlanguage and propose a method for rapidly creating Japanese multimodal datasets\nfrom scratch. We collect Japanese image-text pairs and interleaved data from\nweb archives and generate Japanese instruction data directly from images using\nan existing VLM. Our experimental results show that a VLM trained on these\nnative datasets outperforms those relying on machine-translated content.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": "15 pages, 7 figures",
        "日期": "2024-10-30T06:46:33+00:00",
        "概述": "本文旨在解决非英语语言（如日语）视觉语言模型（VLM）缺乏相应多模态资源的问题。研究团队提出了一种方法，从零开始收集日语文本-图像对和交错数据，并利用现有VLM直接从图像生成日语文本指令数据。实验结果表明，基于这些原生数据集训练的VLM优于使用机器翻译内容训练的模型。",
        "摘要译文": "为了开发高性能的视觉语言模型（VLMs），准备多媒体资源至关重要，例如图像-文本配对、交错数据和指令数据。虽然英语的多媒体资源充足，但非英语语言，如日语，对应的多媒体资源却严重不足。为解决这一问题，我们以日语为例，提出了一种从头快速创建日语多媒体数据集的方法。我们从网络存档中收集日语的图像-文本配对和交错数据，并利用现有的VLM直接从图像中生成日语指令数据。我们的实验结果表明，这些本地数据集训练出的VLM性能优于依赖机器翻译内容的模型。"
    },
    {
        "序号": 112,
        "标题": "MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning",
        "链接": "http://arxiv.org/abs/2410.22782v1",
        "作者": [
            "Xujia Wang",
            "Haiyan Zhao",
            "Shuo Wang",
            "Hanqing Wang",
            "Zhiyuan Liu"
        ],
        "摘要": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly\nimproved the adaptation of LLMs to downstream tasks in a resource-efficient\nmanner. However, in multi-task scenarios, challenges such as training imbalance\nand the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which\ncombines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by\npromoting task-specific learning across experts. Despite this, MoLoRA remains\ninefficient in terms of training speed, parameter utilization, and overall\nmulti-task performance. In this paper, we propose Mixture of Asymmetric\nLow-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages\nasymmetric optimization across LoRA experts. MALoRA reduces the number of\ntrainable parameters by 30% to 48%, increases training speed by 1.2x, and\nmatches the computational efficiency of single-task LoRA models. Additionally,\nMALoRA addresses overfitting issues commonly seen in high-rank configurations,\nenhancing performance stability. Extensive experiments across diverse\nmulti-task learning scenarios demonstrate that MALoRA consistently outperforms\nall baseline methods in both inter-domain and intra-domain tasks.",
        "分类": [
            "cs.CL",
            "cs.LG",
            "I.2.7"
        ],
        "补充信息": "14 pages, 5 figures",
        "日期": "2024-10-30T07:53:52+00:00",
        "概述": "该论文旨在解决多任务学习中的训练不平衡和“跷跷板效应”等问题。它提出了Mixture of Asymmetric Low-Rank Adaptation (MALoRA)方法，通过在LoRA专家中采用非对称优化，减少了可训练参数30%至48%，加速了训练速度1.2倍，并保持了与单任务LoRA模型相同的计算效率。实验结果显示，MALoRA在跨域和同域任务中均优于现有基线方法，提高了性能稳定性。",
        "摘要译文": "Parameter-Efficient Fine-Tuning（PEFT）方法如LoRA极大地提高了大规模语言模型（LLMs）在下游任务中的适应性，同时实现了资源高效。然而，在多任务场景中，训练不平衡和跷跷板效应等挑战经常出现。Mixture-of-LoRA（MoLoRA），将LoRA与稀疏Mixture-of-Experts相结合，通过促进专家间的任务特定学习来缓解部分这些问题。尽管如此，MoLoRA在训练速度、参数利用和整体多任务性能方面仍然不够高效。在本文中，我们提出了一种名为Mixture of Asymmetric Low-Rank Adaptation（MALoRA）的灵活的微调框架，利用LoRA专家之间的非对称优化。MALoRA将可训练参数减少了30%至48%，加快了1.2倍的训练速度，并且在单任务LoRA模型的计算效率方面保持一致。此外，MALoRA解决了高秩配置中常见的过拟合问题，增强了性能的稳定性。广泛的实验表明，在跨域和同域任务中，MALoRA始终优于所有基线方法。"
    },
    {
        "序号": 113,
        "标题": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models",
        "链接": "http://arxiv.org/abs/2410.22770v1",
        "作者": [
            "Hao Li",
            "Xiaogeng Liu",
            "Chaowei Xiao"
        ],
        "摘要": "Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard.",
        "分类": [
            "cs.CL",
            "cs.AI",
            "cs.CR"
        ],
        "补充信息": null,
        "日期": "2024-10-30T07:39:42+00:00",
        "概述": "该研究针对大语言模型（LLMs）面临的关键威胁——提示注入攻击，提出了InjecGuard模型。研究发现，现有的提示守卫模型存在过度防御的问题，误将良性的输入标记为恶意的。为此，研究者开发了NotInject数据集，系统性地评估模型的过度防御情况，并提出了InjecGuard模型，通过一种称为“免费减轻过度防御”的新训练策略MOF，显著减少了对触发词的偏见。实验结果显示，InjecGuard在NotInject等基准测试中表现最佳，比现有最好模型高30.8%，展示了其优越性和开放性。",
        "摘要译文": "提示注入攻击对大型语言模型（LLMs）构成严重威胁，能够导致目标劫持和数据泄露。虽然提示防护模型在防御方面非常有效，但它们会因触发词偏差而过度防御，错误地将良性输入标记为恶意输入。为了解决这一问题，我们引入了NotInject，这是一个系统性测量各种提示防护模型过度防御情况的评价数据集。NotInject包含339个带有常见提示注入攻击触发词的良性样本，使得细粒度的评估成为可能。我们的结果显示，现有的最先进的模型存在过度防御的问题，准确率下降到近乎随机猜测的水平（约60%）。为了缓解这一问题，我们提出了InjecGuard，这是一个全新的提示防护模型，它引入了一种新的训练策略——免费缓解过度防御（MOF），显著减少了对触发词的偏见。InjecGuard在包括NotInject在内的多种基准测试中表现出最先进的性能，超越现有的最佳模型30.8%，提供了一个强大且开源的提示注入攻击检测解决方案。代码和数据集发布在https://github.com/SaFoLab-W ISC/InjecGuard。"
    },
    {
        "序号": 114,
        "标题": "Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot",
        "链接": "http://arxiv.org/abs/2410.22767v1",
        "作者": [
            "Sejin Lee",
            "Dongha Kim",
            "Min Song"
        ],
        "摘要": "Goal-oriented chatbots are essential for automating user tasks, such as\nbooking flights or making restaurant reservations. A key component of these\nsystems is Dialogue State Tracking (DST), which interprets user intent and\nmaintains the dialogue state. However, existing DST methods often rely on fixed\nontologies and manually compiled slot values, limiting their adaptability to\nopen-domain dialogues. We propose a novel approach that leverages instruction\ntuning and advanced prompt strategies to enhance DST performance, without\nrelying on any predefined ontologies. Our method enables Large Language Model\n(LLM) to infer dialogue states through carefully designed prompts and includes\nan anti-hallucination mechanism to ensure accurate tracking in diverse\nconversation contexts. Additionally, we employ a Variational Graph Auto-Encoder\n(VGAE) to model and predict subsequent user intent. Our approach achieved\nstate-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST\nmodels, and performed well in open-domain real-world conversations. This work\npresents a significant advancement in creating more adaptive and accurate\ngoal-oriented chatbots.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "There are 10 chapters, including references, and 2 figures used. To\n  be presented at the 15th IEEE International Conference on Knowledge Graphs\n  (ICKG2024)",
        "日期": "2024-10-30T07:36:23+00:00",
        "概述": "本文致力于改进目标导向聊天机器人的对话状态跟踪（DST），旨在解决现有方法依赖固定 ontology 和手动编写的槽值的问题。研究提出了一种结合指令调优和先进提示策略的方法，无需任何预定义的 ontology。通过精心设计的提示和变分图自编码器（VGAE）预测后续意图，以增强 DST 性能，并引入反幻觉机制确保准确追踪。实验结果表明，提出的算法在泛化对话中表现优异，JGA 达到 42.57%，超越现有无 ontology DST 模型，显著提升了聊天机器人的适应性和准确性。",
        "摘要译文": "面向目标的聊天机器人对于自动化用户任务（如预订航班或餐馆订位）至关重要。这些系统的关键组件是对话状态跟踪（DST），它解释用户意图并维护对话状态。然而，现有的DST方法往往依赖于固定的本体和手动编写的槽值，限制了它们在开放式领域对话中的适应性。我们提出了一种新的方法，该方法利用指令调整和高级提示策略来增强DST性能，而不依赖于任何预定义的本体。我们的方法通过精心设计的提示使大型语言模型（LLM）能够推断对话状态，并包含一种反幻觉机制，以确保在各种对话情景中准确跟踪。此外，我们还采用了变分图自编码器（VGAE）来建模和预测后续用户意图。我们的方法在JGA为42.57%的情况下达到了最先进的水平，超越了现有的无本体DST模型，并在开放式领域的真实对话中表现良好。这项工作为创建更适应性和更准确的面向目标的聊天机器人带来了重要进展。"
    },
    {
        "序号": 111,
        "标题": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations",
        "链接": "http://arxiv.org/abs/2410.22821v1",
        "作者": [
            "Jia Li",
            "Ge Li",
            "Xuanming Zhang",
            "Yunfei Zhao",
            "Yihong Dong",
            "Zhi Jin",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "摘要": "How to evaluate Large Language Models (LLMs) in code generation remains an\nopen question. Existing benchmarks have two limitations - data leakage and lack\nof domain-specific evaluation. The former hurts the fairness of benchmarks, and\nthe latter hinders practitioners from selecting superior LLMs for specific\nprogramming domains. To address these two limitations, we propose a new\nbenchmark - EvoCodeBench, which has the following advances: (1) Evolving data.\nEvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid\ndata leakage. This paper releases the first version - EvoCodeBench-2403,\ncontaining 275 samples from 25 repositories. (2) A domain taxonomy and domain\nlabels. Based on the statistics of open-source communities, we design a\nprogramming domain taxonomy consisting of 10 popular domains. Based on the\ntaxonomy, we annotate each sample in EvoCodeBench with a domain label. (3)\nDomain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific\nImprovement (DSI) and define LLMs' comfort and strange domains. These\nevaluations help practitioners select superior LLMs in specific domains and\ndiscover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g.,\ngpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights.\nEvoCodeBench reveals the actual abilities of these LLMs in real-world\nrepositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is\nonly 20.74%. Besides, we evaluate LLMs in different domains and discover their\ncomfort and strange domains. For example, gpt-4 performs best in most domains\nbut falls behind others in the Internet domain. StarCoder 2-15B unexpectedly\nperforms well in the Database domain and even outperforms 33B LLMs.\nEvoCodeBench has been released.",
        "分类": [
            "cs.CL",
            "cs.SE"
        ],
        "补充信息": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
        "日期": "2024-10-30T08:57:59+00:00",
        "概述": "该研究针对现有代码生成模型评估基准存在的数据泄露和缺乏领域特定评估的问题，提出了一种新的动态更新的评估基准EvoCodeBench。EvoCodeBench包含10个领域分类，每个样本都标注了领域标签，并计算领域特定改进等指标，帮助用户选择更适合特定领域的代码生成模型。研究表明，不同模型在不同领域的表现差异显著。例如，gpt-4在大多数领域表现良好，但在互联网领域表现不佳，而StarCoder 2-15B在数据库领域表现超过33B模型。",
        "摘要译文": "如何评估大型语言模型（LLMs）在代码生成方面的性能仍然是一个开放的问题。现有的基准测试有两个限制——数据泄漏和缺乏针对特定领域的评估。前者损害了基准测试的公平性，后者阻碍了 practitioners 从众多 LLM 中挑选出最适合特定编程领域的顶级模型。为了解决这两个限制，我们提出了一个新基准——EvoCodeBench，它有以下进步：（1）动态发展的数据。EvoCodeBench 将每隔一定时期（如6个月）动态更新，以避免数据泄漏。本文发布了第一个版本——EvoCodeBench-2403，包含来自25个仓库的275个样本。（2）领域分类和领域标签。基于开源社区的统计数据，我们设计了一个由10个流行领域组成的编程领域分类。基于分类体系，我们为 EvoCodeBench 中的每个样本标注了领域标签。（3）针对特定领域的评估。除了 Pass@k 外，我们还计算了领域特定改进（DSI）并定义了 LLM 的舒适领域和陌生领域。这些评估有助于 practitioners 在特定领域选择更优的 LLM，并发现现有 LLM 的不足之处。我们对8个流行的 LLM（如 gpt-4, DeepSeek Coder）进行了 EvoCodeBench 的评估，并总结了一些见解。EvoCodeBench 显示了这些 LLM 在实际仓库中的真实能力。例如，EvoCodeBench-2403 中 gpt-4 的最高 Pass@1 只有20.74%。此外，我们还分别在不同领域评估了 LLM，并发现它们的舒适领域和陌生领域。例如，gpt-4 在多数领域表现最佳，但在互联网领域却落后于其他模型。StarCoder 2-15B 在数据库领域出人意料地表现良好，并且甚至超越了33B的LLM。EvoCodeBench 已经发布。"
    },
    {
        "序号": 116,
        "标题": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings",
        "链接": "http://arxiv.org/abs/2410.22685v1",
        "作者": [
            "Yashvir S. Grewal",
            "Edwin V. Bonilla",
            "Thang D. Bui"
        ],
        "摘要": "Accurately quantifying uncertainty in large language models (LLMs) is crucial\nfor their reliable deployment, especially in high-stakes applications. Current\nstate-of-the-art methods for measuring semantic uncertainty in LLMs rely on\nstrict bidirectional entailment criteria between multiple generated responses\nand also depend on sequence likelihoods. While effective, these approaches\noften overestimate uncertainty due to their sensitivity to minor wording\ndifferences, additional correct information, and non-important words in the\nsequence. We propose a novel approach that leverages semantic embeddings to\nachieve smoother and more robust estimation of semantic uncertainty in LLMs. By\ncapturing semantic similarities without depending on sequence likelihoods, our\nmethod inherently reduces any biases introduced by irrelevant words in the\nanswers. Furthermore, we introduce an amortised version of our approach by\nexplicitly modelling semantics as latent variables in a joint probabilistic\nmodel. This allows for uncertainty estimation in the embedding space with a\nsingle forward pass, significantly reducing computational overhead compared to\nexisting multi-pass methods. Experiments across multiple question-answering\ndatasets and frontier LLMs demonstrate that our embedding-based methods provide\nmore accurate and nuanced uncertainty quantification than traditional\napproaches.",
        "分类": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T04:41:46+00:00",
        "概述": "本文旨在通过引入语义嵌入方法提高大型语言模型（LLMs）的不确定性量化准确性，特别是在高风险应用场景中。现有方法依赖于生成响应间的双向蕴含关系和序列似然性，但容易因细微语义差异、多余正确信息等造成过高估计。文中提出了一种新方法，利用语义嵌入捕捉语义相似性，减少无关词汇带来的偏差，同时引入一种通过联合概率模型使语义作为潜在变量来实现单次前向传递计算不确定性估计的策略，显著降低计算开销。实验表明，该方法提供了更准确和细腻的不确定性量化。",
        "摘要译文": "准确量化大型语言模型（LLMs）中的不确定性对于其可靠部署至关重要，尤其是在高风险应用场景中。当前用于测量LLMs语义不确定性的最先进方法依赖于多个生成响应之间的严格双向蕴含标准，并且依赖于序列似然性。虽然这些方法有效，但它们往往会因对细微措辞差异、额外正确的信息以及序列中的非重要词语的敏感性而高估不确定性。我们提出了一种新颖的方法，利用语义嵌入来实现对LLMs语义不确定性的更平滑和更 robust 的估计。通过捕捉语义相似性而不依赖于序列似然性，我们的方法本身会减少由于无关词语引入的任何偏差。此外，我们通过在联合概率模型中明确将语义作为潜在变量来引入我们方法的计算化版本。这使得我们可以仅通过单次前向传递就在嵌入空间中进行不确定性估计，极大地减少了与现有多次传递方法相比的计算开销。跨多个问答数据集和前沿LLMs的实验表明，我们的基于嵌入的方法提供了比传统方法更准确和细腻的不确定性量化。"
    },
    {
        "序号": 119,
        "标题": "Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation",
        "链接": "http://arxiv.org/abs/2410.22642v1",
        "作者": [
            "Ruiyu Xiao",
            "Lei Wu",
            "Yuhang Gou",
            "Weinan Zhang",
            "Ting Liu"
        ],
        "摘要": "Argumentative essay generation (AEG) aims to generate complete texts on\nspecific controversial topics or debates. Although current AEG methods can\ngenerate individual opinions, they often overlook the high-level connections\nbetween these opinions. This often leads to the generated results being mired\nin logical confusion, unable to proof their own arguments effectively. The\ngenerated essay may present evidence that contradicts the claims or they may\nfail to assemble the claims into logical flow. In this paper, we present a\nunified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for\nAEG with a focus on logical enhancement. Specifically, we first construct\npseudo-labels for logical information,claims and grounds, using a large\nlanguage model. We then propose a tree planning approach that introduces proof\nprinciples and ensures logical consistency. Extensive experimental results show\nthat, benefiting from proof principle guidance, PESA generates argumentative\nessays with better logical validity and persuasiveness than strong baseline\nmodels.",
        "分类": [
            "cs.CL",
            "cs.AI"
        ],
        "补充信息": "EMNLP 2024",
        "日期": "2024-10-30T02:13:39+00:00",
        "概述": "本文旨在解决当前自动论文字生成功率低且逻辑不连贯的问题。为改善这一状况，作者提出了一种名为Proof-Enhancement and Self-Annotation（PESA）的统一框架，分为逻辑增强和自我注释两个阶段。PESA首先利用大规模语言模型构建逻辑信息伪标签，然后采用树规划方法引入证明原则以保证逻辑一致性。实验结果显示，PESA生成的论文字在逻辑有效性和说服力方面优于基准模型。",
        "摘要译文": "论點性文章生成（AEG）的目标是在特定有争议的主题或辩论上生成完整的文本。尽管当前的AEG方法可以生成个人观点，但它们常常忽略了这些观点之间的高层联系。这通常导致生成的结果陷入逻辑混乱，无法有效地证明自己的论点。生成的文章可能会呈现与主张相矛盾的证据，或者无法将主张组织成逻辑流程。在本文中，我们提出了一种统一的两阶段框架：论据增强和自我注释（PESA），重点关注逻辑增强。具体而言，我们首先使用大型语言模型构建逻辑信息、主张和根据的伪标签。然后，我们提出了一种树木规划方法，引入了论据原则并确保逻辑一致性。广泛的实验结果表明，得益于论据原则的指导，PESA生成的论点性文章在逻辑有效性和说服力方面优于强大的基线模型。"
    },
    {
        "序号": 117,
        "标题": "Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers",
        "链接": "http://arxiv.org/abs/2410.22663v1",
        "作者": [
            "Lam Nguyen Tung",
            "Steven Cho",
            "Xiaoning Du",
            "Neelofar Neelofar",
            "Valerio Terragni",
            "Stefano Ruberto",
            "Aldeida Aleti"
        ],
        "摘要": "Machine learning (ML) for text classification has been widely used in various\ndomains, such as toxicity detection, chatbot consulting, and review analysis.\nThese applications can significantly impact ethics, economics, and human\nbehavior, raising serious concerns about trusting ML decisions. Several studies\nindicate that traditional metrics, such as model confidence and accuracy, are\ninsufficient to build human trust in ML models. These models often learn\nspurious correlations during training and predict based on them during\ninference. In the real world, where such correlations are absent, their\nperformance can deteriorate significantly. To avoid this, a common practice is\nto test whether predictions are reasonable. Along with this, a challenge known\nas the trustworthiness oracle problem has been introduced. Due to the lack of\nautomated trustworthiness oracles, the assessment requires manual validation of\nthe decision process disclosed by explanation methods, which is time-consuming\nand not scalable. We propose TOKI, the first automated trustworthiness oracle\ngeneration method for text classifiers, which automatically checks whether the\nprediction-contributing words are related to the predicted class using\nexplanation methods and word embeddings. To demonstrate its practical\nusefulness, we introduce a novel adversarial attack method targeting\ntrustworthiness issues identified by TOKI. We compare TOKI with a naive\nbaseline based solely on model confidence using human-created ground truths of\n6,000 predictions. We also compare TOKI-guided adversarial attack method with\nA2T, a SOTA adversarial attack method. Results show that relying on prediction\nuncertainty cannot distinguish between trustworthy and untrustworthy\npredictions, TOKI achieves 142% higher accuracy than the naive baseline, and\nTOKI-guided adversarial attack method is more effective with fewer\nperturbations than A2T.",
        "分类": [
            "cs.SE",
            "cs.CL",
            "cs.CR"
        ],
        "补充信息": null,
        "日期": "2024-10-30T03:26:37+00:00",
        "概述": "本文研究了机器学习文本分类器的信任度或acles生成问题，动机在于传统评估指标不足以建立人类对ML模型的信任。当前方法需要手动验证解释方法披露的决策过程，耗时且不具备扩展性。为此，作者提出了TOKI，一种自动化信任度或acles生成方法，通过解释方法和词嵌入技术自动检查预测贡献词是否与预测类别相关。实验结果表明，TOKI在区分可信与不可信预测上比基于模型置信度的基线方法准确度高142%，且引导的对抗攻击比当前最先进的方法更有效。",
        "摘要译文": "机器学习（ML）在文本分类中的应用广泛应用于不同领域，如毒性检测、聊天机器人咨询和评论分析。这些应用可以显著影响伦理、经济学和人类行为，引发对信任ML决策的严重关切。多项研究表明，传统的模型信心和准确率等指标不足以建立人类对ML模型的信任。在训练过程中，这些模型往往会学习到虚假的相关性，并且在推理时基于这些相关性进行预测。在现实世界中，由于这些相关性不存在，它们的表现可能会显著恶化。为了避免这种情况，一个常见的做法是测试预测是否合理。与此相关，已经提出了一个称为信任度预言机问题的新挑战。由于缺乏自动的信任度预言机，评估需要手动验证解释方法披露的决策过程，这既耗时又不具备扩展性。我们提出了一种名为TOKI的自动信任度预言机生成方法，它是第一个用于文本分类器的方法，它能够自动检查预测贡献的词是否与预测类别相关。为了展示其实用性，我们引入了一种新的针对TOKI识别的信任度问题的对抗攻击方法。我们将TOKI与仅依赖模型信心的简单基线方法进行了比较，使用人类创建的6,000个预测的真实值作为参照。我们还比较了TOKI指导下的对抗攻击方法与当前最优的A2T对抗攻击方法。结果表明，依赖预测不确定性无法区分可信赖和不可信赖的预测，TOKI的准确率比简单基线高出142%，并且TOKI指导下的对抗攻击方法在较少扰动的情况下更有效。"
    },
    {
        "序号": 118,
        "标题": "Linguistics Theory Meets LLM: Code-Switched Text Generation via Equivalence Constrained Large Language Models",
        "链接": "http://arxiv.org/abs/2410.22660v1",
        "作者": [
            "Garry Kuwanto",
            "Chaitanya Agarwal",
            "Genta Indra Winata",
            "Derry Tanti Wijaya"
        ],
        "摘要": "Code-switching, the phenomenon of alternating between two or more languages\nin a single conversation, presents unique challenges for Natural Language\nProcessing (NLP). Most existing research focuses on either syntactic\nconstraints or neural generation, with few efforts to integrate linguistic\ntheory with large language models (LLMs) for generating natural code-switched\ntext. In this paper, we introduce EZSwitch, a novel framework that combines\nEquivalence Constraint Theory (ECT) with LLMs to produce linguistically valid\nand fluent code-switched text. We evaluate our method using both human\njudgments and automatic metrics, demonstrating a significant improvement in the\nquality of generated code-switching sentences compared to baseline LLMs. To\naddress the lack of suitable evaluation metrics, we conduct a comprehensive\ncorrelation study of various automatic metrics against human scores, revealing\nthat current metrics often fail to capture the nuanced fluency of code-switched\ntext. Additionally, we create CSPref, a human preference dataset based on human\nratings and analyze model performance across ``hard`` and ``easy`` examples.\nOur findings indicate that incorporating linguistic constraints into LLMs leads\nto more robust and human-aligned generation, paving the way for scalable\ncode-switching text generation across diverse language pairs.",
        "分类": [
            "cs.CL"
        ],
        "补充信息": null,
        "日期": "2024-10-30T03:03:32+00:00",
        "概述": "本文介绍了EZSwitch框架，通过将等价约束理论（ECT）与大型语言模型（LLMs）结合，解决代码转换（指在同一对话中交替使用两种或多种语言）的自然语言处理（NLP）挑战。研究发现，与基础LLMs相比，该方法在生成代码转换句子的质量上有了显著提升。研究还揭示了现有自动评估指标往往无法准确捕捉代码转换文本的流利度，并创建了CSPref人类偏好数据集来评估模型表现。研究结果表明，将语言约束纳入LLMs可以提高生成的鲁棒性和人类一致性，为多种语言对的代码转换文本生成提供途径。",
        "摘要译文": "代码转换，即在同一对话中交替使用两种或多种语言的现象，为自然语言处理（NLP）带来了独特的挑战。现有的大多数研究要么关注语法约束，要么关注神经生成，很少有研究尝试将语言理论与大型语言模型（LLMs）结合以生成自然的代码转换文本。在本文中，我们提出了EZSwitch，这是一个结合等价约束理论（ECT）和LLMs的新框架，用于生成语义正确且流畅的代码转换文本。我们使用人工评估和自动度量来评估我们的方法，结果显示与基础LLMs相比，生成的代码转换句子的质量有了显著提高。为了解决合适的评估度量缺失的问题，我们进行了各种自动度量与人工评分之间相关性的全面研究，揭示了当前度量往往无法捕捉代码转换文本的精细流畅性。此外，我们基于人工评价创建了CSPref数据集，并分析了模型在“硬”和“易”示例上的性能。我们的研究结果表明，将语言约束纳入LLMs中可以产生更加稳健和与人类目标对齐的生成结果，为跨多种语言对的大规模代码转换文本生成铺平了道路。"
    }
]